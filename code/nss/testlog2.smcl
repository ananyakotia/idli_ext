{smcl}
{com}{sf}{ul off}{txt}{.-}
      name:  {res}<unnamed>
       {txt}log:  {res}C:\Users\ayush\OneDrive\Documents\idli_ext\code\nss\testlog2.smcl
  {txt}log type:  {res}smcl
 {txt}opened on:  {res}19 Nov 2025, 15:55:38

{com}. include "C:\Users\ayush\AppData\Local\Temp\STD33c0_000000.tmp"
{txt}
{com}. * preamble
. clear all
{res}{txt}
{com}. set varabbrev off
{txt}
{com}. macro drop _all
{txt}
{com}. set more off
{txt}
{com}. pause on
{txt}
{com}. set excelxlsxlargefile on
{txt}
{com}. 
. global packages_install = 1
{txt}
{com}. global packages_update = 1
{txt}
{com}. 
. * install Stata packages
. if $packages_install {c -(}
.         foreach package in /// colrspace ///
>                 confirmdir ///
>                 distinct ///
>                 grstyle ///
>                 gtools ///
>                 palettes /// for grstyle
>                 colrspace /// for grstyle
>                 mipolate ///
>                 reghdfe ///
>                 estout /// 
>                 ftools /// needed for reghdfe
>                 coefplot ///
>                 freqindex /// required for matchit
>                 grc1leg2 /// graph combine with common legend
>                 nicelabels ///
>                 {c -(}
{txt}  2{com}.                 cap which `package'
{txt}  3{com}.                 if _rc ssc install `package', all replace
{txt}  4{com}.         {c )-}
{txt}checking {hilite:palettes} consistency and verifying not already installed...
all files already exist and are up to date.
checking {hilite:colrspace} consistency and verifying not already installed...
all files already exist and are up to date.
{com}.         cap which renvars
.         if _rc {c -(}
.                 net install dm88_1, from ("http://www.stata-journal.com/software/sj5-4")
.                 cap n net get dm88_1
.                 cap n net install dm88_1
.         {c )-}
.         cap which grc1leg2
.         if _rc {c -(}
.                 net install grc1leg2.pkg, from (http://digital.cgdev.org/doc/stata/MO/Misc/)
.         {c )-}
. {c )-}
{txt}
{com}. 
. if $packages_update {c -(}
.         cap n gtools, upgrade
checking {hilite:gtools} consistency and verifying not already installed...
installing into C:\Users\ayush\ado\plus\...
installation complete.
.         cap n adoupdate, update
{res}{txt}{p 0 6 2}note: {bf:ado update} updates community-contributed files; type {bf:update} to check for updates to official Stata.{p_end}

{txt}Checking status of installed packages:

{p 4 8 2}
{txt}[1] {res:grstyle} at https://raw.githubusercontent.com/benjann/grstyle/master:{break}
installed package is up to date
{p_end}

{p 4 8 2}
{txt}[2] {res:palettes} at http://fmwww.bc.edu/repec/bocode/p:{break}
installed package is up to date
{p_end}

{p 4 8 2}
{txt}[3] {res:colrspace} at http://fmwww.bc.edu/repec/bocode/c:{break}
installed package is up to date
{p_end}

{p 4 8 2}
{txt}[4] {res:confirmdir} at http://fmwww.bc.edu/repec/bocode/c:{break}
installed package is up to date
{p_end}

{p 4 8 2}
{txt}[5] {res:nicelabels} at http://fmwww.bc.edu/repec/bocode/n:{break}
installed package is up to date
{p_end}

{p 4 8 2}
{txt}[6] {res:ereplace} at http://fmwww.bc.edu/repec/bocode/e:{break}
installed package is up to date
{p_end}

{p 4 8 2}
{txt}[7] {res:distinct} at http://fmwww.bc.edu/repec/bocode/d:{break}
installed package is up to date
{p_end}

{p 4 8 2}
{txt}[8] {res:mipolate} at http://fmwww.bc.edu/repec/bocode/m:{break}
installed package is up to date
{p_end}

{p 4 8 2}
{txt}[9] {res:reghdfe} at http://fmwww.bc.edu/repec/bocode/r:{break}
installed package is up to date
{p_end}

{p 3 8 2}
{txt}[10] {res:estout} at http://fmwww.bc.edu/repec/bocode/e:{break}
installed package is up to date
{p_end}

{p 3 8 2}
{txt}[11] {res:ftools} at http://fmwww.bc.edu/repec/bocode/f:{break}
installed package is up to date
{p_end}

{p 3 8 2}
{txt}[12] {res:coefplot} at http://fmwww.bc.edu/repec/bocode/c:{break}
installed package is up to date
{p_end}

{p 3 8 2}
{txt}[13] {res:freqindex} at http://fmwww.bc.edu/repec/bocode/f:{break}
installed package is up to date
{p_end}

{p 3 8 2}
{txt}[14] {res:grc1leg2} at http://fmwww.bc.edu/repec/bocode/g:{break}
installed package is up to date
{p_end}

{p 3 8 2}
{txt}[15] {res:dm88_1} at http://www.stata-journal.com/software/sj5-4:{break}
installed package is up to date
{p_end}

{p 3 8 2}
{txt}[16] {res:gtools} at https://raw.githubusercontent.com/mcaceresb/stata-gtools/master/build:{break}
installed package is up to date
{p_end}

{txt}(no packages require updating)
{com}. {c )-}
{txt}
{com}. 
. * define directories /// an user add thier system path below, and access the directories
> global root = ""
. foreach dir in ///
>         "/Users/kotia/Dropbox" /// KOTIA MAC 
>         "/users/kotiaa/Documents" /// KOTIA FABIAN SERVER
>         "C:/Users/bhara/Dropbox" /// BHARAT'S PC 
>         "C:/Users/NAILA FATIMA" /// NAILA WINDOWS
>         "/Users/gabrielhill/Dropbox" /// GABRIEL MAC 
>         "/Users/nailafatima/Library/CloudStorage/Dropbox" /// NAILA'S MAC
>         "C:/Users/ayush/Dropbox" /// Ayush 
>         "C:/Users/Meghana/Dropbox" /// Meghana 
>         {c -(}
{txt}  2{com}.         confirmdir "`dir'"
{txt}  3{com}.         if !_rc global root = "`dir'"
{txt}  4{com}. {c )-}
{txt}
{com}. 
. if "${c -(}root{c )-}" == "" {c -(}
.         di as error "USER ERROR: Cannot find main directory (\$root)!"
.         error 1
. {c )-}
{txt}
{com}. 
. else if "${c -(}root{c )-}" == "C:/Users/bhara/Dropbox" {c -(}
.         global idl "$root/idl"
.         global idl_git "C:/Users/bhara/OneDrive/Informal_Formal_Report/Documents/Github/idli_ext"
.         global nic_concordances "$root/nic_concordances/data/clean"
. {c )-}
{txt}
{com}. 
. else if "${c -(}root{c )-}" == "/Users/kotia/Dropbox" {c -(}
.         global idl "$root/idl"
.         global idl_git "Users/kotia/Documents/Github/idl"
.         global nic_concordances "$root/nic_concordances/data/clean"
. {c )-}
{txt}
{com}. 
. else if "${c -(}root{c )-}" == "C:/Users/NAILA FATIMA" {c -(}
.         global idl "$root/Dropbox/idl"
.         global ida "$root/Dropbox/india_labor"
.         global idl_git "$root/Documents/GitHub/idl"
.         global nic_concordances "$root/Dropbox/nic_concordances/data/clean"
.         global asi_state_nic "$root/Dropbox/ASI state x nic3"
. 
. {c )-}
{txt}
{com}. 
. else if "${c -(}root{c )-}" == "/Users/nailafatima/Library/CloudStorage/Dropbox" {c -(}
.         global idl "$root/idl"
.         global ida "$root/india_labor"
.         global idl_git "/Users/nailafatima/Documents/GitHub/idl"
.         global nic_concordances "$root/nic_concordances/data/clean"
.         global asi_state_nic "$root/ASI state x nic3"
. 
. {c )-}
{txt}
{com}. else if "${c -(}root{c )-}" == "C:/Users/ayush/Dropbox" {c -(}
.         global idl "$root/idl"
.         global idl_git "C:/Users/ayush/OneDrive/Documents/idli_ext"
.         global nic_concordances "$root/nic_concordances/data/clean"
. {c )-}
{txt}
{com}. 
. else if "${c -(}root{c )-}" == "C:\Users\Meghana\Dropbox" {c -(}
.         global idl "$root\idl"
.         global idl_git "C:\Users\Meghana\OneDrive\Documents\GitHub\idl"
.         global nic_concordances "$root\nic_concordances\data\clean"
. {c )-}
{txt}
{com}. 
. else {c -(}
.         global idl "$root/Research/idl"
. {c )-}
{txt}
{com}. 
. 
. * NSS
. global nss "$idl/nss"
{txt}
{com}. global nss_lab "$nss/labour"
{txt}
{com}. global nss_ent "$nss/enterprise"
{txt}
{com}. global nss_cons "$nss/consumption"
{txt}
{com}. global dist "$nss/district"
{txt}
{com}. global shrug "$nss/district/shrug"
{txt}
{com}. global code "$idl_git/code"
{txt}
{com}. 
. global asi "$idl/asi"
{txt}
{com}. 
. 
. 
. global fig "$idl/output/fig"
{txt}
{com}. 
. * for figs
. grstyle init
{txt}Scheme {bf:_GRSTYLE_} already active although {bf:grstyle} is not running
Press any key to reset default scheme to {bf:s2color} and overwrite scheme {bf:_GRSTYLE_}, or Break to abort
({cmd:set scheme} preference recorded)
{res}{txt}
{com}. grstyle set plain
{txt}
{com}. grstyle set color Set1
{res}{txt}
{com}. grstyle set symbol
{res}{txt}
{com}. grstyle set lpattern
{txt}
{com}. grstyle set nogrid
{txt}
{com}. grstyle linestyle legend none
{txt}
{com}. grstyle set legend, inside
{txt}
{com}. grstyle set linewidth 2.4pt: plineplot
{txt}
{com}. *grstyle set graphsize 10cm 10cm
. grstyle set margin "1pt 1pt 1pt 1pt": graph
{res}{txt}
{com}. graph set window fontface "Arial"
{txt}
{com}. 
. 
. 
. . include "C:\Users\ayush\AppData\Local\Temp\STD33c0_000000.tmp"
{txt}
{com}. 
. /*
> We are cleaning the following surveys:
>         - Employment and Unemployment Survey: NSS 43rd Round : July 1987 - June 1988
>         - Employment and Unemployment Survey: NSS 50th Round : July 1993 - June 1994
>         - Employment and Unemployment Survey: NSS 55th Round : July 1999 - June 2000
>         - Employment and Unemployment Survey: NSS 60th Round : July 2003 - June 2004
>         - Employment and Unemployment Survey: NSS 61st Round : July 2004 - June 2005
>         - Employment and Unemployment Survey: NSS 62nd Round : July 2005 - June 2006
>         - Employment and Unemployment Survey: NSS 64th Round : July 2007 - June 2008
>         - Employment and Unemployment Survey: NSS 66th Round : July 2009 - June 2010
>         - Employment and Unemployment Survey: NSS 68th Round : July 2011 - June 2012
>         
> */
. 
. 
. /* Clean and Appennd Individual and Household Characterstics datasets (yearwise), followed by labelling,
> checking for misssing values, and then by making graphs on key variables to check the trend in data. */
. 
. * Clean Household Characterstics datasets of NSS Labour Surveys 
.         foreach yr in "1983" "1987" "1993" "1999" "2003" "2004" "2005" "2007" "2009" "2011" {c -(}
{txt}  2{com}.         
.                 do "$code/nss/nss_lab/01_1_`yr'_clean_hc.do"
{txt}  3{com}.         {c )-}
{txt}
{com}. * Purpose: To clean data at the person-level for year 1983                           
. 
. *** ---------------------------------------------------------------------------
. *** Block 1-3: Household Records
. *** ---------------------------------------------------------------------------
. 
. use ///
>         Hhold_key Sector State Region B3_q1_hh_size B3_q3 B3_q4_relgn B3_q5_hh_grup ///
>         Wgt4_pooled /// Weight variable- Sub-round pooled
>         using "$nss_lab/raw/1983/extracted dta files/Block-1-3-Household-records.dta", clear
{txt}
{com}. 
. * Renaming variables
. rename *, lower
{res}{txt}
{com}. rename b3_q1_hh_size    hh_size
{res}{txt}
{com}. rename hhold_key                hh_key
{res}{txt}
{com}. rename b3_q3                    emp_type_hh1
{res}{txt}
{com}. rename state                    st_code
{res}{txt}
{com}. rename b3_q4_relgn              religion
{res}{txt}
{com}. rename b3_q5_hh_grup    caste 
{res}{txt}
{com}. rename wgt4_pooled              weight
{res}{txt}
{com}. 
. * Dropping hh_key duplicates
. bys hh_key: gen dup = cond(_N==1,0,_n)
{txt}
{com}. drop if dup> 1
{txt}(12 observations deleted)

{com}. drop dup
{txt}
{com}. 
. * Saving the dataset
. save "$nss_lab/intermediate/Blk_hc_1983", replace
{txt}{p 0 4 2}
file {bf}
C:/Users/ayush/Dropbox/idl/nss/labour/intermediate/Blk_hc_1983.dta{rm}
saved
{p_end}

{com}. 
. 
{txt}end of do-file

{com}. * Purpose: To clean data at the person-level for year 1987                          
. 
. *** ---------------------------------------------------------------------------
. *** Block 1-3: Household Records
. *** ---------------------------------------------------------------------------
. 
. * Loading the household levele dataset
. use ///
>         Hhold_key Sector State Region District /// Common variables
>         B3_q1_Hhsize B3_q3 B3_q4_Relgn B3_q5_Hgrup B3_q7 B3_q19  /// Native variables
>         Wgt4_pooled /// Weight variable- Sub-round pooled
>         using "$nss_lab/raw/1987/extracted dta files/Block-1-3-Household-Records.dta", clear
{txt}
{com}. 
. * Renaming variables
. rename *, lower
{res}{txt}
{com}. rename b3_q1_hhsize             hh_size
{res}{txt}
{com}. rename hhold_key                hh_key
{res}{txt}
{com}. rename b3_q3                    emp_type_hh1
{res}{txt}
{com}. rename state                    st_code
{res}{txt}
{com}. rename district                 dist_code
{res}{txt}
{com}. rename b3_q4_relgn              religion
{res}{txt}
{com}. rename b3_q5_hgrup              caste 
{res}{txt}
{com}. rename b3_q7                    land_owned // (in 0.00 hectares) 
{res}{txt}
{com}. rename b3_q19                   total_exp_hh
{res}{txt}
{com}. rename wgt4_pooled              weight
{res}{txt}
{com}. 
. * Destringing variables
. replace caste = "." if caste == ";" 
{txt}(3 real changes made)

{com}. destring sector emp_type_hh1 religion caste total_exp_hh hh_size, replace
{txt}sector: all characters numeric; {res}replaced {txt}as {res}byte
{txt}emp_type_hh1: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(251 missing values generated)
{res}{txt}religion: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(231 missing values generated)
{res}{txt}caste: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(47 missing values generated)
{res}{txt}total_exp_hh already numeric; no {res}replace
{txt}hh_size already numeric; no {res}replace
{txt}
{com}. 
. * Generating nss region variable
. gegen nss_region = concat(st_code region)
{txt}concat() is not a gtools function and no by(); falling back on egen

{com}. 
. * Dropping hh_key duplicates
. bys hh_key: gen dup = cond(_N==1,0,_n)
{txt}
{com}. drop if dup> 1
{txt}(16 observations deleted)

{com}. drop dup
{txt}
{com}. 
. * Saving the dataset
. save "$nss_lab/intermediate/Blk_hc_1987", replace
{txt}{p 0 4 2}
file {bf}
C:/Users/ayush/Dropbox/idl/nss/labour/intermediate/Blk_hc_1987.dta{rm}
saved
{p_end}

{com}. 
. 
{txt}end of do-file

{com}. * Purpose: To clean data at the person-level for year 1993                        
. 
. *** ---------------------------------------------------------------------------
. *** Block 1-3: Household Records
. *** ---------------------------------------------------------------------------
. 
. * Loading the household levele dataset
. use ///
>         Hhold_Key Round Sector State Region /// Common variables
>         B3_q3 B3_q4_relgn_cd B3_q5_sgrup_Cd B3_q20 B3_q1_hh_size /// Native variables 
>         WGT_Pooled                                                                 /// Weight variable- Sub-round pooled/sub-samples combined(0.00) 
>         using "$nss_lab/raw/1993/extracted dta files/Block-1-3-Household-Records.dta", clear
{txt}
{com}. 
. * Renaming variables
. rename *, lower
{res}{txt}
{com}. rename hhold_key                hh_key
{res}{txt}
{com}. rename b3_q1_hh_size    hh_size
{res}{txt}
{com}. rename round                    nss_round
{res}{txt}
{com}. rename state                    st_code
{res}{txt}
{com}. rename b3_q3                    emp_type_hh1
{res}{txt}
{com}. rename b3_q4_relgn_cd   religion
{res}{txt}
{com}. rename b3_q5_sgrup_cd   caste
{res}{txt}
{com}. rename b3_q20                   total_exp_hh
{res}{txt}
{com}. rename wgt_pooled               weight
{res}{txt}
{com}. 
. * Destringing variables
. destring nss_round sector emp_type_hh1 religion caste total_exp_hh hh_size, replace
{txt}nss_round: all characters numeric; {res}replaced {txt}as {res}byte
{txt}sector: all characters numeric; {res}replaced {txt}as {res}byte
{txt}emp_type_hh1 already numeric; no {res}replace
{txt}religion: all characters numeric; {res}replaced {txt}as {res}byte
{txt}caste: all characters numeric; {res}replaced {txt}as {res}byte
{txt}total_exp_hh already numeric; no {res}replace
{txt}hh_size already numeric; no {res}replace
{txt}
{com}. 
. * Saving the dataset
. save "$nss_lab/intermediate/Blk_hc_1993", replace
{txt}{p 0 4 2}
file {bf}
C:/Users/ayush/Dropbox/idl/nss/labour/intermediate/Blk_hc_1993.dta{rm}
saved
{p_end}

{com}. 
. 
. 
. 
. 
. 
. 
{txt}end of do-file

{com}. * Purpose: To clean data at the person-level for year 1999                   
. 
. *** ---------------------------------------------------------------------------
. *** Block 1-3: Household Records
. *** ---------------------------------------------------------------------------
. 
. * Loading the household levele dataset
. use ///
>         RecID Sector State Region District fsu_no Visit_no Seg_no Stg2_stratm Hhhold_Slno /// Common variables
>         B3_q2 B3_q3 B3_q4 B3_q5 B3_q6 /// Native variables 
>         Wgt_SR_comb                                                                /// Weight variable- Sub-round pooled/sub-samples combined(0.00) 
>         using "$nss_lab/raw/1999/extracted dta files/Block3-sch10--Household-Characteristics-records", clear
{txt}
{com}. 
. * Generating hh_key because the hh_key variable in the person dataset is of 11 width but the hh char block has width 13
. gegen hh_key = concat(fsu_no Visit_no Seg_no Stg2_stratm Hhhold_Slno)
{txt}concat() is not a gtools function and no by(); falling back on egen

{com}. 
. drop fsu_no Visit_no Seg_no Stg2_stratm Hhhold_Slno
{txt}
{com}. 
. * Renaming variables
. rename *, lower
{res}{txt}
{com}. rename recid                    block 
{res}{txt}
{com}. rename state                    st_code
{res}{txt}
{com}. rename district                 dist_code
{res}{txt}
{com}. rename b3_q2                    caste                   
{res}{txt}
{com}. rename b3_q3                    religion
{res}{txt}
{com}. rename b3_q4                    emp_type_hh1
{res}{txt}
{com}. rename b3_q5                    total_exp_hh
{res}{txt}
{com}. rename b3_q6                    land_owned // (in 0.00 hectares)
{res}{txt}
{com}. rename wgt_sr_comb              weight
{res}{txt}
{com}. 
. * Destringing variables
. destring sector emp_type_hh1 religion caste total_exp_hh, replace
{txt}sector: all characters numeric; {res}replaced {txt}as {res}byte
{txt}emp_type_hh1: all characters numeric; {res}replaced {txt}as {res}byte
{txt}religion: all characters numeric; {res}replaced {txt}as {res}byte
{txt}caste: all characters numeric; {res}replaced {txt}as {res}byte
{txt}total_exp_hh already numeric; no {res}replace
{txt}
{com}. 
. * Generating the nss region variable by concatenating state and region variables
. gegen nss_region = concat(st_code region)
{txt}concat() is not a gtools function and no by(); falling back on egen

{com}. 
. * Saving the dataset
. save "$nss_lab/intermediate/Blk_hc_1999", replace
{txt}{p 0 4 2}
file {bf}
C:/Users/ayush/Dropbox/idl/nss/labour/intermediate/Blk_hc_1999.dta{rm}
saved
{p_end}

{com}. 
{txt}end of do-file

{com}. * Purpose: To clean data at the person-level for year 1999                   
. 
. *** ---------------------------------------------------------------------------
. *** Block 1-3: Household Records
. *** ---------------------------------------------------------------------------
. 
. * Loading the household levele dataset
. use ///
>         Key_hhold Rec_id Sector State_Region State District /// Common variables
>         B3_q1 B3_q4 B3_q5 B3_q6 B3_q8 /// Native variables
>         wgt_combined /// Multiplier combined
>         using "$nss_lab/raw/2003/extracted dta files/Block-3-Household-characteristics-records.dta", clear
{txt}
{com}. 
. * Apply harmonized naming convention
. rename *, lower
{res}{txt}
{com}. rename key_hhold                hh_key
{res}{txt}
{com}. rename rec_id                   block 
{res}{txt}
{com}. rename state_region     nss_region
{res}{txt}
{com}. rename state                    st_code
{res}{txt}
{com}. rename district                 dist_code
{res}{txt}
{com}. rename b3_q1                    hh_size
{res}{txt}
{com}. rename b3_q6                    caste                   
{res}{txt}
{com}. rename b3_q5                    religion
{res}{txt}
{com}. rename b3_q4                    emp_type_hh1
{res}{txt}
{com}. rename b3_q8                    total_exp_hh    
{res}{txt}
{com}. rename wgt_combined             weight
{res}{txt}
{com}. 
. * Destringing variables
. destring sector emp_type_hh1 religion caste total_exp_hh, replace
{txt}sector: all characters numeric; {res}replaced {txt}as {res}byte
{txt}emp_type_hh1: all characters numeric; {res}replaced {txt}as {res}byte
{txt}religion: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(5 missing values generated)
{res}{txt}caste: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(6 missing values generated)
{res}{txt}total_exp_hh already numeric; no {res}replace
{txt}
{com}. 
. * Making Monthly expenditure per capita. Currently it is at household level
. replace total_exp_hh = total_exp_hh/hh_size
{txt}variable {bf}{res}total_exp_hh{sf}{txt} was {bf}{res}long{sf}{txt} now {bf}{res}double{sf}
{txt}(56,103 real changes made)

{com}. 
. * Saving the dataset
. save "$nss_lab/intermediate/Blk_hc_2004a", replace
{txt}{p 0 4 2}
file {bf}
C:/Users/ayush/Dropbox/idl/nss/labour/intermediate/Blk_hc_2004a.dta{rm}
saved
{p_end}

{com}. 
{txt}end of do-file

{com}. * Purpose: To clean data at the household-level for 2004-05 (July-June)                  
. 
. *** ---------------------------------------------------------------------------
. *** Block 1-3: Household Records
. *** ---------------------------------------------------------------------------
. 
. * Loading the household level dataset
. use HHID Round Sector State_region District STATE_CODE /// Common variables
>         RELIGION SOCIAL_GRP MPCE Household_type_code HH_SIZE LAND_OWNED /// Native variables
>         WEIGHT_COMBINED /// Weight combined
>         using "$nss_lab/raw/2004/extracted dta files/Block_1_2_and_3_level_01.dta", clear
{txt}
{com}. 
. * Renaming variables
. rename *, lower
{res}{txt}
{com}. rename hhid                                     hh_key
{res}{txt}
{com}. rename state_code                       st_code
{res}{txt}
{com}. rename district                         dist_code
{res}{txt}
{com}. rename state_region             nss_region
{res}{txt}
{com}. rename social_grp                       caste                   
{res}{txt}
{com}. rename household_type_code      emp_type_hh1
{res}{txt}
{com}. rename mpce                                     total_exp_hh    
{res}{txt}
{com}. rename weight_combined          weight
{res}{txt}
{com}. 
. * Destringing variables
. destring sector emp_type_hh1 religion caste total_exp_hh hh_size, replace
{txt}sector: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(1 missing value generated)
{res}{txt}emp_type_hh1: all characters numeric; {res}replaced {txt}as {res}byte
{txt}religion: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(10 missing values generated)
{res}{txt}caste: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(86 missing values generated)
{res}{txt}total_exp_hh: all characters numeric; {res}replaced {txt}as {res}long
{txt}hh_size already numeric; no {res}replace
{txt}
{com}. 
. * Making Monthly expenditure per capita. Currently it is at household level
. replace total_exp_hh = total_exp_hh/hh_size
{txt}variable {bf}{res}total_exp_hh{sf}{txt} was {bf}{res}long{sf}{txt} now {bf}{res}double{sf}
{txt}(116,846 real changes made)

{com}. 
. * Saving the dataset
. save "$nss_lab/intermediate/Blk_hc_2004b", replace
{txt}{p 0 4 2}
file {bf}
C:/Users/ayush/Dropbox/idl/nss/labour/intermediate/Blk_hc_2004b.dta{rm}
saved
{p_end}

{com}. 
{txt}end of do-file

{com}. * Purpose: To clean data at the household-level for 2005-06 (July-June)                  
. 
. *** ---------------------------------------------------------------------------
. *** Block 1-3: Household Records
. *** ---------------------------------------------------------------------------
. 
. * Loading the household level dataset
. use Hhold_key Sector State Region District /// Common variables
>         B3_q1 B3_q4 B3_q5 B3_q6 B3_q15 /// Native variables
>         WGT_Comb /// Weight combined
>         using "$nss_lab/raw/2005/extracted dta files/Block-3-Household-Characteristics-records", clear
{txt}
{com}. 
. * Generating NSS region
. gegen nss_region = concat(State Region)
{txt}concat() is not a gtools function and no by(); falling back on egen

{com}. 
. * Renaming variables
. rename *, lower
{res}{txt}
{com}. rename hhold_key                        hh_key
{res}{txt}
{com}. rename state                            st_code
{res}{txt}
{com}. rename district                         dist_code
{res}{txt}
{com}. rename b3_q1                            hh_size
{res}{txt}
{com}. rename b3_q6                            caste   
{res}{txt}
{com}. rename b3_q5                            religion
{res}{txt}
{com}. rename b3_q4                            emp_type_hh1
{res}{txt}
{com}. rename b3_q15                           total_exp_hh    
{res}{txt}
{com}. rename wgt_comb                         weight
{res}{txt}
{com}. 
. * Destringing variables
. destring sector emp_type_hh1 religion caste total_exp_hh hh_size, replace
{txt}sector: all characters numeric; {res}replaced {txt}as {res}byte
{txt}emp_type_hh1: all characters numeric; {res}replaced {txt}as {res}byte
{txt}religion: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(2 missing values generated)
{res}{txt}caste: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(12 missing values generated)
{res}{txt}total_exp_hh already numeric; no {res}replace
{txt}hh_size already numeric; no {res}replace
{txt}
{com}. 
. * Making Monethly expenditure per capita. Currently it is at household level
. replace total_exp_hh = total_exp_hh/hh_size
{txt}variable {bf}{res}total_exp_hh{sf}{txt} was {bf}{res}long{sf}{txt} now {bf}{res}double{sf}
{txt}(73,355 real changes made)

{com}. 
. * Saving the dataset
. save "$nss_lab/intermediate/Blk_hc_2005", replace
{txt}{p 0 4 2}
file {bf}
C:/Users/ayush/Dropbox/idl/nss/labour/intermediate/Blk_hc_2005.dta{rm}
saved
{p_end}

{com}. 
{txt}end of do-file

{com}. * Purpose: To clean data at the household-level for 2007-08 (July-June)                  
. 
. *** ---------------------------------------------------------------------------
. *** Block 1-3: Household Records
. *** ---------------------------------------------------------------------------
. 
. * Loading the household level dataset
. use Key_hhold Rec_id Sector State_Region state District /// Common variables
>         B3_q1 B3_Q4 B3_q5 B3_q6 B3_q17 /// Native variables
>         wgt_combined /// Weight combined
>         using "$nss_lab/raw/2007/extracted dta files/Block-3-household-characteristics-ecords", clear
{txt}
{com}. 
. * Renaming variables
. rename *, lower
{res}{txt}
{com}. rename key_hhold                        hh_key
{res}{txt}
{com}. rename state                            st_code
{res}{txt}
{com}. rename district                         dist_code
{res}{txt}
{com}. rename rec_id                           block
{res}{txt}
{com}. rename b3_q1                            hh_size
{res}{txt}
{com}. rename state_region                     nss_region
{res}{txt}
{com}. rename b3_q6                            caste   
{res}{txt}
{com}. rename b3_q5                            religion
{res}{txt}
{com}. rename b3_q4                            emp_type_hh1
{res}{txt}
{com}. rename b3_q17                           total_exp_hh    
{res}{txt}
{com}. rename wgt_combined                     weight
{res}{txt}
{com}. 
. * Destringing variables
. destring sector emp_type_hh1 religion caste total_exp_hh, replace
{txt}sector: all characters numeric; {res}replaced {txt}as {res}byte
{txt}emp_type_hh1: all characters numeric; {res}replaced {txt}as {res}byte
{txt}religion: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(3 missing values generated)
{res}{txt}caste: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(11 missing values generated)
{res}{txt}total_exp_hh already numeric; no {res}replace
{txt}
{com}. 
. * Making Monethly expenditure per capita. Currently it is at household level
. replace total_exp_hh = total_exp_hh/hh_size
{txt}variable {bf}{res}total_exp_hh{sf}{txt} was {bf}{res}long{sf}{txt} now {bf}{res}double{sf}
{txt}(117,097 real changes made)

{com}. 
. * Saving the dataset
. save "$nss_lab/intermediate/Blk_hc_2007", replace
{txt}{p 0 4 2}
file {bf}
C:/Users/ayush/Dropbox/idl/nss/labour/intermediate/Blk_hc_2007.dta{rm}
saved
{p_end}

{com}. 
. 
. 
{txt}end of do-file

{com}. * Purpose: To clean data at the household-level for 2009-10 (July-June)                  
. 
. *** ---------------------------------------------------------------------------
. *** Block 1-3: Household Records
. *** ---------------------------------------------------------------------------
. 
. * Loading the household level dataset
. use ///
>         HHID State Sector State_Region District /// Common variables
>         Religion Social_Group Household_Type HH_Size Land_Owned /// Native variables
>         WEIGHT ///      weight to attach while all sub-round combined estimation
>         using "$nss_lab/raw/2009/extracted dta files/Block_3_Household characteristics", clear
{txt}
{com}. 
. * Renaming variables
. rename *, lower 
{res}{txt}
{com}. rename hhid                                     hh_key
{res}{txt}
{com}. rename state                            st_code
{res}{txt}
{com}. rename district                         dist_code
{res}{txt}
{com}. rename state_region                     nss_region
{res}{txt}
{com}. rename social_group                     caste   
{res}{txt}
{com}. rename household_type           emp_type_hh1
{res}{txt}
{com}. 
. * Destringing variables
. destring sector emp_type_hh1 religion caste hh_size, replace
{txt}sector: all characters numeric; {res}replaced {txt}as {res}byte
{txt}emp_type_hh1: all characters numeric; {res}replaced {txt}as {res}byte
{txt}religion: all characters numeric; {res}replaced {txt}as {res}byte
{txt}caste: all characters numeric; {res}replaced {txt}as {res}byte
{txt}hh_size already numeric; no {res}replace
{txt}
{com}. 
. * Saving the dataset
. save "$nss_lab/intermediate/Blk_hc_2009", replace
{txt}{p 0 4 2}
file {bf}
C:/Users/ayush/Dropbox/idl/nss/labour/intermediate/Blk_hc_2009.dta{rm}
saved
{p_end}

{com}. 
. 
. 
. 
{txt}end of do-file

{com}. * Purpose: To clean data at the household-level for 2011-12 (July-June)                  
. 
. *** ---------------------------------------------------------------------------
. *** Block 1-3: Household Records
. *** ---------------------------------------------------------------------------
. 
. * Loading the household level dataset
. use ///
>         Sector State_Region District FSU_Serial_No Hamlet_Group_Sub_Block_No Second_Stage_Stratum_No Sample_Hhld_No /// Common variables
>         Religion Social_Group Household_Type HH_Size Land_Owned /// Native variables
>         Multiplier_comb /// Multiplier combined
>         using "$nss_lab/raw/2011/extracted dta files/Block_3_Household characteristics", clear
{txt}
{com}. 
. * Generating hh_key variable
. gegen hh_key = concat(FSU_Serial_No Hamlet_Group_Sub_Block_No Second_Stage_Stratum_No Sample_Hhld_No)
{txt}concat() is not a gtools function and no by(); falling back on egen

{com}. 
. drop FSU_Serial_No Hamlet_Group_Sub_Block_No Second_Stage_Stratum_No Sample_Hhld_No
{txt}
{com}. 
. * Renaming variables
. rename *, lower 
{res}{txt}
{com}. rename district                         dist_code
{res}{txt}
{com}. rename state_region                     nss_region
{res}{txt}
{com}. rename social_group                     caste   
{res}{txt}
{com}. rename household_type           emp_type_hh1
{res}{txt}
{com}. rename multiplier_comb          weight
{res}{txt}
{com}. 
. * Destringing variables
. destring sector emp_type_hh1 religion caste hh_size, replace
{txt}sector: all characters numeric; {res}replaced {txt}as {res}byte
{txt}emp_type_hh1: all characters numeric; {res}replaced {txt}as {res}byte
{txt}religion: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(1 missing value generated)
{res}{txt}caste: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(2 missing values generated)
{res}{txt}hh_size already numeric; no {res}replace
{txt}
{com}. 
. *Gnerating state code
. gen st_code = substr(nss_region, 1, 2) 
{txt}
{com}. 
. * Saving the dataset
. save "$nss_lab/intermediate/Blk_hc_2011", replace
{txt}{p 0 4 2}
file {bf}
C:/Users/ayush/Dropbox/idl/nss/labour/intermediate/Blk_hc_2011.dta{rm}
saved
{p_end}

{com}. 
. 
{txt}end of do-file

{com}. 
. * Clean Person Characterstics datasets of NSS Labour Surveys 
.         foreach yr in "1983" "1987" "1993" "1999" "2003" "2004" "2005" "2007" "2009" "2011" {c -(}
{txt}  2{com}.         
.                 do "$code/nss/nss_lab/01_2_`yr'_clean_pc.do"
{txt}  3{com}.         {c )-}
{txt}
{com}. * Purpose: To clean data at the person-level EUS for year 1983                           
. *
. 
. *** ---------------------------------------------------------------------------
. *** Block 6: Usual Activity Status
. *** ---------------------------------------------------------------------------
. 
. * Loading the dataset
. use ///
>         Hhold_key Person_key Sector State Region ///
>         B6_q2 B6_q6 B6_q5 B6_q6 ///
>         Wgt4_pooled /// Combined weight
>         using "$nss_lab/raw/1983/extracted dta files/Block-6-Persons-UsualActivity-records.dta", clear
{txt}
{com}. 
. * Standardize variable names to lowercase for consistency
. rename *, lower
{res}{txt}
{com}. 
. * Apply harmonized naming convention
. rename hhold_key                hh_key
{res}{txt}
{com}. rename state                    st_code
{res}{txt}
{com}. rename b6_q2                    act_code
{res}{txt}
{com}. rename b6_q5                    nic_1970_3d
{res}{txt}
{com}. rename b6_q6                    nco_1968_3d
{res}{txt}
{com}. rename wgt4_pooled              weight
{res}{txt}
{com}. 
. * Create 2-digit industry code from 3-digit NIC
. gen nic_1970_2d = substr(nic_1970_3d,1,2)
{txt}(398,928 missing values generated)

{com}. 
. sort hh_key person_key act_code
{txt}
{com}. 
. * Remove duplicate records per person_key
. bys person_key: gen dup = cond(_N==1,0,_n)
{txt}
{com}. drop if dup> 1
{txt}(48 observations deleted)

{com}. drop dup
{txt}
{com}. 
. * Clean industry codes:
. *   - Remove spaces
. *   - Drop records with placeholder characters (X, Y)
. *   - Drop invalid or incomplete codes
. replace nic_1970_3d = subinstr(nic_1970_3d, " ", "",.)
{txt}(5 real changes made)

{com}. drop if regexm(nic_1970_3d, "x") | regexm(nic_1970_3d, "X") | regexm(nic_1970_3d, "Y") 
{txt}(46 observations deleted)

{com}. drop if strlen(nic_1970_3d)!= 3 & !mi(nic_1970_3d) 
{txt}(75 observations deleted)

{com}. drop if nic_1970_3d == "000" 
{txt}(117,182 observations deleted)

{com}. 
. * Save as a temporary file for later merge
. tempfile pc
{txt}
{com}. save `pc'
{txt}{p 0 4 2}
file {bf}
C:\Users\ayush\AppData\Local\Temp\ST_33c0_000001.tmp{rm}
saved
as .dta format
{p_end}

{com}. 
. *** ---------------------------------------------------------------------------
. *** Block 4.1: Demographics and Weekly Activity Status
. *** ---------------------------------------------------------------------------
. 
. * Load Block 4.1 dataset (demographics + current weekly activity)
. use Hhold_key Person_key Person_slno Sector State Region Hhold_Slno ///
>         B41_q4 B41_q5 B41_q6 B41_q7 B41_q8 B41_q9 B41_q13 B41_q14 B41_q17 B41_q18 ///
>         Wgt4_pooled ///
>         using "$nss_lab/raw/1983/extracted dta files/Block-41-Persons-Demogrphic-weelyActivity-records.dta", clear
{txt}
{com}. 
. * Standardize variable names to lowercase
. rename *, lower
{res}{txt}
{com}. 
. * Apply harmonized naming convention
. rename hhold_key                hh_key
{res}{txt}
{com}. rename state                    st_code
{res}{txt}
{com}. rename person_slno              person_srl_no
{res}{txt}
{com}. rename b41_q4                   relation_to_head
{res}{txt}
{com}. rename b41_q5                   sex
{res}{txt}
{com}. rename b41_q6                   age
{res}{txt}
{com}. rename b41_q7                   marital_status
{res}{txt}
{com}. rename b41_q8                   gen_edu_raw
{res}{txt}
{com}. rename b41_q9                   tech_edu_raw
{res}{txt}
{com}. rename b41_q13                  reg_emp_exch
{res}{txt}
{com}. rename b41_q14                  cwa_status
{res}{txt}
{com}. rename b41_q17                  cwa_nic_1970_3d
{res}{txt}
{com}. rename b41_q18                  cwa_nco_1968_3d
{res}{txt}
{com}. rename wgt4_pooled              weight
{res}{txt}
{com}. 
. * Remove duplicate person records
. * (e.g., cases where same person_key has inconsistent demographic/activity data)
. bys person_key: gen dup = cond(_N==1,0,_n) // for the same person key there are different ages and current weekly activity status
{txt}
{com}. drop if dup> 1
{txt}(48 observations deleted)

{com}. drop dup
{txt}
{com}. 
. * Merge with Block 6 data (usual activity) using person_key
. merge 1:1 person_key using `pc'
{res}
{txt}{col 5}Result{col 33}Number of obs
{col 5}{hline 41}
{col 5}Not matched{col 30}{res}         117,303
{txt}{col 9}from master{col 30}{res}         117,303{txt}  (_merge==1)
{col 9}from using{col 30}{res}               0{txt}  (_merge==2)

{col 5}Matched{col 30}{res}         506,143{txt}  (_merge==3)
{col 5}{hline 41}

{com}. drop _merge
{txt}
{com}. 
. * Save merged Block 4.1 + Block 6 file for later steps
. tempfile demographics
{txt}
{com}. save `demographics'
{txt}{p 0 4 2}
file {bf}
C:\Users\ayush\AppData\Local\Temp\ST_33c0_000002.tmp{rm}
saved
as .dta format
{p_end}

{com}. 
. *** ---------------------------------------------------------------------------
. *** Block 5: Persons Daily Activity Records
. *** ---------------------------------------------------------------------------
. 
. * Load Block 5 dataset (daily activity details)
. use Sector State FSU_Slno Region Hhold_Slno Person_slno ///
>         Activity_slno B5_q3 B5_q6 B5_q7 B5_q8 B5_q9 B5_q10 B5_q11 B5_q12 B5_q13 B5_q14 B5_q15 B5_q16 ///
>         using "$nss_lab/raw/1983/extracted dta files/Block-5-Persons-DailyActivity-records.dta", clear
{txt}
{com}. 
. * Reconstruct person_key (as it is not properly generated in Block 5)
. gegen person_key = concat(Sector State Region FSU_Slno Hhold_Slno Person_slno)
{txt}concat() is not a gtools function and no by(); falling back on egen

{com}. 
. * Dropping variables which are not required after generating person key 
. drop Sector State Region FSU_Slno Hhold_Slno Person_slno
{txt}
{com}. 
. * Renaming variables
. rename *, lower
{res}{txt}
{com}. rename activity_slno    cda_srl_no
{res}{txt}
{com}. rename b5_q3                    cdas
{res}{txt}
{com}. rename b5_q6                    cda_seventh 
{res}{txt}
{com}. rename b5_q7                    cda_sixth
{res}{txt}
{com}. rename b5_q8                    cda_fifth
{res}{txt}
{com}. rename b5_q9                    cda_fourth
{res}{txt}
{com}. rename b5_q10                   cda_third
{res}{txt}
{com}. rename b5_q11                   cda_second
{res}{txt}
{com}. rename b5_q12                   cda_first
{res}{txt}
{com}. rename b5_q13                   cda_no_of_days
{res}{txt}
{com}. rename b5_q14                   wages_cash
{res}{txt}
{com}. rename b5_q15                   wages_kind                      
{res}{txt}
{com}. rename b5_q16                   wages_total
{res}{txt}
{com}. 
. * Remove duplicate person-key + activity-serial combinations
. bys person_key cda_srl_no: gen dup = cond(_N==1,0,_n) 
{txt}
{com}. drop if dup> 0 & cdas == ""
{txt}(226,977 observations deleted)

{com}. drop dup
{txt}
{com}. 
. * There are still some dupliactes remaining as there are different cdas for a given person key and activity serial number
. bys person_key cda_srl_no: gen dup = cond(_N==1,0,_n) 
{txt}
{com}. drop if dup> 1
{txt}(598 observations deleted)

{com}. drop dup
{txt}
{com}. 
. * Reshape daily activity data from long to wide format
. reshape wide cdas cda_seventh cda_sixth cda_fifth cda_fourth cda_third cda_second cda_first cda_no_of_days wages_cash wages_kind wages_total, i(person_key) j(cda_srl_no) string
{txt}(j = 0 1 2 3 4 5 6 7 8 9)

Data{col 36}Long{col 43}->{col 48}Wide
{hline 77}
Number of observations     {res}     318,623   {txt}->   {res}227,469     
{txt}Number of variables        {res}          14   {txt}->   {res}121         
{txt}j variable (10 values)       {res}cda_srl_no   {txt}->   (dropped)
xij variables:
                                   {res}cdas   {txt}->   {res}cdas0 cdas1 ... cdas9
                            cda_seventh   {txt}->   {res}cda_seventh0 cda_seventh1 ... cda_seventh9
                              cda_sixth   {txt}->   {res}cda_sixth0 cda_sixth1 ... cda_sixth9
                              cda_fifth   {txt}->   {res}cda_fifth0 cda_fifth1 ... cda_fifth9
                             cda_fourth   {txt}->   {res}cda_fourth0 cda_fourth1 ... cda_fourth9
                              cda_third   {txt}->   {res}cda_third0 cda_third1 ... cda_third9
                             cda_second   {txt}->   {res}cda_second0 cda_second1 ... cda_second9
                              cda_first   {txt}->   {res}cda_first0 cda_first1 ... cda_first9
                         cda_no_of_days   {txt}->   {res}cda_no_of_days0 cda_no_of_days1 ... cda_no_of_days9
                             wages_cash   {txt}->   {res}wages_cash0 wages_cash1 ... wages_cash9
                             wages_kind   {txt}->   {res}wages_kind0 wages_kind1 ... wages_kind9
                            wages_total   {txt}->   {res}wages_total0 wages_total1 ... wages_total9
{txt}{hline 77}

{com}. 
. * Merge daily activity with demographics + usual activity dataset
. merge 1:1 person_key using `demographics'
{res}
{txt}{col 5}Result{col 33}Number of obs
{col 5}{hline 41}
{col 5}Not matched{col 30}{res}         395,979
{txt}{col 9}from master{col 30}{res}               1{txt}  (_merge==1)
{col 9}from using{col 30}{res}         395,978{txt}  (_merge==2)

{col 5}Matched{col 30}{res}         227,468{txt}  (_merge==3)
{col 5}{hline 41}

{com}. drop if _merge == 1
{txt}(1 observation deleted)

{com}. drop _merge
{txt}
{com}. 
. * Save as temporary file for next merge
. tempfile cdas
{txt}
{com}. save `cdas'
{txt}{p 0 4 2}
file {bf}
C:\Users\ayush\AppData\Local\Temp\ST_33c0_000003.tmp{rm}
saved
as .dta format
{p_end}

{com}. 
. *** ---------------------------------------------------------------------------
. *** Block 7: Subsidiary Activity Status
. *** ---------------------------------------------------------------------------
. 
. * Load Block 7 dataset (subsidiary activity details)
. use Hhold_key Person_key Sector State Region ///
>         B7_q3 B7_q7 B7_q10 B7_q11 ///
>         Wgt4_pooled ///
>         using "$nss_lab/raw/1983/extracted dta files/Block-7-Persons-Notworking-subsidiary-activity-record.dta", clear
{txt}
{com}. 
. * Standardize variable names    
. rename *, lower
{res}{txt}
{com}. rename hhold_key                hh_key
{res}{txt}
{com}. rename state                    st_code
{res}{txt}
{com}. rename b7_q3                    act_code_sub 
{res}{txt}
{com}. rename b7_q7                    sub_act_status
{res}{txt}
{com}. rename b7_q10                   sub_nic_1970_3d
{res}{txt}
{com}. rename b7_q11                   sub_nco_1968_3d
{res}{txt}
{com}. rename wgt4_pooled              weight
{res}{txt}
{com}. 
. * Remove duplicate person records
. bys person_key: gen dup = cond(_N==1,0,_n)
{txt}
{com}. drop if dup> 1
{txt}(9 observations deleted)

{com}. drop dup
{txt}
{com}. 
. * Merge subsidiary activity with all prior merged person-level data
. merge 1:1 person_key using `cdas'
{res}
{txt}{col 5}Result{col 33}Number of obs
{col 5}{hline 41}
{col 5}Not matched{col 30}{res}         350,968
{txt}{col 9}from master{col 30}{res}               0{txt}  (_merge==1)
{col 9}from using{col 30}{res}         350,968{txt}  (_merge==2)

{col 5}Matched{col 30}{res}         272,478{txt}  (_merge==3)
{col 5}{hline 41}

{com}. drop _merge
{txt}
{com}. 
. *** ---------------------------------------------------------------------------
. *** Merge with Household Characteristics
. *** ---------------------------------------------------------------------------
. 
. * Merge household-level characteristics to person-level dataset
. merge m:1 hh_key using "$nss_lab/intermediate/Blk_hc_1983"
{res}
{txt}{col 5}Result{col 33}Number of obs
{col 5}{hline 41}
{col 5}Not matched{col 30}{res}             117
{txt}{col 9}from master{col 30}{res}              41{txt}  (_merge==1)
{col 9}from using{col 30}{res}              76{txt}  (_merge==2)

{col 5}Matched{col 30}{res}         623,405{txt}  (_merge==3)
{col 5}{hline 41}

{com}. 
. * Check number of mismatches
. count if _merge !=3     // 4,688 obs
  {res}117
{txt}
{com}. 
. * Drop households with no matching person record
. drop if _merge == 2 
{txt}(76 observations deleted)

{com}. 
. drop _merge 
{txt}
{com}. 
. * Generating year and round variable
. gen year = 1983
{txt}
{com}. gen nss_round = 38
{txt}
{com}. 
. * Generating nss region variable
. gegen nss_region = concat(st_code region)
{txt}concat() is not a gtools function and no by(); falling back on egen

{com}. 
. * Destringing variables
. destring sector emp_type_hh1 religion caste act_code_sub act_code, replace
{txt}sector: all characters numeric; {res}replaced {txt}as {res}byte
{txt}emp_type_hh1: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(41 missing values generated)
{res}{txt}religion: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(108 missing values generated)
{res}{txt}caste: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(95 missing values generated)
{res}{txt}act_code_sub: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(351006 missing values generated)
{res}{txt}act_code: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(117303 missing values generated)
{res}{txt}
{com}. 
. *** ---------------------------------------------------------------------------
. *** Save Final Dataset
. *** ---------------------------------------------------------------------------
. 
. save "$nss_lab/intermediate/Blk_merged_1983.dta", replace
{txt}{p 0 4 2}
file {bf}
C:/Users/ayush/Dropbox/idl/nss/labour/intermediate/Blk_merged_1983.dta{rm}
saved
{p_end}

{com}. 
. 
{txt}end of do-file

{com}. * Purpose: To clean data at the person-level EUS for year 1987                       
. 
. *** ---------------------------------------------------------------------------
. *** Block 6: Usual Activity Status
. *** ---------------------------------------------------------------------------
. 
. * Loading the dataset
. use ///
>         Hhold_key Person_key District Sector State Region /// Common variables
>         B6_q2 B6_q5 B6_q6 B6_q7 B6_q8 B6_q11 B6_q12 B6_q13 Prsn_Slno /// Native variables
>         Wgt4_pooled /// Combined weight
>         using "$nss_lab/raw/1987/extracted dta files/Block-6--Persons-Usual-activity- migration-Records.dta", clear
{txt}
{com}. 
. * Renaming variables
. rename *, lower
{res}{txt}
{com}. rename hhold_key                hh_key
{res}{txt}
{com}. rename b6_q2                    act_code
{res}{txt}
{com}. rename prsn_slno                person_srl_no
{res}{txt}
{com}. rename district                 dist_code
{res}{txt}
{com}. rename state                    st_code
{res}{txt}
{com}. rename b6_q5                    nic_1970_3d
{res}{txt}
{com}. rename b6_q6                    nco_1968_3d
{res}{txt}
{com}. rename b6_q8                    sub_act_status
{res}{txt}
{com}. rename b6_q11                   sub_nic_1970_3d
{res}{txt}
{com}. rename b6_q12                   sub_nco_1968_3d
{res}{txt}
{com}. rename b6_q13                   work_location
{res}{txt}
{com}. rename b6_q7                    act_code_sub
{res}{txt}
{com}. rename wgt4_pooled              weight
{res}{txt}
{com}. 
. * Destringing variables
. destring act_code work_location act_code_sub work_location sector, replace
{txt}act_code: all characters numeric; {res}replaced {txt}as {res}byte
{txt}work_location: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(441773 missing values generated)
{res}{txt}act_code_sub: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(44 missing values generated)
{res}{txt}work_location already numeric; no {res}replace
{txt}sector: all characters numeric; {res}replaced {txt}as {res}byte
{txt}
{com}. 
. gen nic_1970_2d = substr(nic_1970_3d,1,2)
{txt}(431,508 missing values generated)

{com}. 
. * Generating nss region variable as it is not there in the dataset
. gegen nss_region = concat(st_code region)
{txt}concat() is not a gtools function and no by(); falling back on egen

{com}. 
. * Remove duplicate records per person_key
. bys person_key: gen dup = cond(_N==1,0,_n)
{txt}
{com}. drop if dup> 1
{txt}(44 observations deleted)

{com}. drop dup
{txt}
{com}. 
. drop if person_key == "" // dropping if person key is missing so data can be isid
{txt}(0 observations deleted)

{com}. 
. * Clean industry codes:
. *   - Remove spaces
. *   - Drop records with placeholder characters (X, Y)
. *   - Drop invalid or incomplete codes
. replace nic_1970_3d = subinstr(nic_1970_3d, " ", "",.)
{txt}(2 real changes made)

{com}. drop if regexm(nic_1970_3d, "x") | regexm(nic_1970_3d, "X") | regexm(nic_1970_3d, "Y") 
{txt}(247 observations deleted)

{com}. drop if strlen(nic_1970_3d)!= 3 & !mi(nic_1970_3d) 
{txt}(102 observations deleted)

{com}. drop if nic_1970_3d == "000" 
{txt}(116,847 observations deleted)

{com}. 
. 
. * Save as a temporary file for later merge
. tempfile pc
{txt}
{com}. save `pc'
{txt}{p 0 4 2}
file {bf}
C:\Users\ayush\AppData\Local\Temp\ST_33c0_000001.tmp{rm}
saved
as .dta format
{p_end}

{com}. 
. *** ---------------------------------------------------------------------------
. *** Block 4.1: Demographics and Weekly Activity Status
. *** ---------------------------------------------------------------------------
. 
. * Load Block 4 dataset (demographics + current weekly activity)
. use ///
>         Hhold_key Person_key State District Region Sector /// Common variables
>         B4_q4 B4_q5 B4_q6 B4_q7 B4_q8 B4_q11 B4_q14 B4_q15 /// Native variables
>         Wgt4_pooled /// Combined weight
>         using "$nss_lab/raw/1987/extracted dta files/Block-4-Persons-Demographic-current-weekly-activity- Records.dta", clear
{txt}
{com}. 
. *renaming variables
. rename *, lower
{res}{txt}
{com}. rename hhold_key                hh_key
{res}{txt}
{com}. rename state                    st_code
{res}{txt}
{com}. rename district                 dist_code
{res}{txt}
{com}. rename b4_q4                    sex
{res}{txt}
{com}. rename b4_q5                    age
{res}{txt}
{com}. rename b4_q6                    marital_status
{res}{txt}
{com}. rename b4_q7                    gen_edu_raw
{res}{txt}
{com}. rename b4_q8                    tech_edu_raw
{res}{txt}
{com}. rename b4_q11                   cwa_status
{res}{txt}
{com}. rename b4_q14                   cwa_nic_1970_3d
{res}{txt}
{com}. rename b4_q15                   cwa_nco_1968_3d
{res}{txt}
{com}. rename wgt4_pooled              weight
{res}{txt}
{com}. 
. destring sector, replace
{txt}sector: all characters numeric; {res}replaced {txt}as {res}byte
{txt}
{com}. 
. * Remove duplicate records per person_key
. bys person_key: gen dup = cond(_N==1,0,_n)
{txt}
{com}. drop if dup> 1
{txt}(44 observations deleted)

{com}. drop dup
{txt}
{com}. 
. * Generating nss region variable
. gegen nss_region = concat(st_code region)
{txt}concat() is not a gtools function and no by(); falling back on egen

{com}. 
. * Merge with Block 6 data (usual activity) using person_key
. merge 1:m person_key using `pc'
{res}
{txt}{col 5}Result{col 33}Number of obs
{col 5}{hline 41}
{col 5}Not matched{col 30}{res}         117,200
{txt}{col 9}from master{col 30}{res}         117,198{txt}  (_merge==1)
{col 9}from using{col 30}{res}               2{txt}  (_merge==2)

{col 5}Matched{col 30}{res}         550,606{txt}  (_merge==3)
{col 5}{hline 41}

{com}. drop _merge
{txt}
{com}. 
. 
. * Save merged Block 4.1 + Block 6 file for later steps
. tempfile demographics
{txt}
{com}. save `demographics'
{txt}{p 0 4 2}
file {bf}
C:\Users\ayush\AppData\Local\Temp\ST_33c0_000002.tmp{rm}
saved
as .dta format
{p_end}

{com}. 
. *** ---------------------------------------------------------------------------
. *** Block 5: Persons Daily Activity Records
. *** ---------------------------------------------------------------------------
. 
. * Load Block 5 dataset (daily activity details)
. use ///
>         Hhold_key Person_key /// Common variables
>         Activity_Slno B5_q3 B5_q6 B5_q7 B5_q8 B5_q9 B5_q10 B5_q11 B5_q12 B5_q13 B5_q14 B5_q15 B5_q16 /// Native variables
>         using "$nss_lab/raw/1987/extracted dta files/Block-5-Persons-Daily- activity- time-disposion-Records.dta", clear
{txt}
{com}. 
. * Renaming variables
. rename *, lower
{res}{txt}
{com}. rename activity_slno    cda_srl_no
{res}{txt}
{com}. rename b5_q3                    cdas
{res}{txt}
{com}. rename b5_q6                    cda_seventh 
{res}{txt}
{com}. rename b5_q7                    cda_sixth
{res}{txt}
{com}. rename b5_q8                    cda_fifth
{res}{txt}
{com}. rename b5_q9                    cda_fourth
{res}{txt}
{com}. rename b5_q10                   cda_third
{res}{txt}
{com}. rename b5_q11                   cda_second
{res}{txt}
{com}. rename b5_q12                   cda_first
{res}{txt}
{com}. rename b5_q13                   cda_no_of_days
{res}{txt}
{com}. rename b5_q14                   wages_cash
{res}{txt}
{com}. rename b5_q15                   wages_kind                      
{res}{txt}
{com}. rename b5_q16                   wages_total
{res}{txt}
{com}. 
. * Remove duplicate records per person_key
. bys person_key cda_srl_no: gen dup = cond(_N==1,0,_n) // for the same person key there are different ages and current daily activity status
{txt}
{com}. drop if dup> 0 & cdas == ""
{txt}(240,687 observations deleted)

{com}. drop dup
{txt}
{com}. 
. * There are still some duplicates remaining as there are different cdas for a given person key and activity serial number
. bys person_key cda_srl_no: gen dup = cond(_N==1,0,_n) // for the same person key there are different ages and current daily activity status
{txt}
{com}. drop if dup> 1
{txt}(859 observations deleted)

{com}. drop dup
{txt}
{com}. 
. * Reshape daily activity data from long to wide format
. reshape wide cdas cda_seventh cda_sixth cda_fifth cda_fourth cda_third cda_second cda_first cda_no_of_days wages_cash wages_kind wages_total, i(person_key) j(cda_srl_no) string
{txt}(j = 0 1 2 3 4 5 6 7 8 9)

Data{col 36}Long{col 43}->{col 48}Wide
{hline 77}
Number of observations     {res}     764,317   {txt}->   {res}668,005     
{txt}Number of variables        {res}          15   {txt}->   {res}122         
{txt}j variable (10 values)       {res}cda_srl_no   {txt}->   (dropped)
xij variables:
                                   {res}cdas   {txt}->   {res}cdas0 cdas1 ... cdas9
                            cda_seventh   {txt}->   {res}cda_seventh0 cda_seventh1 ... cda_seventh9
                              cda_sixth   {txt}->   {res}cda_sixth0 cda_sixth1 ... cda_sixth9
                              cda_fifth   {txt}->   {res}cda_fifth0 cda_fifth1 ... cda_fifth9
                             cda_fourth   {txt}->   {res}cda_fourth0 cda_fourth1 ... cda_fourth9
                              cda_third   {txt}->   {res}cda_third0 cda_third1 ... cda_third9
                             cda_second   {txt}->   {res}cda_second0 cda_second1 ... cda_second9
                              cda_first   {txt}->   {res}cda_first0 cda_first1 ... cda_first9
                         cda_no_of_days   {txt}->   {res}cda_no_of_days0 cda_no_of_days1 ... cda_no_of_days9
                             wages_cash   {txt}->   {res}wages_cash0 wages_cash1 ... wages_cash9
                             wages_kind   {txt}->   {res}wages_kind0 wages_kind1 ... wages_kind9
                            wages_total   {txt}->   {res}wages_total0 wages_total1 ... wages_total9
{txt}{hline 77}

{com}. 
. * Merge daily activity with demographics + usual activity dataset
. merge 1:1 person_key using `demographics'
{res}
{txt}{col 5}Result{col 33}Number of obs
{col 5}{hline 41}
{col 5}Not matched{col 30}{res}             215
{txt}{col 9}from master{col 30}{res}             207{txt}  (_merge==1)
{col 9}from using{col 30}{res}               8{txt}  (_merge==2)

{col 5}Matched{col 30}{res}         667,798{txt}  (_merge==3)
{col 5}{hline 41}

{com}. drop if _merge == 1
{txt}(207 observations deleted)

{com}. drop _merge
{txt}
{com}. 
. *** ---------------------------------------------------------------------------
. *** Merge with Household Characteristics
. *** ---------------------------------------------------------------------------
. 
. * Merge household-level characteristics to person-level dataset
. merge m:1 hh_key using "$nss_lab/intermediate/Blk_hc_1987"
{res}
{txt}{col 5}Result{col 33}Number of obs
{col 5}{hline 41}
{col 5}Not matched{col 30}{res}             159
{txt}{col 9}from master{col 30}{res}              34{txt}  (_merge==1)
{col 9}from using{col 30}{res}             125{txt}  (_merge==2)

{col 5}Matched{col 30}{res}         667,772{txt}  (_merge==3)
{col 5}{hline 41}

{com}. 
. count if _merge !=3     
  {res}159
{txt}
{com}. //assert `r(N)' < 10 // to ensure quality of merge
. 
. drop if _merge == 2 // dropping all those hh keys which dont have person key (125 obs dropped)
{txt}(125 observations deleted)

{com}. *34 observations dont have hh characteristics and certain demographics like religion against them 
. drop _merge 
{txt}
{com}. 
. * Generating year and round variable
. gen year = 1987
{txt}
{com}. gen nss_round = 43
{txt}
{com}. 
. *** ---------------------------------------------------------------------------
. *** Save Final Dataset
. *** ---------------------------------------------------------------------------
. 
. save "$nss_lab/intermediate/Blk_merged_1987.dta", replace
{txt}{p 0 4 2}
file {bf}
C:/Users/ayush/Dropbox/idl/nss/labour/intermediate/Blk_merged_1987.dta{rm}
saved
{p_end}

{com}. 
. 
. 
. 
. 
{txt}end of do-file

{com}. * Purpose: To clean data at the person-level EUS for year 1993                    
. 
. *** ---------------------------------------------------------------------------
. *** Block 4: Demographics and Usual Activity Status
. *** ---------------------------------------------------------------------------
. 
. * Loading the dataset
. use ///
>         Hhold_Key Person_key Prsn_slno B4_q3  /// Common variables
>         B4_q4 B4_q5 B4_q11 B4_q12 B4_q14 B4_q15 B4_q16 B4_q17 B4_q23 /// Native variables 
>         WGT_Pooled                                                                 /// Weight variable- Sub-round pooled/sub-samples combined(0.00) 
>         using "$nss_lab/raw/1993/extracted dta files/Block-4-Persons-Records", clear
{txt}
{com}. 
. * Renaming variables
. rename *, lower
{res}{txt}
{com}. rename hhold_key        hh_key
{res}{txt}
{com}. rename prsn_slno        person_srl_no
{res}{txt}
{com}. rename b4_q3            relation_to_head
{res}{txt}
{com}. rename b4_q4            sex
{res}{txt}
{com}. rename b4_q5            age 
{res}{txt}
{com}. rename b4_q11           skill
{res}{txt}
{com}. rename b4_q12           act_code
{res}{txt}
{com}. rename b4_q14           nic_1987_3d
{res}{txt}
{com}. rename b4_q15           nco_1968_3d
{res}{txt}
{com}. rename b4_q16           work_location
{res}{txt}
{com}. rename b4_q17           act_code_sub
{res}{txt}
{com}. rename b4_q23           seeking_work
{res}{txt}
{com}. rename wgt_pooled       weight
{res}{txt}
{com}. 
. * Destringing variables
. destring age skill act_code work_location act_code_sub seeking_work work_location, replace
{txt}age already numeric; no {res}replace
{txt}skill: all characters numeric; {res}replaced {txt}as {res}byte
{txt}act_code: all characters numeric; {res}replaced {txt}as {res}byte
{txt}work_location: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(362391 missing values generated)
{res}{txt}act_code_sub: all characters numeric; {res}replaced {txt}as {res}byte
{txt}seeking_work: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(8523 missing values generated)
{res}{txt}work_location already numeric; no {res}replace
{txt}
{com}. 
. * Generating industry at the 2-digit level
. gen nic_1987_2d = substr(nic_1987_3d,1,2)
{txt}(362,463 missing values generated)

{com}. 
. * Clean industry codes:
. *   - Remove spaces
. *   - Drop records with placeholder characters (X, Y)
. *   - Drop invalid or incomplete codes
. replace nic_1987_3d = subinstr(nic_1987_3d, " ", "",.)
{txt}(1 real change made)

{com}. drop if regexm(nic_1987_3d, "x") | regexm(nic_1987_3d, "X") | regexm(nic_1987_3d, "Y") 
{txt}(251 observations deleted)

{com}. drop if strlen(nic_1987_3d)!= 3 & !mi(nic_1987_3d) 
{txt}(117 observations deleted)

{com}. 
. * Save as a temporary file for later merge
. tempfile pc
{txt}
{com}. save `pc'
{txt}{p 0 4 2}
file {bf}
C:\Users\ayush\AppData\Local\Temp\ST_33c0_000001.tmp{rm}
saved
as .dta format
{p_end}

{com}. 
. *** ---------------------------------------------------------------------------
. *** Block 5: Weekly Activity Status
. *** ---------------------------------------------------------------------------
. 
. * Load Block 4 dataset (demographics + current weekly activity)
. use ///
>         Hhold_Key Prsn_slno  /// Common variables
>         B5_q3 B5_q4 B5_q3 B5_q7 B5_q8 B5_q9 B5_q10 B5_q11 B5_q12 B5_q13 B5_q14 B5_q15 B5_q16 B5_q17 B5_q19 B5_q20 B5_q21 /// Native variables 
>         using "$nss_lab/raw/1993/extracted dta files/Block-5-Persons-Activity-Records", clear
{txt}
{com}. 
. * This block does not have person key so it is constructed by concatenating hhold key and perosn serial number 
. gegen person_key = concat(Hhold_Key Prsn_slno)
{txt}concat() is not a gtools function and no by(); falling back on egen

{com}. 
. drop Hhold_Key Prsn_slno
{txt}
{com}. 
. * Renaming variables
. rename *, lower
{res}{txt}
{com}. rename b5_q3                    cda_srl_no
{res}{txt}
{com}. rename b5_q4                    cdas
{res}{txt}
{com}. rename b5_q7                    cda_seventh 
{res}{txt}
{com}. rename b5_q8                    cda_sixth
{res}{txt}
{com}. rename b5_q9                    cda_fifth
{res}{txt}
{com}. rename b5_q10                   cda_fourth
{res}{txt}
{com}. rename b5_q11                   cda_third
{res}{txt}
{com}. rename b5_q12                   cda_second
{res}{txt}
{com}. rename b5_q13                   cda_first
{res}{txt}
{com}. rename b5_q14                   cda_no_of_days
{res}{txt}
{com}. rename b5_q15                   wages_cash
{res}{txt}
{com}. rename b5_q16                   wages_kind                      
{res}{txt}
{com}. rename b5_q17                   wages_total
{res}{txt}
{com}. rename b5_q19                   cwa_status
{res}{txt}
{com}. rename b5_q20                   cwa_nic_1987_3d
{res}{txt}
{com}. rename b5_q21                   cwa_nco_1968_3d
{res}{txt}
{com}. 
. * Assert to check that they are constant across person key
. foreach var of varlist cwa_status cwa_nic_1987_3d cwa_nco_1968_3d {c -(}
{txt}  2{com}.         bysort person_key: assert `var' == `var'[1]
{txt}  3{com}. {c )-}
{txt}
{com}. 
. * Dropping person_key duplicates
. bys person_key cda_srl_no (cdas): gen dup = cond(_N==1,0,_n) // for the same person key there are different ages and current daily activity status
{txt}
{com}. drop if dup> 0 & cdas == "" // Dropping only if there is no code for current daily activity status code
{txt}(227,265 observations deleted)

{com}. drop dup
{txt}
{com}. 
. *there are still some dupliactes remaining as there are different cdas for a given person key and activity serial number
. bys person_key cda_srl_no: gen dup = cond(_N==1,0,_n) // for the same person key there are different ages and current daily activity status
{txt}
{com}. drop if dup> 1
{txt}(2 observations deleted)

{com}. drop dup
{txt}
{com}. 
. * Reshape daily activity data from long to wide format
. reshape wide cdas cda_seventh cda_sixth cda_fifth cda_fourth cda_third cda_second cda_first cda_no_of_days wages_cash wages_kind wages_total, i(person_key cwa_status cwa_nic_1987_3d cwa_nco_1968_3d) j(cda_srl_no) string
{txt}(j = 0 1 2 3 4 5 6 7 8 9)

Data{col 36}Long{col 43}->{col 48}Wide
{hline 77}
Number of observations     {res}     647,513   {txt}->   {res}564,740     
{txt}Number of variables        {res}          17   {txt}->   {res}124         
{txt}j variable (10 values)       {res}cda_srl_no   {txt}->   (dropped)
xij variables:
                                   {res}cdas   {txt}->   {res}cdas0 cdas1 ... cdas9
                            cda_seventh   {txt}->   {res}cda_seventh0 cda_seventh1 ... cda_seventh9
                              cda_sixth   {txt}->   {res}cda_sixth0 cda_sixth1 ... cda_sixth9
                              cda_fifth   {txt}->   {res}cda_fifth0 cda_fifth1 ... cda_fifth9
                             cda_fourth   {txt}->   {res}cda_fourth0 cda_fourth1 ... cda_fourth9
                              cda_third   {txt}->   {res}cda_third0 cda_third1 ... cda_third9
                             cda_second   {txt}->   {res}cda_second0 cda_second1 ... cda_second9
                              cda_first   {txt}->   {res}cda_first0 cda_first1 ... cda_first9
                         cda_no_of_days   {txt}->   {res}cda_no_of_days0 cda_no_of_days1 ... cda_no_of_days9
                             wages_cash   {txt}->   {res}wages_cash0 wages_cash1 ... wages_cash9
                             wages_kind   {txt}->   {res}wages_kind0 wages_kind1 ... wages_kind9
                            wages_total   {txt}->   {res}wages_total0 wages_total1 ... wages_total9
{txt}{hline 77}

{com}. 
. * Merge daily activity with demographics + usual activity dataset
. merge 1:1 person_key using `pc'
{res}
{txt}{col 5}Result{col 33}Number of obs
{col 5}{hline 41}
{col 5}Not matched{col 30}{res}             370
{txt}{col 9}from master{col 30}{res}             369{txt}  (_merge==1)
{col 9}from using{col 30}{res}               1{txt}  (_merge==2)

{col 5}Matched{col 30}{res}         564,371{txt}  (_merge==3)
{col 5}{hline 41}

{com}. drop _merge
{txt}
{com}. 
. *** ---------------------------------------------------------------------------
. *** Merge with Household Characteristics
. *** ---------------------------------------------------------------------------
. 
. * Merge household-level characteristics to person-level dataset
. merge m:1 hh_key using "$nss_lab/intermediate/Blk_hc_1993"
{res}
{txt}{col 5}Result{col 33}Number of obs
{col 5}{hline 41}
{col 5}Not matched{col 30}{res}             394
{txt}{col 9}from master{col 30}{res}             372{txt}  (_merge==1)
{col 9}from using{col 30}{res}              22{txt}  (_merge==2)

{col 5}Matched{col 30}{res}         564,369{txt}  (_merge==3)
{col 5}{hline 41}

{com}. count if _merge !=3
  {res}394
{txt}
{com}. //qui assert `r(N)' < 10 // to ensure quality of merge
. drop _merge 
{txt}
{com}. 
. /*
>            Result                      Number of obs
>            -----------------------------------------
>            Not matched                             3
>            from master                         3  (_merge==1)
>            from using                           0  (_merge==2)
> 
>            Matched                           564,369  (_merge==3)
> */ 
. 
. 
. save "$nss_lab/intermediate/Blk_merged_1993.dta", replace
{txt}{p 0 4 2}
file {bf}
C:/Users/ayush/Dropbox/idl/nss/labour/intermediate/Blk_merged_1993.dta{rm}
saved
{p_end}

{com}. 
. 
. 
. 
{txt}end of do-file

{com}. * Purpose: To clean data at the person-level EUS for year 1999                      
. 
. 
. *** ---------------------------------------------------------------------------
. *** Block 5: Usual Activity Status
. *** ---------------------------------------------------------------------------
. 
. * Loading the dataset
. use ///
>         Key_hhold Key_prsn FileID sub_round Sub_sample fsu_no visit_no seg_no Stg2_stratm Hhold_Slno Prsn_slno_B51_q1 /// Common variables
>         B51_q2 B51_q3 B51_q5 B51_q6 B51_q7 B51_q8 B51_q9 B51_q10 B51_q13 B51_q19 B51_q20 /// Native variables
>         Wgt_SR_comb /// Multiplier Subround Combined(generated)
>         using "$nss_lab/raw/1999/extracted dta files/Block51-sch10-Persons-usual-principal-activity-Records", clear
{txt}
{com}. 
. //gegen hh_key = concat(sub_round Sub_sample fsu_no visit_no seg_no Stg2_stratm Hhold_Slno)
. 
. * Dropping irrelevant variables
. drop sub_round Sub_sample fsu_no visit_no seg_no Stg2_stratm Hhold_Slno Prsn_slno_B51_q1
{txt}
{com}. 
. * Renaming variables
. rename *, lower
{res}{txt}
{com}. rename key_hhold                hh_key
{res}{txt}
{com}. rename key_prsn                 person_key
{res}{txt}
{com}. rename fileid                   block
{res}{txt}
{com}. rename b51_q2                   age                                     
{res}{txt}
{com}. rename b51_q3                   act_code 
{res}{txt}
{com}. rename b51_q5                   nic_1998_5d
{res}{txt}
{com}. rename b51_q6                   nco_1968_3d
{res}{txt}
{com}. rename b51_q7                   act_code_sub
{res}{txt}
{com}. rename b51_q8                   sub_act_no
{res}{txt}
{com}. rename b51_q9                   work_location
{res}{txt}
{com}. rename b51_q10                  ent_type
{res}{txt}
{com}. rename b51_q13                  use_electricty
{res}{txt}
{com}. rename b51_q19                  skill
{res}{txt}
{com}. rename b51_q20                  seeking_work
{res}{txt}
{com}. rename wgt_sr_comb              weight
{res}{txt}
{com}. 
. * Destringing variables
. destring age skill act_code work_location act_code_sub sub_act_no seeking_work work_location ent_type, replace
{txt}age already numeric; no {res}replace
{txt}skill: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(409805 missing values generated)
{res}{txt}act_code: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(1157 missing values generated)
{res}{txt}work_location: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(492548 missing values generated)
{res}{txt}act_code_sub: all characters numeric; {res}replaced {txt}as {res}byte
{txt}sub_act_no: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(538084 missing values generated)
{res}{txt}seeking_work: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(7544 missing values generated)
{res}{txt}work_location already numeric; no {res}replace
{txt}ent_type: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(492548 missing values generated)
{res}{txt}
{com}. 
. gen nic_1998_4d = substr(nic_1998_5d,1,4)
{txt}(386,856 missing values generated)

{com}. gen nic_1998_3d = substr(nic_1998_5d,1,3)
{txt}(386,856 missing values generated)

{com}. 
. * Generating hh_size variable
. bysort hh_key: gen hh_size = _N
{txt}
{com}. 
. gisid person_key
{res}{txt}
{com}. 
. * Save as a temporary file for later merge
. tempfile pc
{txt}
{com}. save `pc'
{txt}{p 0 4 2}
file {bf}
C:\Users\ayush\AppData\Local\Temp\ST_33c0_000001.tmp{rm}
saved
as .dta format
{p_end}

{com}. 
. *** ---------------------------------------------------------------------------
. *** Block 4: Demographics and Weekly Activity Status
. *** ---------------------------------------------------------------------------
. 
. * Load Block 4 dataset (demographics + current weekly activity)
. use ///
>         key_Hhold key_prsn RecID sub_round Sub_sample fsu_no visit_no seg_no Stg2_stratm Hhold_Slno Prsn_Slno_B4_q1 /// Common variables
>         B4_q3 B4_q4 B4_q6 B4_q7 B4_q8 B4_q10 /// Native variables
>         Wgt_SR_Comb /// Multiplier Subround Combined(generated)
>         using "$nss_lab/raw/1999/extracted dta files/Block4-sch10-persons-demographic-migration-records", clear
{txt}
{com}. 
. * Dropping irrelevant variables
. drop RecID sub_round Sub_sample fsu_no visit_no seg_no Stg2_stratm Hhold_Slno 
{txt}
{com}. 
. * Renaming variables
. rename *, lower
{res}{txt}
{com}. rename key_hhold                hh_key
{res}{txt}
{com}. rename key_prsn                 person_key
{res}{txt}
{com}. rename prsn_slno_b4_q1  person_srl_no
{res}{txt}
{com}. rename b4_q3                    relation_to_head
{res}{txt}
{com}. rename b4_q4                    sex 
{res}{txt}
{com}. rename b4_q6                    marital_status
{res}{txt}
{com}. rename b4_q7                    gen_edu_raw
{res}{txt}
{com}. rename b4_q8                    tech_edu_raw
{res}{txt}
{com}. rename b4_q10                   reg_emp_exch
{res}{txt}
{com}. rename wgt_sr_comb              weight
{res}{txt}
{com}. 
. * Merge with Block 6 data (usual activity) using person_key
. merge 1:1 person_key using `pc'
{res}
{txt}{col 5}Result{col 33}Number of obs
{col 5}{hline 41}
{col 5}Not matched{col 30}{res}               0
{txt}{col 5}Matched{col 30}{res}         596,686{txt}  (_merge==3)
{col 5}{hline 41}

{com}. drop _merge
{txt}
{com}. 
. /*
>            Result                      Number of obs
>            -----------------------------------------
>            Not matched                             0
>            Matched                           596,686  (_merge==3)
>            -----------------------------------------
> */
. 
. * Save merged Block 4.1 + Block 6 file for later steps
. tempfile demographics
{txt}
{com}. save `demographics'
{txt}{p 0 4 2}
file {bf}
C:\Users\ayush\AppData\Local\Temp\ST_33c0_000002.tmp{rm}
saved
as .dta format
{p_end}

{com}. 
. *** ---------------------------------------------------------------------------
. *** Block 5: Persons Daily Activity Records
. *** ---------------------------------------------------------------------------
. 
. * Load Block 5 dataset (daily activity details)
. use ///
>         Key_PRSN /// Common variables
>         B53_q3 B53_q4 B53_q14 B53_q15 B53_q16 B53_q17 B53_q20 B53_q21 B53_q22 /// Native variables
>         using "$nss_lab/raw/1999/extracted dta files/Block53-sch10-Persons-daily-activity-time-disposition-Records", clear
{txt}
{com}. 
. * Renaming variables
. rename *, lower
{res}{txt}
{com}. rename key_prsn                 person_key_old
{res}{txt}
{com}. rename b53_q3                   cda_srl_no
{res}{txt}
{com}. rename b53_q4                   cdas
{res}{txt}
{com}. rename b53_q14                  cda_no_of_days
{res}{txt}
{com}. rename b53_q15                  wages_cash
{res}{txt}
{com}. rename b53_q16                  wages_kind                      
{res}{txt}
{com}. rename b53_q17                  wages_total
{res}{txt}
{com}. rename b53_q20                  cwa_status
{res}{txt}
{com}. rename b53_q21                  cwa_nic_1998_5d
{res}{txt}
{com}. rename b53_q22                  cwa_nco_1968_3d
{res}{txt}
{com}. 
. gen person_key = substr(person_key_old, 3, .) // in this block we have two characters extra in the beginning of the person key, so we remove them 
{txt}
{com}. 
. drop person_key_old
{txt}
{com}. 
. gduplicates drop

{p 0 4}{txt}Duplicates in terms of {txt} all variables{p_end}
{res}{txt}{res}
{txt}(1,394 observations deleted)

{com}. 
. * Assert to check that they are constant across person key
. foreach var of varlist cwa_status cwa_nic_1998_5d cwa_nco_1968_3d {c -(}
{txt}  2{com}.         bysort person_key: assert `var' == `var'[1]
{txt}  3{com}. {c )-}
{txt}
{com}. 
. destring cda_srl_no, replace
{txt}cda_srl_no: all characters numeric; {res}replaced {txt}as {res}byte
{txt}
{com}. tostring cda_srl_no, replace
{txt}cda_srl_no was {res:byte} now {res:str1}

{com}. 
. * Remove duplicate person-key + activity-serial combinations
. bys person_key cda_srl_no (cdas): gen dup = cond(_N==1,0,_n) // this includes cdas == 97 which we drop 
{txt}
{com}. drop if dup > 1
{txt}(6 observations deleted)

{com}. drop dup
{txt}
{com}. 
. * Reshape daily activity data from long to wide format
. reshape wide cdas cda_no_of_days wages_cash wages_kind wages_total, i(person_key cwa_status cwa_nic_1998_5d cwa_nco_1968_3d) j(cda_srl_no) string
{txt}(j = 1 2 3 4 5)

Data{col 36}Long{col 43}->{col 48}Wide
{hline 77}
Number of observations     {res}     699,532   {txt}->   {res}595,529     
{txt}Number of variables        {res}          10   {txt}->   {res}29          
{txt}j variable (5 values)        {res}cda_srl_no   {txt}->   (dropped)
xij variables:
                                   {res}cdas   {txt}->   {res}cdas1 cdas2 ... cdas5
                         cda_no_of_days   {txt}->   {res}cda_no_of_days1 cda_no_of_days2 ... cda_no_of_days5
                             wages_cash   {txt}->   {res}wages_cash1 wages_cash2 ... wages_cash5
                             wages_kind   {txt}->   {res}wages_kind1 wages_kind2 ... wages_kind5
                            wages_total   {txt}->   {res}wages_total1 wages_total2 ... wages_total5
{txt}{hline 77}

{com}. 
. * Merge daily activity with demographics + usual activity dataset
. merge 1:1 person_key using `demographics'
{res}
{txt}{col 5}Result{col 33}Number of obs
{col 5}{hline 41}
{col 5}Not matched{col 30}{res}           1,157
{txt}{col 9}from master{col 30}{res}               0{txt}  (_merge==1)
{col 9}from using{col 30}{res}           1,157{txt}  (_merge==2)

{col 5}Matched{col 30}{res}         595,529{txt}  (_merge==3)
{col 5}{hline 41}

{com}. drop if _merge == 1
{txt}(0 observations deleted)

{com}. drop _merge
{txt}
{com}. 
. *** ---------------------------------------------------------------------------
. *** Merge with Household Characteristics
. *** ---------------------------------------------------------------------------
. 
. * Merge household-level characteristics to person-level dataset
. merge m:1 hh_key using "$nss_lab/intermediate/Blk_hc_1999"
{res}
{txt}{col 5}Result{col 33}Number of obs
{col 5}{hline 41}
{col 5}Not matched{col 30}{res}               0
{txt}{col 5}Matched{col 30}{res}         596,686{txt}  (_merge==3)
{col 5}{hline 41}

{com}. count if _merge !=3
  {res}0
{txt}
{com}. qui assert `r(N)' < 10 // to ensure quality of merge
{txt}
{com}. drop _merge 
{txt}
{com}. 
. * Clean industry codes:
. *   - Remove spaces
. *   - Drop records with placeholder characters (X, Y)
. *   - Drop invalid or incomplete codes
. replace nic_1998_5d = subinstr(nic_1998_5d, " ", "",.)
{txt}(0 real changes made)

{com}. drop if regexm(nic_1998_5d, "x") | regexm(nic_1998_5d, "X") | regexm(nic_1998_5d, "Y") // 0 obs deleted
{txt}(0 observations deleted)

{com}. drop if strlen(nic_1998_5d)!= 5 & !mi(nic_1998_5d) // 0 obs dropped
{txt}(0 observations deleted)

{com}. drop if nic_1998_5d == "00000" 
{txt}(0 observations deleted)

{com}. 
. * In the documentation this is how we use the weights
. replace weight  = weight/4      
{txt}(596,686 real changes made)

{com}. 
. * Generating year and round variable 
. gen year = 1999
{txt}
{com}. gen nss_round = 55
{txt}
{com}. 
. *** ---------------------------------------------------------------------------
. *** Save Final Dataset
. *** ---------------------------------------------------------------------------
. 
. save "$nss_lab/intermediate/Blk_merged_1999.dta", replace
{txt}{p 0 4 2}
file {bf}
C:/Users/ayush/Dropbox/idl/nss/labour/intermediate/Blk_merged_1999.dta{rm}
saved
{p_end}

{com}. 
. 
. 
{txt}end of do-file

{com}. * Purpose: To clean data at the person-level EUS for year 2004 (Jan-June)                  
. 
. 
. *** ---------------------------------------------------------------------------
. *** Block 4: Demographics + Usual Activity Status
. *** ---------------------------------------------------------------------------
. 
. * Loading the dataset
. use ///
>         Key_memb Key_hhold Record_id /// Common variables
>         B4_c1 B4_c3 B4_c4 B4_c5 B4_c6 B4_c7 B4_c8 B4_c9 B4_c10 B4_c11 B4_c12 B4_c13 B4_c14 B4_c15 /// Native variables
>         wgt_combined /// Multiplier combined
>         using "$nss_lab/raw/2003/extracted dta files/Block-4-Demographic-usual- activity-members-records", clear
{txt}
{com}. 
. 
. * Apply harmonized naming convention
. rename *, lower
{res}{txt}
{com}. rename key_memb                 person_key
{res}{txt}
{com}. rename key_hhold                hh_key
{res}{txt}
{com}. rename record_id                block
{res}{txt}
{com}. rename b4_c1                    person_srl_no
{res}{txt}
{com}. rename b4_c3                    relation_to_head 
{res}{txt}
{com}. rename b4_c4                    sex
{res}{txt}
{com}. rename b4_c5                    age 
{res}{txt}
{com}. rename b4_c6                    marital_status 
{res}{txt}
{com}. rename b4_c7                    gen_edu_raw
{res}{txt}
{com}. rename b4_c8                    tech_edu_raw
{res}{txt}
{com}. rename b4_c9                    act_code
{res}{txt}
{com}. rename b4_c10                   nic_1998_5d
{res}{txt}
{com}. rename b4_c11                   nco_1968_3d
{res}{txt}
{com}. rename b4_c12                   act_code_sub 
{res}{txt}
{com}. rename b4_c13                   sub_act_status 
{res}{txt}
{com}. rename b4_c14                   sub_nic_1998_5d 
{res}{txt}
{com}. rename b4_c15                   sub_nco_1968_3d 
{res}{txt}
{com}. rename wgt_combined             weight
{res}{txt}
{com}. 
. * Destringing variables
. destring age act_code act_code_sub, replace
{txt}age already numeric; no {res}replace
{txt}act_code: all characters numeric; {res}replaced {txt}as {res}byte
{txt}act_code_sub: all characters numeric; {res}replaced {txt}as {res}byte
{txt}
{com}. 
. gen nic_1998_4d = substr(nic_1998_5d,1,4)
{txt}(194,183 missing values generated)

{com}. gen nic_1998_3d = substr(nic_1998_5d,1,3)
{txt}(194,183 missing values generated)

{com}. 
. * Save as a temporary file for later merge
. tempfile demographics
{txt}
{com}. save `demographics'
{txt}{p 0 4 2}
file {bf}
C:\Users\ayush\AppData\Local\Temp\ST_33c0_000001.tmp{rm}
saved
as .dta format
{p_end}

{com}. 
. *** ---------------------------------------------------------------------------
. *** Block 5: Person Daily and Weekly Activity Status
. *** ---------------------------------------------------------------------------
. 
. * Load Block 5 dataset (daily + current weekly activity)
. use ///
>         Key_hhold B5_c1 /// Common variables
>         B5_c3 B5_c4 B5_c7 B5_c8 B5_c9 B5_c10 B5_c11 B5_c12 B5_c13 B5_c14 B5_c15 B5_c16 B5_c17 B5_c18 B5_c19 B5_c20  /// Native variables
>         using "$nss_lab/raw/2003/extracted dta files/Block-5-Members-time-disposition-records", clear
{txt}
{com}. 
. * This block does not have person key so i constrct by concatenating hhold key and person serial number 
. gegen person_key = concat(Key_hhold B5_c1)
{txt}concat() is not a gtools function and no by(); falling back on egen

{com}. 
. drop Key_hhold B5_c1
{txt}
{com}. 
. * Apply harmonized naming convention
. rename *, lower
{res}{txt}
{com}. rename b5_c3                    cda_srl_no
{res}{txt}
{com}. rename b5_c4                    cdas
{res}{txt}
{com}. rename b5_c7                    cda_seventh 
{res}{txt}
{com}. rename b5_c8                    cda_sixth
{res}{txt}
{com}. rename b5_c9                    cda_fifth
{res}{txt}
{com}. rename b5_c10                   cda_fourth
{res}{txt}
{com}. rename b5_c11                   cda_third
{res}{txt}
{com}. rename b5_c12                   cda_second
{res}{txt}
{com}. rename b5_c13                   cda_first
{res}{txt}
{com}. rename b5_c14                   cda_no_of_days
{res}{txt}
{com}. rename b5_c15                   wages_cash
{res}{txt}
{com}. rename b5_c16                   wages_kind                      
{res}{txt}
{com}. rename b5_c17                   wages_total
{res}{txt}
{com}. rename b5_c18                   cwa_status
{res}{txt}
{com}. rename b5_c19                   cwa_nic_1998_5d
{res}{txt}
{com}. rename b5_c20                   cwa_nco_1968_3d
{res}{txt}
{com}. 
. drop if person_key == "43102110405"
{txt}(2 observations deleted)

{com}. drop if person_key == "41966110104"
{txt}(2 observations deleted)

{com}. 
. /*
>            *Assert to check that they are constant across person key
>            foreach var of varlist cwa_status cwa_nic_1998_5d cwa_nco_1968_3d {c -(}
>            bysort person_key: assert `var' == `var'[1] // assertion is false
>            {c )-}
> 
>            gen flag = 0
> 
>            foreach var of varlist cwa_status cwa_nic_1998_5d cwa_nco_1968_3d {c -(}
>            bysort person_key: replace flag = 1 if `var' != `var'[1]
>            {c )-}
> */
. 
. * Reshape daily activity data from long to wide format
. reshape wide cdas cda_seventh cda_sixth cda_fifth cda_fourth cda_third cda_second cda_first cda_no_of_days wages_cash wages_kind wages_total, i(person_key cwa_status cwa_nic_1998_5d cwa_nco_1968_3d) j(cda_srl_no) string
{txt}(j = 1 2 3 4 5)

Data{col 36}Long{col 43}->{col 48}Wide
{hline 77}
Number of observations     {res}     347,166   {txt}->   {res}303,827     
{txt}Number of variables        {res}          17   {txt}->   {res}64          
{txt}j variable (5 values)        {res}cda_srl_no   {txt}->   (dropped)
xij variables:
                                   {res}cdas   {txt}->   {res}cdas1 cdas2 ... cdas5
                            cda_seventh   {txt}->   {res}cda_seventh1 cda_seventh2 ... cda_seventh5
                              cda_sixth   {txt}->   {res}cda_sixth1 cda_sixth2 ... cda_sixth5
                              cda_fifth   {txt}->   {res}cda_fifth1 cda_fifth2 ... cda_fifth5
                             cda_fourth   {txt}->   {res}cda_fourth1 cda_fourth2 ... cda_fourth5
                              cda_third   {txt}->   {res}cda_third1 cda_third2 ... cda_third5
                             cda_second   {txt}->   {res}cda_second1 cda_second2 ... cda_second5
                              cda_first   {txt}->   {res}cda_first1 cda_first2 ... cda_first5
                         cda_no_of_days   {txt}->   {res}cda_no_of_days1 cda_no_of_days2 ... cda_no_of_days5
                             wages_cash   {txt}->   {res}wages_cash1 wages_cash2 ... wages_cash5
                             wages_kind   {txt}->   {res}wages_kind1 wages_kind2 ... wages_kind5
                            wages_total   {txt}->   {res}wages_total1 wages_total2 ... wages_total5
{txt}{hline 77}

{com}. 
. * Merge daily & weekly activity with demographics + usual activity dataset
. merge 1:1 person_key using `demographics'
{res}
{txt}{col 5}Result{col 33}Number of obs
{col 5}{hline 41}
{col 5}Not matched{col 30}{res}               3
{txt}{col 9}from master{col 30}{res}               1{txt}  (_merge==1)
{col 9}from using{col 30}{res}               2{txt}  (_merge==2)

{col 5}Matched{col 30}{res}         303,826{txt}  (_merge==3)
{col 5}{hline 41}

{com}. drop if _merge == 1
{txt}(1 observation deleted)

{com}. drop _merge
{txt}
{com}. 
. *** ---------------------------------------------------------------------------
. *** Merge with Household Characteristics
. *** ---------------------------------------------------------------------------
. 
. * Merge household-level characteristics to person-level dataset
. merge m:1 hh_key using "$nss_lab/intermediate/Blk_hc_2004a"
{res}
{txt}{col 5}Result{col 33}Number of obs
{col 5}{hline 41}
{col 5}Not matched{col 30}{res}               0
{txt}{col 5}Matched{col 30}{res}         303,828{txt}  (_merge==3)
{col 5}{hline 41}

{com}. count if _merge !=3
  {res}0
{txt}
{com}. assert `r(N)' < 10 // to ensure quality of merge
{txt}
{com}. drop _merge 
{txt}
{com}. 
. /*
>            Result                      Number of obs
>            -----------------------------------------
>            Not matched                             0
>            Matched                           303,828  (_merge==3)
>            -----------------------------------------
> */
. 
. * Clean industry codes:
. *   - Remove spaces
. *   - Drop records with placeholder characters (X, Y)
. *   - Drop invalid or incomplete codes
. replace nic_1998_5d = subinstr(nic_1998_5d, " ", "",.)
{txt}(0 real changes made)

{com}. drop if regexm(nic_1998_5d, "x") | regexm(nic_1998_5d, "X") | regexm(nic_1998_5d, "Y") // 0 obs deleted
{txt}(0 observations deleted)

{com}. drop if strlen(nic_1998_5d)!= 5 & !mi(nic_1998_5d) // 0 obs dropped
{txt}(0 observations deleted)

{com}. drop if nic_1998_5d == "00000" 
{txt}(0 observations deleted)

{com}. 
. * Generating year and round variable
. gen year = 2004
{txt}
{com}. gen nss_round = 60
{txt}
{com}. 
. save "$nss_lab/intermediate/Blk_merged_2004a.dta", replace
{txt}{p 0 4 2}
file {bf}
C:/Users/ayush/Dropbox/idl/nss/labour/intermediate/Blk_merged_2004a.dta{rm}
saved
{p_end}

{com}. 
. 
{txt}end of do-file

{com}. * Purpose: To clean data at the person-level EUS for year 2004-05 (July-June)                  
. 
. *** ---------------------------------------------------------------------------
. *** Block 5: Usual Activity Status
. *** ---------------------------------------------------------------------------
. 
. * Loading the dataset
. use ///
>         HHID PID /// Common variables
>         Usual_principal_activity_status Usual_principal_activity_NIC_5_d Usual_principal_activity_NCO_3_d Whether_in_subsidiary_activity /// Native variables
>         Location_of_workplace Enterprise_type Availability_of_work Age Sex Enterprise_uses_electricity /// Native variables
>         WEIGHT_COMBINED /// Multiplier combined
>         using "$nss_lab/raw/2004/extracted dta files/Block_5pt1_level_04.dta", clear
{txt}
{com}. 
. * Renaming variables
. rename *, lower
{res}{txt}
{com}. rename pid                                                              person_key
{res}{txt}
{com}. rename hhid                                                             hh_key
{res}{txt}
{com}. rename usual_principal_activity_status  act_code
{res}{txt}
{com}. rename usual_principal_activity_nic_5_d nic_1998_5d
{res}{txt}
{com}. rename usual_principal_activity_nco_3_d nco_1968_3d
{res}{txt}
{com}. rename whether_in_subsidiary_activity   act_code_sub 
{res}{txt}
{com}. rename availability_of_work                             seeking_work
{res}{txt}
{com}. rename location_of_workplace                    work_location
{res}{txt}
{com}. rename enterprise_type                                  ent_type
{res}{txt}
{com}. rename enterprise_uses_electricity              use_electricity
{res}{txt}
{com}. rename weight_combined                                  weight
{res}{txt}
{com}. 
. * Destringing variables
. destring age act_code act_code_sub ent_type work_location seeking_work, replace
{txt}age already numeric; no {res}replace
{txt}act_code: all characters numeric; {res}replaced {txt}as {res}byte
{txt}act_code_sub: all characters numeric; {res}replaced {txt}as {res}byte
{txt}ent_type: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(480230 missing values generated)
{res}{txt}work_location: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(480300 missing values generated)
{res}{txt}seeking_work: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(80933 missing values generated)
{res}{txt}
{com}. 
. gen nic_1998_4d = substr(nic_1998_5d,1,4)
{txt}(381,524 missing values generated)

{com}. gen nic_1998_3d = substr(nic_1998_5d,1,3)
{txt}(381,524 missing values generated)

{com}. 
. * Drop if missing person key
. drop if person_key == ""
{txt}(0 observations deleted)

{com}. 
. * Save as a temporary file for later merge
. tempfile pc
{txt}
{com}. save `pc'
{txt}{p 0 4 2}
file {bf}
C:\Users\ayush\AppData\Local\Temp\ST_33c0_000001.tmp{rm}
saved
as .dta format
{p_end}

{com}. 
. *** ---------------------------------------------------------------------------
. *** Block 4: Demographics  
. *** ---------------------------------------------------------------------------
. 
. * Load Block 4 dataset (demographics)
. use ///
>         HHID PID /// Identification variables
>         Personal_serial_no Relation_to_head Marital_status General_education Technical_education Registered_with_employment_excha /// Native variables
>         WEIGHT_COMBINED /// Multiplier combined
>         using "$nss_lab/raw/2004/extracted dta files/Block_4_level_03.dta", clear
{txt}
{com}. 
. * Renaming variables
. rename *, lower
{res}{txt}
{com}. rename pid                                                              person_key
{res}{txt}
{com}. rename hhid                                                             hh_key
{res}{txt}
{com}. rename personal_serial_no                               person_srl_no
{res}{txt}
{com}. rename general_education                                gen_edu_raw
{res}{txt}
{com}. rename technical_education                              tech_edu_raw
{res}{txt}
{com}. rename registered_with_employment_excha reg_emp_exch
{res}{txt}
{com}. rename weight_combined                                  weight
{res}{txt}
{com}. 
. * Merge demographics with usual activity dataset
. merge 1:1 person_key using `pc'
{res}
{txt}{col 5}Result{col 33}Number of obs
{col 5}{hline 41}
{col 5}Not matched{col 30}{res}               0
{txt}{col 5}Matched{col 30}{res}         602,833{txt}  (_merge==3)
{col 5}{hline 41}

{com}. keep if _merge == 3
{txt}(0 observations deleted)

{com}. drop _merge
{txt}
{com}. 
. * Save as a temporary file for later merge 
. tempfile demographics
{txt}
{com}. save `demographics'
{txt}{p 0 4 2}
file {bf}
C:\Users\ayush\AppData\Local\Temp\ST_33c0_000002.tmp{rm}
saved
as .dta format
{p_end}

{com}. 
. *** ---------------------------------------------------------------------------
. *** Block 5: Person Daily and Weekly Activity Status
. *** ---------------------------------------------------------------------------
. 
. * Load Block 5 dataset (daily + current weekly activity)
. use ///
>         FSU Hamlet Second_stratum Sample_hhld_no Personal_srl_no /// Identification variables
>         Srl_no_of_current_day_activity Current_day_activity_Status Current_day_activity_Status Current_day_activity_intensity_7 Current_day_activity_intensity_6 Current_day_activity_intensity_5 Current_day_activity_intensity_4 Current_day_activity_intensity_3 Current_day_activity_intensity_2 Current_day_activity_intensity_1 Total_no_of_days_in_current_acti Wage_salary_cash_during_the_week Wage_salary_earnings_kind_during Wage_salary_earnings_total_durin Current_weekly_activity_status Current_weekly_activity_NIC_1998 Current_weekly_activity_NCO_1968 /// Native variables
>         using "$nss_lab/raw/2004/extracted dta files/Block_5pt3_level_06.dta", clear
{txt}
{com}. 
. *This block does not have person key so it is constructed by concatenating household key and person serial number 
. gegen person_key = concat(FSU Hamlet Second_stratum Sample_hhld_no Personal_srl_no)
{txt}concat() is not a gtools function and no by(); falling back on egen

{com}. 
. drop FSU Hamlet Second_stratum Sample_hhld_no Personal_srl_no
{txt}
{com}. 
. * Renaming variables
. rename *, lower
{res}{txt}
{com}. rename srl_no_of_current_day_activity   cda_srl_no
{res}{txt}
{com}. rename current_day_activity_status              cdas
{res}{txt}
{com}. rename current_day_activity_intensity_7 cda_seventh 
{res}{txt}
{com}. rename current_day_activity_intensity_6 cda_sixth
{res}{txt}
{com}. rename current_day_activity_intensity_5 cda_fifth
{res}{txt}
{com}. rename current_day_activity_intensity_4 cda_fourth
{res}{txt}
{com}. rename current_day_activity_intensity_3 cda_third
{res}{txt}
{com}. rename current_day_activity_intensity_2 cda_second
{res}{txt}
{com}. rename current_day_activity_intensity_1 cda_first
{res}{txt}
{com}. rename total_no_of_days_in_current_acti cda_no_of_days
{res}{txt}
{com}. rename wage_salary_cash_during_the_week wages_cash
{res}{txt}
{com}. rename wage_salary_earnings_kind_during wages_kind                      
{res}{txt}
{com}. rename wage_salary_earnings_total_durin wages_total
{res}{txt}
{com}. rename current_weekly_activity_status   cwa_status
{res}{txt}
{com}. rename current_weekly_activity_nic_1998 cwa_nic_1998_5d
{res}{txt}
{com}. rename current_weekly_activity_nco_1968 cwa_nco_1968_3d
{res}{txt}
{com}. 
. * Reshape daily activity data from long to wide format
. reshape wide cdas cda_seventh cda_sixth cda_fifth cda_fourth cda_third cda_second cda_first cda_no_of_days wages_cash wages_kind wages_total, i(person_key cwa_status cwa_nic_1998_5d cwa_nco_1968_3d) j(cda_srl_no) string
{txt}(j = 1 2 3 4 5)

Data{col 36}Long{col 43}->{col 48}Wide
{hline 77}
Number of observations     {res}     682,506   {txt}->   {res}602,897     
{txt}Number of variables        {res}          17   {txt}->   {res}64          
{txt}j variable (5 values)        {res}cda_srl_no   {txt}->   (dropped)
xij variables:
                                   {res}cdas   {txt}->   {res}cdas1 cdas2 ... cdas5
                            cda_seventh   {txt}->   {res}cda_seventh1 cda_seventh2 ... cda_seventh5
                              cda_sixth   {txt}->   {res}cda_sixth1 cda_sixth2 ... cda_sixth5
                              cda_fifth   {txt}->   {res}cda_fifth1 cda_fifth2 ... cda_fifth5
                             cda_fourth   {txt}->   {res}cda_fourth1 cda_fourth2 ... cda_fourth5
                              cda_third   {txt}->   {res}cda_third1 cda_third2 ... cda_third5
                             cda_second   {txt}->   {res}cda_second1 cda_second2 ... cda_second5
                              cda_first   {txt}->   {res}cda_first1 cda_first2 ... cda_first5
                         cda_no_of_days   {txt}->   {res}cda_no_of_days1 cda_no_of_days2 ... cda_no_of_days5
                             wages_cash   {txt}->   {res}wages_cash1 wages_cash2 ... wages_cash5
                             wages_kind   {txt}->   {res}wages_kind1 wages_kind2 ... wages_kind5
                            wages_total   {txt}->   {res}wages_total1 wages_total2 ... wages_total5
{txt}{hline 77}

{com}. 
. * Since for a given person_key, the cdas is coming in two different rows for some observations, I am collapsing the data to take max for each person key when there are duplicates.
. 
. bys person_key: gen dup = cond(_N==1,0,_n) 
{txt}
{com}. 
. * Keep only duplicates
. preserve
{txt}
{com}. 
. keep if dup > 0
{txt}(602,769 observations deleted)

{com}. 
. ds person_key dup, not
{txt}{col 1}cwa_status{col 15}wages_kind1{col 29}wages_total2{col 43}cdas4{col 57}cda_seventh5
{col 1}cwa_nic_1~5d{col 15}wages_total1{col 29}cdas3{col 43}cda_seventh4{col 57}cda_sixth5
{col 1}cwa_nco_1~3d{col 15}cdas2{col 29}cda_seventh3{col 43}cda_sixth4{col 57}cda_fifth5
{col 1}cdas1{col 15}cda_seventh2{col 29}cda_sixth3{col 43}cda_fifth4{col 57}cda_fourth5
{col 1}cda_seventh1{col 15}cda_sixth2{col 29}cda_fifth3{col 43}cda_fourth4{col 57}cda_third5
{col 1}cda_sixth1{col 15}cda_fifth2{col 29}cda_fourth3{col 43}cda_third4{col 57}cda_second5
{col 1}cda_fifth1{col 15}cda_fourth2{col 29}cda_third3{col 43}cda_second4{col 57}cda_first5
{col 1}cda_fourth1{col 15}cda_third2{col 29}cda_second3{col 43}cda_first4{col 57}cda_no_of_~5
{col 1}cda_third1{col 15}cda_second2{col 29}cda_first3{col 43}cda_no_of_~4{col 57}wages_cash5
{col 1}cda_second1{col 15}cda_first2{col 29}cda_no_of_~3{col 43}wages_cash4{col 57}wages_kind5
{col 1}cda_first1{col 15}cda_no_of_~2{col 29}wages_cash3{col 43}wages_kind4{col 57}wages_total5
{col 1}cda_no_of_~1{col 15}wages_cash2{col 29}wages_kind3{col 43}wages_total4
{col 1}wages_cash1{col 15}wages_kind2{col 29}wages_total3{col 43}cdas5

{com}. local allvars `r(varlist)'
{txt}
{com}. 
. ds `allvars', has(type numeric)
{txt}{col 1}cda_seventh1{col 15}cda_seventh2{col 29}cda_seventh3{col 43}cda_seventh4{col 57}cda_seventh5
{col 1}cda_sixth1{col 15}cda_sixth2{col 29}cda_sixth3{col 43}cda_sixth4{col 57}cda_sixth5
{col 1}cda_fifth1{col 15}cda_fifth2{col 29}cda_fifth3{col 43}cda_fifth4{col 57}cda_fifth5
{col 1}cda_fourth1{col 15}cda_fourth2{col 29}cda_fourth3{col 43}cda_fourth4{col 57}cda_fourth5
{col 1}cda_third1{col 15}cda_third2{col 29}cda_third3{col 43}cda_third4{col 57}cda_third5
{col 1}cda_second1{col 15}cda_second2{col 29}cda_second3{col 43}cda_second4{col 57}cda_second5
{col 1}cda_first1{col 15}cda_first2{col 29}cda_first3{col 43}cda_first4{col 57}cda_first5
{col 1}cda_no_of_~1{col 15}cda_no_of_~2{col 29}cda_no_of_~3{col 43}cda_no_of_~4{col 57}cda_no_of_~5
{col 1}wages_cash1{col 15}wages_cash2{col 29}wages_cash3{col 43}wages_cash4{col 57}wages_cash5
{col 1}wages_kind1{col 15}wages_kind2{col 29}wages_kind3{col 43}wages_kind4{col 57}wages_kind5
{col 1}wages_total1{col 15}wages_total2{col 29}wages_total3{col 43}wages_total4{col 57}wages_total5

{com}. local numvars `r(varlist)'
{txt}
{com}. 
. ds `allvars', has(type string)
{txt}{col 1}cwa_status{col 15}cwa_nco_1~3d{col 29}cdas2{col 43}cdas4
{col 1}cwa_nic_1~5d{col 15}cdas1{col 29}cdas3{col 43}cdas5

{com}. local strvars `r(varlist)'
{txt}
{com}. 
. * Collapse numeric variables by max
. collapse (max) `numvars' (first) `strvars', by(person_key)
{res}{txt}
{com}. 
. * Save temporary results
. tempfile dupmax
{txt}
{com}. save `dupmax'
{txt}{p 0 4 2}
file {bf}
C:\Users\ayush\AppData\Local\Temp\ST_33c0_000004.tmp{rm}
saved
as .dta format
{p_end}

{com}. 
. restore
{txt}
{com}. 
. drop if dup > 0
{txt}(128 observations deleted)

{com}. drop dup
{txt}
{com}. 
. * Append back the collapsed duplicates
. append using `dupmax'
{txt}
{com}. 
. * Merge with demographics and usual activity dataset
. merge 1:1 person_key using `demographics'
{res}
{txt}{col 5}Result{col 33}Number of obs
{col 5}{hline 41}
{col 5}Not matched{col 30}{res}               0
{txt}{col 5}Matched{col 30}{res}         602,833{txt}  (_merge==3)
{col 5}{hline 41}

{com}. drop if _merge == 1
{txt}(0 observations deleted)

{com}. drop _merge
{txt}
{com}. 
. *** ---------------------------------------------------------------------------
. *** Merge with Household Characteristics
. *** ---------------------------------------------------------------------------
. 
. * Merge household-level characteristics to person-level dataset
. merge m:1 hh_key using "$nss_lab/intermediate/Blk_hc_2004b"
{res}
{txt}{col 5}Result{col 33}Number of obs
{col 5}{hline 41}
{col 5}Not matched{col 30}{res}              20
{txt}{col 9}from master{col 30}{res}              19{txt}  (_merge==1)
{col 9}from using{col 30}{res}               1{txt}  (_merge==2)

{col 5}Matched{col 30}{res}         602,814{txt}  (_merge==3)
{col 5}{hline 41}

{com}. count if _merge !=3
  {res}20
{txt}
{com}. //assert `r(N)' < 10 // to ensure quality of merge-- 19 are unmerged in this dataset
. drop if _merge == 2
{txt}(1 observation deleted)

{com}. drop _merge 
{txt}
{com}. 
. 
. * Generating year and round variable
. gen year = 2004
{txt}
{com}. gen nss_round = 61
{txt}
{com}. 
. * Clean industry codes:
. *   - Remove spaces
. *   - Drop records with placeholder characters (X, Y)
. *   - Drop invalid or incomplete codes
. replace nic_1998_5d = subinstr(nic_1998_5d, " ", "",.)
{txt}(0 real changes made)

{com}. drop if regexm(nic_1998_5d, "x") | regexm(nic_1998_5d, "X") | regexm(nic_1998_5d, "Y") // 0 obs deleted
{txt}(0 observations deleted)

{com}. drop if strlen(nic_1998_5d)!= 5 & !mi(nic_1998_5d) // 0 obs dropped
{txt}(0 observations deleted)

{com}. drop if nic_1998_5d == "00000" 
{txt}(0 observations deleted)

{com}. 
. * Saving final dataset
. save "$nss_lab/intermediate/Blk_merged_2004b.dta", replace
{txt}{p 0 4 2}
file {bf}
C:/Users/ayush/Dropbox/idl/nss/labour/intermediate/Blk_merged_2004b.dta{rm}
saved
{p_end}

{com}. 
. 
{txt}end of do-file

{com}. * Purpose: To clean data at the person-level EUS for year 2005-06 (July-June)                  
. 
. *** ---------------------------------------------------------------------------
. *** Block 5: Usual Activity Status
. *** ---------------------------------------------------------------------------
. 
. * Loading the dataset
. use Hhold_key Person_key /// Common variables
>         B5_q2 B5_q3 B5_q5 B5_q6 B5_q7 /// Native variables
>         WGT_Comb /// Weight combined
>         using "$nss_lab/raw/2005/extracted dta files/Block-5-Persons-usual-activity-records", clear
{txt}
{com}. 
. * Renaming variables
. rename *, lower
{res}{txt}
{com}. rename hhold_key                                                hh_key
{res}{txt}
{com}. rename b5_q2                                                    age
{res}{txt}
{com}. rename b5_q3                                                    act_code
{res}{txt}
{com}. rename b5_q5                                                    nic_2004_5d
{res}{txt}
{com}. rename b5_q6                                                    nco_1968_3d
{res}{txt}
{com}. rename b5_q7                                                    act_code_sub 
{res}{txt}
{com}. rename wgt_comb                                                 weight
{res}{txt}
{com}. 
. * Destringing variables
. destring age act_code act_code_sub , replace
{txt}age already numeric; no {res}replace
{txt}act_code: all characters numeric; {res}replaced {txt}as {res}byte
{txt}act_code_sub: all characters numeric; {res}replaced {txt}as {res}byte
{txt}
{com}. 
. * Generating NIC 2004 at the 3- and 4-digit level
. gen nic_2004_4d = substr(nic_2004_5d,1,4)
{txt}(242,424 missing values generated)

{com}. gen nic_2004_3d = substr(nic_2004_5d,1,3)
{txt}(242,424 missing values generated)

{com}. 
. * Save as a temporary file for later merge
. tempfile pc
{txt}
{com}. save `pc'
{txt}{p 0 4 2}
file {bf}
C:\Users\ayush\AppData\Local\Temp\ST_33c0_000001.tmp{rm}
saved
as .dta format
{p_end}

{com}. 
. *** ---------------------------------------------------------------------------
. *** Block 4: Demographics  
. *** ---------------------------------------------------------------------------
. 
. * Load Block 4 dataset (demographics)
. use Hhold_key Person_key /// Identification variables
>         Person_slno_B4_q1 B4_q3 B4_q4 B4_q6 B4_q7 B4_q8 /// Native variables
>         using "$nss_lab/raw/2005/extracted dta files/Block-4-Persons-demographic-particulars-records", clear
{txt}
{com}. 
. *renaming variables
. rename *, lower
{res}{txt}
{com}. rename hhold_key                                                hh_key
{res}{txt}
{com}. rename person_slno_b4_q1                                person_srl_no
{res}{txt}
{com}. rename b4_q3                                                    relation_to_head
{res}{txt}
{com}. rename b4_q4                                                    sex
{res}{txt}
{com}. rename b4_q6                                                    marital_status
{res}{txt}
{com}. rename b4_q7                                                    gen_edu_raw
{res}{txt}
{com}. rename b4_q8                                                    tech_edu_raw
{res}{txt}
{com}. 
. * Merge demographics with usual activity dataset
. merge 1:1 person_key using `pc' 
{res}
{txt}{col 5}Result{col 33}Number of obs
{col 5}{hline 41}
{col 5}Not matched{col 30}{res}               0
{txt}{col 5}Matched{col 30}{res}         377,377{txt}  (_merge==3)
{col 5}{hline 41}

{com}. drop _merge
{txt}
{com}. 
. * Save as a temporary file for later merge 
. tempfile demographics
{txt}
{com}. save `demographics'
{txt}{p 0 4 2}
file {bf}
C:\Users\ayush\AppData\Local\Temp\ST_33c0_000002.tmp{rm}
saved
as .dta format
{p_end}

{com}. 
. *** ---------------------------------------------------------------------------
. *** Block 5: Person Daily and Weekly Activity Status
. *** ---------------------------------------------------------------------------
. 
. * Load Block 5 dataset (daily + current weekly activity)
. use Person_key /// Common variables
>         B6_q3 B6_q4 B6_q7 B6_q8 B6_q9 B6_q10 B6_q11 B6_q12 B6_q13 B6_q14 B6_q15 B6_q16 B6_q17 B6_q18 B6_q19 B6_q20 /// Native variables
>         using "$nss_lab/raw/2005/extracted dta files/Block-6-Persons-daily-activity-time-disposition-reecords", clear
{txt}
{com}. 
. * Renaming variables
. rename *, lower
{res}{txt}
{com}. rename b6_q3                    cda_srl_no
{res}{txt}
{com}. rename b6_q4                    cdas
{res}{txt}
{com}. rename b6_q7                    cda_seventh 
{res}{txt}
{com}. rename b6_q8                    cda_sixth
{res}{txt}
{com}. rename b6_q9                    cda_fifth
{res}{txt}
{com}. rename b6_q10                   cda_fourth
{res}{txt}
{com}. rename b6_q11                   cda_third
{res}{txt}
{com}. rename b6_q12                   cda_second
{res}{txt}
{com}. rename b6_q13                   cda_first
{res}{txt}
{com}. rename b6_q14                   cda_no_of_days
{res}{txt}
{com}. rename b6_q15                   wages_cash
{res}{txt}
{com}. rename b6_q16                   wages_kind                      
{res}{txt}
{com}. rename b6_q17                   wages_total
{res}{txt}
{com}. rename b6_q18                   cwa_status
{res}{txt}
{com}. rename b6_q19                   cwa_nic_2004_5d
{res}{txt}
{com}. rename b6_q20                   cwa_nco_1968_3d
{res}{txt}
{com}. 
. * Reshape daily activity data from long to wide format
. reshape wide cdas cda_seventh cda_sixth cda_fifth cda_fourth cda_third cda_second cda_first cda_no_of_days wages_cash wages_kind wages_total, i(person_key cwa_status cwa_nic_2004_5d cwa_nco_1968_3d) j(cda_srl_no) string
{txt}(j = 1 2 3 4)

Data{col 36}Long{col 43}->{col 48}Wide
{hline 77}
Number of observations     {res}     413,657   {txt}->   {res}377,895     
{txt}Number of variables        {res}          17   {txt}->   {res}52          
{txt}j variable (4 values)        {res}cda_srl_no   {txt}->   (dropped)
xij variables:
                                   {res}cdas   {txt}->   {res}cdas1 cdas2 ... cdas4
                            cda_seventh   {txt}->   {res}cda_seventh1 cda_seventh2 ... cda_seventh4
                              cda_sixth   {txt}->   {res}cda_sixth1 cda_sixth2 ... cda_sixth4
                              cda_fifth   {txt}->   {res}cda_fifth1 cda_fifth2 ... cda_fifth4
                             cda_fourth   {txt}->   {res}cda_fourth1 cda_fourth2 ... cda_fourth4
                              cda_third   {txt}->   {res}cda_third1 cda_third2 ... cda_third4
                             cda_second   {txt}->   {res}cda_second1 cda_second2 ... cda_second4
                              cda_first   {txt}->   {res}cda_first1 cda_first2 ... cda_first4
                         cda_no_of_days   {txt}->   {res}cda_no_of_days1 cda_no_of_days2 ... cda_no_of_days4
                             wages_cash   {txt}->   {res}wages_cash1 wages_cash2 ... wages_cash4
                             wages_kind   {txt}->   {res}wages_kind1 wages_kind2 ... wages_kind4
                            wages_total   {txt}->   {res}wages_total1 wages_total2 ... wages_total4
{txt}{hline 77}

{com}. 
. 
. * Since for a given person_key, the cdas is coming in two different rows for some observations, I am collapsing the data to take max for each person key when there are duplicates.
. 
. bys person_key: gen dup = cond(_N==1,0,_n) 
{txt}
{com}. 
. * Keep only duplicates
. preserve
{txt}
{com}. 
. keep if dup > 0
{txt}(376,859 observations deleted)

{com}. 
. ds person_key dup, not
{txt}{col 1}cwa_status{col 15}cda_no_of_~1{col 29}cda_first2{col 43}cda_second3{col 57}cda_third4
{col 1}cwa_nic_2~5d{col 15}wages_cash1{col 29}cda_no_of_~2{col 43}cda_first3{col 57}cda_second4
{col 1}cwa_nco_1~3d{col 15}wages_kind1{col 29}wages_cash2{col 43}cda_no_of_~3{col 57}cda_first4
{col 1}cdas1{col 15}wages_total1{col 29}wages_kind2{col 43}wages_cash3{col 57}cda_no_of_~4
{col 1}cda_seventh1{col 15}cdas2{col 29}wages_total2{col 43}wages_kind3{col 57}wages_cash4
{col 1}cda_sixth1{col 15}cda_seventh2{col 29}cdas3{col 43}wages_total3{col 57}wages_kind4
{col 1}cda_fifth1{col 15}cda_sixth2{col 29}cda_seventh3{col 43}cdas4{col 57}wages_total4
{col 1}cda_fourth1{col 15}cda_fifth2{col 29}cda_sixth3{col 43}cda_seventh4
{col 1}cda_third1{col 15}cda_fourth2{col 29}cda_fifth3{col 43}cda_sixth4
{col 1}cda_second1{col 15}cda_third2{col 29}cda_fourth3{col 43}cda_fifth4
{col 1}cda_first1{col 15}cda_second2{col 29}cda_third3{col 43}cda_fourth4

{com}. local allvars `r(varlist)'
{txt}
{com}. 
. ds `allvars', has(type numeric)
{txt}{col 1}cda_seventh1{col 15}wages_kind1{col 29}cda_no_of_~2{col 43}cda_second3{col 57}cda_fourth4
{col 1}cda_sixth1{col 15}wages_total1{col 29}wages_cash2{col 43}cda_first3{col 57}cda_third4
{col 1}cda_fifth1{col 15}cda_seventh2{col 29}wages_kind2{col 43}cda_no_of_~3{col 57}cda_second4
{col 1}cda_fourth1{col 15}cda_sixth2{col 29}wages_total2{col 43}wages_cash3{col 57}cda_first4
{col 1}cda_third1{col 15}cda_fifth2{col 29}cda_seventh3{col 43}wages_kind3{col 57}cda_no_of_~4
{col 1}cda_second1{col 15}cda_fourth2{col 29}cda_sixth3{col 43}wages_total3{col 57}wages_cash4
{col 1}cda_first1{col 15}cda_third2{col 29}cda_fifth3{col 43}cda_seventh4{col 57}wages_kind4
{col 1}cda_no_of_~1{col 15}cda_second2{col 29}cda_fourth3{col 43}cda_sixth4{col 57}wages_total4
{col 1}wages_cash1{col 15}cda_first2{col 29}cda_third3{col 43}cda_fifth4

{com}. local numvars `r(varlist)'
{txt}
{com}. 
. ds `allvars', has(type string)
{txt}{col 1}cwa_status{col 15}cwa_nco_1~3d{col 29}cdas2{col 43}cdas4
{col 1}cwa_nic_2~5d{col 15}cdas1{col 29}cdas3

{com}. local strvars `r(varlist)'
{txt}
{com}. 
. * Collapse numeric variables by max
. collapse (max) `numvars' (first) `strvars', by(person_key)
{res}{txt}
{com}. 
. * Save temporary results
. tempfile dupmax
{txt}
{com}. save `dupmax'
{txt}{p 0 4 2}
file {bf}
C:\Users\ayush\AppData\Local\Temp\ST_33c0_000004.tmp{rm}
saved
as .dta format
{p_end}

{com}. 
. restore
{txt}
{com}. 
. drop if dup > 0
{txt}(1,036 observations deleted)

{com}. drop dup
{txt}
{com}. 
. * Append back the collapsed duplicates
. append using `dupmax'
{txt}
{com}. 
. * Merge with demographics and usual activity dataset
. merge 1:1 person_key using `demographics'
{res}
{txt}{col 5}Result{col 33}Number of obs
{col 5}{hline 41}
{col 5}Not matched{col 30}{res}               0
{txt}{col 5}Matched{col 30}{res}         377,377{txt}  (_merge==3)
{col 5}{hline 41}

{com}. drop if _merge == 1
{txt}(0 observations deleted)

{com}. drop _merge
{txt}
{com}. 
. *** ---------------------------------------------------------------------------
. *** Merge with Household Characteristics
. *** ---------------------------------------------------------------------------
. 
. * Merge household-level characteristics to person-level dataset
. merge m:1 hh_key using "$nss_lab/intermediate/Blk_hc_2005"
{res}
{txt}{col 5}Result{col 33}Number of obs
{col 5}{hline 41}
{col 5}Not matched{col 30}{res}               0
{txt}{col 5}Matched{col 30}{res}         377,377{txt}  (_merge==3)
{col 5}{hline 41}

{com}. count if _merge !=3
  {res}0
{txt}
{com}. assert `r(N)' < 10 // to ensure quality of merge
{txt}
{com}. drop if _merge == 2
{txt}(0 observations deleted)

{com}. drop _merge 
{txt}
{com}. 
. * Generating year and round variable
. gen year = 2005
{txt}
{com}. gen nss_round = 62
{txt}
{com}. 
. * Clean industry codes:
. *   - Remove spaces
. *   - Drop records with placeholder characters (X, Y)
. *   - Drop invalid or incomplete codes
. replace nic_2004_5d = subinstr(nic_2004_5d, " ", "",.)
{txt}(0 real changes made)

{com}. drop if regexm(nic_2004_5d, "x") | regexm(nic_2004_5d, "X") | regexm(nic_2004_5d, "Y") // 0 obs deleted
{txt}(0 observations deleted)

{com}. drop if strlen(nic_2004_5d)!= 5 & !mi(nic_2004_5d) // 0 obs dropped
{txt}(0 observations deleted)

{com}. drop if nic_2004_5d == "00000" 
{txt}(0 observations deleted)

{com}. 
. * Saving final dataset
. save "$nss_lab/intermediate/Blk_merged_2005.dta", replace
{txt}{p 0 4 2}
file {bf}
C:/Users/ayush/Dropbox/idl/nss/labour/intermediate/Blk_merged_2005.dta{rm}
saved
{p_end}

{com}. 
. 
. 
{txt}end of do-file

{com}. * Purpose: To clean data at the person-level EUS for year 2007-08 (July-June)                  
. 
. *** ---------------------------------------------------------------------------
. *** Block 4: Demographics + Usual Activity Status
. *** ---------------------------------------------------------------------------
. 
. * Loading the dataset
. use key_memb key_hhold Rec_id /// Common variables
>         key_memb B4_c1 B4_c3 B4_c4 B4_c5 B4_c6 B4_c7 B4_c8 B4_c9 B4_c11 B4_c12 B4_c13 B4_c14 B4_c16 B4_c17 /// Native variables
>         wgt_combined /// Weight combined
>         using "$nss_lab/raw/2007/extracted dta files/Block-4-demographic-usual-activity-members-records", clear
{txt}
{com}. 
. * Renaming variables
. rename *, lower
{res}{txt}
{com}. rename key_hhold                                                hh_key
{res}{txt}
{com}. rename key_memb                                                 person_key
{res}{txt}
{com}. rename b4_c1                                                    person_srl_no   
{res}{txt}
{com}. rename b4_c3                                                    relation_to_head        
{res}{txt}
{com}. rename rec_id                                                   block
{res}{txt}
{com}. rename b4_c4                                                    sex
{res}{txt}
{com}. rename b4_c5                                                    age
{res}{txt}
{com}. rename b4_c6                                                    marital_status
{res}{txt}
{com}. rename b4_c7                                                    gen_edu_raw
{res}{txt}
{com}. rename b4_c8                                                    tech_edu_raw
{res}{txt}
{com}. rename b4_c9                                                    act_code
{res}{txt}
{com}. rename b4_c11                                                   nic_2004_5d
{res}{txt}
{com}. rename b4_c12                                                   nco_2004_3d
{res}{txt}
{com}. rename b4_c13                                                   act_code_sub 
{res}{txt}
{com}. rename b4_c14                                                   sub_act_status
{res}{txt}
{com}. rename b4_c16                                                   sub_nic_2004_5d
{res}{txt}
{com}. rename b4_c17                                                   sub_nco_2004_3d
{res}{txt}
{com}. rename wgt_combined                                             weight
{res}{txt}
{com}. 
. * Destringing variables
. destring age act_code act_code_sub , replace
{txt}age already numeric; no {res}replace
{txt}act_code: all characters numeric; {res}replaced {txt}as {res}byte
{txt}act_code_sub: all characters numeric; {res}replaced {txt}as {res}byte
{txt}
{com}. 
. * Generating NIC 2004 at the 3- and 4-digit level
. gen nic_2004_4d = substr(nic_2004_5d,1,4)
{txt}(364,646 missing values generated)

{com}. gen nic_2004_3d = substr(nic_2004_5d,1,3)
{txt}(364,646 missing values generated)

{com}. 
. * Save as a temporary file for later merge
. tempfile demographics
{txt}
{com}. save `demographics'
{txt}{p 0 4 2}
file {bf}
C:\Users\ayush\AppData\Local\Temp\ST_33c0_000001.tmp{rm}
saved
as .dta format
{p_end}

{com}. 
. *** ---------------------------------------------------------------------------
. *** Block 5: Person Daily and Weekly Activity Status
. *** ---------------------------------------------------------------------------
. 
. * Load Block 5 dataset (daily + current weekly activity)
. use key_hhold B5_c1 /// Common variables
>         B5_c3 B5_c4 B5_c7 B5_c8 B5_c9 B5_c10 B5_c11 B5_c12 B5_c13 B5_c14 B5_c15 B5_c16 B5_c17 B5_c18 B5_c19 B5_c20 /// Native variables
>         using "$nss_lab/raw/2007/extracted dta files/Block-5-members-time-disposition-records", clear
{txt}
{com}. 
. * Creating person key because it is not present in the dataset
. gegen person_key = concat(key_hhold B5_c1)
{txt}concat() is not a gtools function and no by(); falling back on egen

{com}. 
. drop key_hhold B5_c1
{txt}
{com}. 
. * Renaming variables
. rename *, lower
{res}{txt}
{com}. rename b5_c3                    cda_srl_no
{res}{txt}
{com}. rename b5_c4                    cdas
{res}{txt}
{com}. rename b5_c7                    cda_seventh 
{res}{txt}
{com}. rename b5_c8                    cda_sixth
{res}{txt}
{com}. rename b5_c9                    cda_fifth
{res}{txt}
{com}. rename b5_c10                   cda_fourth
{res}{txt}
{com}. rename b5_c11                   cda_third
{res}{txt}
{com}. rename b5_c12                   cda_second
{res}{txt}
{com}. rename b5_c13                   cda_first
{res}{txt}
{com}. rename b5_c14                   cda_no_of_days
{res}{txt}
{com}. rename b5_c15                   wages_cash
{res}{txt}
{com}. rename b5_c16                   wages_kind                      
{res}{txt}
{com}. rename b5_c17                   wages_total
{res}{txt}
{com}. rename b5_c18                   cwa_status
{res}{txt}
{com}. rename b5_c19                   cwa_nic_2004_5d
{res}{txt}
{com}. rename b5_c20                   cwa_nco_2004_3d
{res}{txt}
{com}. 
. /*
>            *Assert to check that they are constant across person key
>            foreach var of varlist cwa_status cwa_nic_2004_5d cwa_nco_2004_3d {c -(}
>            bysort person_key: assert `var' == `var'[1] // assertion is false
>            {c )-}
> */
. 
. * Reshape daily activity data from long to wide format
. reshape wide cdas cda_seventh cda_sixth cda_fifth cda_fourth cda_third cda_second cda_first cda_no_of_days wages_cash wages_kind wages_total, i(person_key cwa_status cwa_nic_2004_5d cwa_nco_2004_3d) j(cda_srl_no) string
{txt}(j = 1 2 3 4)

Data{col 36}Long{col 43}->{col 48}Wide
{hline 77}
Number of observations     {res}     638,443   {txt}->   {res}572,258     
{txt}Number of variables        {res}          17   {txt}->   {res}52          
{txt}j variable (4 values)        {res}cda_srl_no   {txt}->   (dropped)
xij variables:
                                   {res}cdas   {txt}->   {res}cdas1 cdas2 ... cdas4
                            cda_seventh   {txt}->   {res}cda_seventh1 cda_seventh2 ... cda_seventh4
                              cda_sixth   {txt}->   {res}cda_sixth1 cda_sixth2 ... cda_sixth4
                              cda_fifth   {txt}->   {res}cda_fifth1 cda_fifth2 ... cda_fifth4
                             cda_fourth   {txt}->   {res}cda_fourth1 cda_fourth2 ... cda_fourth4
                              cda_third   {txt}->   {res}cda_third1 cda_third2 ... cda_third4
                             cda_second   {txt}->   {res}cda_second1 cda_second2 ... cda_second4
                              cda_first   {txt}->   {res}cda_first1 cda_first2 ... cda_first4
                         cda_no_of_days   {txt}->   {res}cda_no_of_days1 cda_no_of_days2 ... cda_no_of_days4
                             wages_cash   {txt}->   {res}wages_cash1 wages_cash2 ... wages_cash4
                             wages_kind   {txt}->   {res}wages_kind1 wages_kind2 ... wages_kind4
                            wages_total   {txt}->   {res}wages_total1 wages_total2 ... wages_total4
{txt}{hline 77}

{com}. 
. * Since for a given person_key, the cdas is coming in two different rows for some observations, I am collapsing the data to take max for each person key when there are duplicates.
. 
. bys person_key: gen dup = cond(_N==1,0,_n) 
{txt}
{com}. 
. * Keep only duplicates
. preserve
{txt}
{com}. 
. keep if dup > 0
{txt}(572,250 observations deleted)

{com}. 
. ds person_key dup, not
{txt}{col 1}cwa_status{col 15}cda_no_of_~1{col 29}cda_first2{col 43}cda_second3{col 57}cda_third4
{col 1}cwa_nic_2~5d{col 15}wages_cash1{col 29}cda_no_of_~2{col 43}cda_first3{col 57}cda_second4
{col 1}cwa_nco_2~3d{col 15}wages_kind1{col 29}wages_cash2{col 43}cda_no_of_~3{col 57}cda_first4
{col 1}cdas1{col 15}wages_total1{col 29}wages_kind2{col 43}wages_cash3{col 57}cda_no_of_~4
{col 1}cda_seventh1{col 15}cdas2{col 29}wages_total2{col 43}wages_kind3{col 57}wages_cash4
{col 1}cda_sixth1{col 15}cda_seventh2{col 29}cdas3{col 43}wages_total3{col 57}wages_kind4
{col 1}cda_fifth1{col 15}cda_sixth2{col 29}cda_seventh3{col 43}cdas4{col 57}wages_total4
{col 1}cda_fourth1{col 15}cda_fifth2{col 29}cda_sixth3{col 43}cda_seventh4
{col 1}cda_third1{col 15}cda_fourth2{col 29}cda_fifth3{col 43}cda_sixth4
{col 1}cda_second1{col 15}cda_third2{col 29}cda_fourth3{col 43}cda_fifth4
{col 1}cda_first1{col 15}cda_second2{col 29}cda_third3{col 43}cda_fourth4

{com}. local allvars `r(varlist)'
{txt}
{com}. 
. ds `allvars', has(type numeric)
{txt}{col 1}cda_seventh1{col 15}wages_kind1{col 29}cda_no_of_~2{col 43}cda_second3{col 57}cda_fourth4
{col 1}cda_sixth1{col 15}wages_total1{col 29}wages_cash2{col 43}cda_first3{col 57}cda_third4
{col 1}cda_fifth1{col 15}cda_seventh2{col 29}wages_kind2{col 43}cda_no_of_~3{col 57}cda_second4
{col 1}cda_fourth1{col 15}cda_sixth2{col 29}wages_total2{col 43}wages_cash3{col 57}cda_first4
{col 1}cda_third1{col 15}cda_fifth2{col 29}cda_seventh3{col 43}wages_kind3{col 57}cda_no_of_~4
{col 1}cda_second1{col 15}cda_fourth2{col 29}cda_sixth3{col 43}wages_total3{col 57}wages_cash4
{col 1}cda_first1{col 15}cda_third2{col 29}cda_fifth3{col 43}cda_seventh4{col 57}wages_kind4
{col 1}cda_no_of_~1{col 15}cda_second2{col 29}cda_fourth3{col 43}cda_sixth4{col 57}wages_total4
{col 1}wages_cash1{col 15}cda_first2{col 29}cda_third3{col 43}cda_fifth4

{com}. local numvars `r(varlist)'
{txt}
{com}. 
. ds `allvars', has(type string)
{txt}{col 1}cwa_status{col 15}cwa_nco_2~3d{col 29}cdas2{col 43}cdas4
{col 1}cwa_nic_2~5d{col 15}cdas1{col 29}cdas3

{com}. local strvars `r(varlist)'
{txt}
{com}. 
. * Collapse numeric variables by max
. collapse (max) `numvars' (first) `strvars', by(person_key)
{res}{txt}
{com}. 
. * Save temporary results
. tempfile dupmax
{txt}
{com}. save `dupmax'
{txt}{p 0 4 2}
file {bf}
C:\Users\ayush\AppData\Local\Temp\ST_33c0_000003.tmp{rm}
saved
as .dta format
{p_end}

{com}. 
. restore
{txt}
{com}. 
. drop if dup > 0
{txt}(8 observations deleted)

{com}. drop dup
{txt}
{com}. 
. * Append back the collapsed duplicates
. append using `dupmax'
{txt}
{com}. 
. * Merge with demographics and usual activity dataset
. merge 1:1 person_key using `demographics'
{res}
{txt}{col 5}Result{col 33}Number of obs
{col 5}{hline 41}
{col 5}Not matched{col 30}{res}               0
{txt}{col 5}Matched{col 30}{res}         572,254{txt}  (_merge==3)
{col 5}{hline 41}

{com}. drop _merge
{txt}
{com}. 
. *** ---------------------------------------------------------------------------
. *** Merge with Household Characteristics
. *** ---------------------------------------------------------------------------
. 
. * Merge household-level characteristics to person-level dataset
. merge m:1 hh_key using "$nss_lab/intermediate/Blk_hc_2007"
{res}
{txt}{col 5}Result{col 33}Number of obs
{col 5}{hline 41}
{col 5}Not matched{col 30}{res}               0
{txt}{col 5}Matched{col 30}{res}         572,254{txt}  (_merge==3)
{col 5}{hline 41}

{com}. count if _merge !=3
  {res}0
{txt}
{com}. assert `r(N)' < 10 // to ensure quality of merge
{txt}
{com}. drop _merge 
{txt}
{com}. 
. /*
>            Result                      Number of obs
>            -----------------------------------------
>            Not matched                             0
>            Matched                           572,254  (_merge==3)
>            -----------------------------------------
> */
. 
. * Generating year and round variable
. gen year = 2007
{txt}
{com}. gen nss_round = 64
{txt}
{com}. 
. * Clean industry codes:
. *   - Remove spaces
. *   - Drop records with placeholder characters (X, Y)
. *   - Drop invalid or incomplete codes
. replace nic_2004_5d = subinstr(nic_2004_5d, " ", "",.)
{txt}(0 real changes made)

{com}. drop if regexm(nic_2004_5d, "x") | regexm(nic_2004_5d, "X") | regexm(nic_2004_5d, "Y") // 0 obs deleted
{txt}(0 observations deleted)

{com}. drop if strlen(nic_2004_5d)!= 5 & !mi(nic_2004_5d) // 0 obs dropped
{txt}(0 observations deleted)

{com}. drop if nic_2004_5d == "00000" 
{txt}(0 observations deleted)

{com}. 
. * Saving final dataset
. save "$nss_lab/intermediate/Blk_merged_2007.dta", replace
{txt}{p 0 4 2}
file {bf}
C:/Users/ayush/Dropbox/idl/nss/labour/intermediate/Blk_merged_2007.dta{rm}
saved
{p_end}

{com}. 
{txt}end of do-file

{com}. * Purpose: To clean data at the person-level EUS for year 2009-10 (July-June)                  
. 
. *** ---------------------------------------------------------------------------
. *** Block 5: Usual Activity Status
. *** ---------------------------------------------------------------------------
. 
. * Loading the dataset
. use ///
>         HHID PID /// Common variables 
>         Age Usual_Principal_Activity_Status Usual_Principal_Activity_NIC2004 Availability_of_Work_during_last ///
>         Usual_Principal_Activity_NCO2004 Whether_in_Subsidiary_Activity Location_of_Workspace Enterprise_Type Enterprise_uses_Electricity /// Native variables
>         WEIGHT /// Weight
>         using "$nss_lab/raw/2009/extracted dta files/Block_5_1_Usual principal activity particulars of household members", clear
{txt}
{com}. 
. * Renaming variables
. rename *, lower
{res}{txt}
{com}. rename hhid                                                                     hh_key
{res}{txt}
{com}. rename pid                                                                      person_key
{res}{txt}
{com}. rename usual_principal_activity_status          act_code
{res}{txt}
{com}. rename usual_principal_activity_nic2004         nic_2004_5d
{res}{txt}
{com}. rename usual_principal_activity_nco2004         nco_2004_3d
{res}{txt}
{com}. rename whether_in_subsidiary_activity           act_code_sub 
{res}{txt}
{com}. rename availability_of_work_during_last         seeking_work
{res}{txt}
{com}. rename location_of_workspace                            work_location
{res}{txt}
{com}. rename enterprise_type                                          ent_type
{res}{txt}
{com}. rename enterprise_uses_electricity                      use_electricity
{res}{txt}
{com}. 
. * Destringing variables
. destring age act_code act_code_sub work_location ent_type seeking_work, replace
{txt}age already numeric; no {res}replace
{txt}act_code: all characters numeric; {res}replaced {txt}as {res}byte
{txt}act_code_sub: all characters numeric; {res}replaced {txt}as {res}byte
{txt}work_location: all characters numeric; {res}replaced {txt}as {res}byte
{txt}ent_type: all characters numeric; {res}replaced {txt}as {res}byte
{txt}seeking_work: all characters numeric; {res}replaced {txt}as {res}byte
{txt}
{com}. 
. * Generating NIC 2004 at the 3- and 4-digit level
. gen nic_2004_4d = substr(nic_2004_5d,1,4)
{txt}
{com}. gen nic_2004_3d = substr(nic_2004_5d,1,3)
{txt}
{com}. 
. * Save as a temporary file for later merge
. tempfile pc
{txt}
{com}. save `pc'
{txt}{p 0 4 2}
file {bf}
C:\Users\ayush\AppData\Local\Temp\ST_33c0_000001.tmp{rm}
saved
as .dta format
{p_end}

{com}. 
. *** ---------------------------------------------------------------------------
. *** Block 4: Demographics
. *** ---------------------------------------------------------------------------
. 
. * Loading the dataset
. use ///
>         HHID PID /// Common variables 
>         Person_Serial_No Relation_to_Head Sex Marital_Status General_Education Technical_Education Registered_with_Emp_Exchange /// Native variables
>         WEIGHT /// Weight
>         using "$nss_lab/raw/2009/extracted dta files/Block_4_Demographic particulars of household members", clear
{txt}
{com}. 
. * Renaming variables
. rename *, lower
{res}{txt}
{com}. rename hhid                                                     hh_key
{res}{txt}
{com}. rename pid                                                      person_key
{res}{txt}
{com}. rename person_serial_no                         person_srl_no
{res}{txt}
{com}. rename general_education                        gen_edu_raw
{res}{txt}
{com}. rename technical_education                      tech_edu_raw
{res}{txt}
{com}. rename registered_with_emp_exchange     reg_emp_exch
{res}{txt}
{com}. 
. * Merge demographics and usual activity dataset
. merge 1:1 person_key using `pc'
{res}
{txt}{col 5}Result{col 33}Number of obs
{col 5}{hline 41}
{col 5}Not matched{col 30}{res}               0
{txt}{col 5}Matched{col 30}{res}         459,784{txt}  (_merge==3)
{col 5}{hline 41}

{com}. drop _merge
{txt}
{com}. 
. * Save as a temporary file for later merge
. tempfile demographics
{txt}
{com}. save `demographics'
{txt}{p 0 4 2}
file {bf}
C:\Users\ayush\AppData\Local\Temp\ST_33c0_000002.tmp{rm}
saved
as .dta format
{p_end}

{com}. 
. *** ---------------------------------------------------------------------------
. *** Block 5: Person Daily and Weekly Activity Status
. *** ---------------------------------------------------------------------------
. 
. * Load Block 5 dataset (daily + current weekly activity)
. use ///
>         PID /// Common variables 
>         Srl_no_of_Activity Status Intensity_7th_Day Intensity_6th_Day Intensity_5th_Day Intensity_4th_Day Intensity_3rd_Day Intensity_2nd_Day Intensity_1st_Day Total_no_days_in_each_activity Wage_and_Salary_Earnings_Cash Wage_and_Salary_Earnings_Kind Wage_and_Salary_Earnings_Total Current_Weekly_Activity_Status Current_Weekly_Activity_NIC_2004 Current_Weekly_Activity_NCO_2004 /// Native variables
>         using "$nss_lab/raw/2009/extracted dta files/Block_5_3_Time disposition during the week ended on ...............dta", clear
{txt}
{com}. 
. * Renaming variables
. rename *, lower
{res}{txt}
{com}. rename pid                                                              person_key
{res}{txt}
{com}. rename srl_no_of_activity                               cda_srl_no
{res}{txt}
{com}. rename status                                                   cdas
{res}{txt}
{com}. rename intensity_7th_day                                cda_seventh 
{res}{txt}
{com}. rename intensity_6th_day                                cda_sixth
{res}{txt}
{com}. rename intensity_5th_day                                cda_fifth
{res}{txt}
{com}. rename intensity_4th_day                                cda_fourth
{res}{txt}
{com}. rename intensity_3rd_day                                cda_third
{res}{txt}
{com}. rename intensity_2nd_day                                cda_second
{res}{txt}
{com}. rename intensity_1st_day                                cda_first
{res}{txt}
{com}. rename total_no_days_in_each_activity   cda_no_of_days
{res}{txt}
{com}. rename wage_and_salary_earnings_cash    wages_cash
{res}{txt}
{com}. rename wage_and_salary_earnings_kind    wages_kind                      
{res}{txt}
{com}. rename wage_and_salary_earnings_total   wages_total
{res}{txt}
{com}. rename current_weekly_activity_status   cwa_status
{res}{txt}
{com}. rename current_weekly_activity_nic_2004 cwa_nic_2004_5d
{res}{txt}
{com}. rename current_weekly_activity_nco_2004 cwa_nco_2004_3d
{res}{txt}
{com}. 
. * Assert to check that they are constant across person key
. foreach var of varlist cwa_status cwa_nic_2004_5d cwa_nco_2004_3d {c -(}
{txt}  2{com}.         bysort person_key: assert `var' == `var'[1] // assertion is false
{txt}  3{com}. {c )-}
{txt}
{com}. 
. * Reshape daily activity data from long to wide format
. reshape wide cdas cda_seventh cda_sixth cda_fifth cda_fourth cda_third cda_second cda_first cda_no_of_days wages_cash wages_kind wages_total, i(person_key cwa_status cwa_nic_2004_5d cwa_nco_2004_3d) j(cda_srl_no) string
{txt}(j = 1 2 3 4)

Data{col 36}Long{col 43}->{col 48}Wide
{hline 77}
Number of observations     {res}     500,262   {txt}->   {res}459,784     
{txt}Number of variables        {res}          17   {txt}->   {res}52          
{txt}j variable (4 values)        {res}cda_srl_no   {txt}->   (dropped)
xij variables:
                                   {res}cdas   {txt}->   {res}cdas1 cdas2 ... cdas4
                            cda_seventh   {txt}->   {res}cda_seventh1 cda_seventh2 ... cda_seventh4
                              cda_sixth   {txt}->   {res}cda_sixth1 cda_sixth2 ... cda_sixth4
                              cda_fifth   {txt}->   {res}cda_fifth1 cda_fifth2 ... cda_fifth4
                             cda_fourth   {txt}->   {res}cda_fourth1 cda_fourth2 ... cda_fourth4
                              cda_third   {txt}->   {res}cda_third1 cda_third2 ... cda_third4
                             cda_second   {txt}->   {res}cda_second1 cda_second2 ... cda_second4
                              cda_first   {txt}->   {res}cda_first1 cda_first2 ... cda_first4
                         cda_no_of_days   {txt}->   {res}cda_no_of_days1 cda_no_of_days2 ... cda_no_of_days4
                             wages_cash   {txt}->   {res}wages_cash1 wages_cash2 ... wages_cash4
                             wages_kind   {txt}->   {res}wages_kind1 wages_kind2 ... wages_kind4
                            wages_total   {txt}->   {res}wages_total1 wages_total2 ... wages_total4
{txt}{hline 77}

{com}. 
. * Merge current activity with demographics and usual activity dataset
. merge 1:1 person_key using `demographics'
{res}
{txt}{col 5}Result{col 33}Number of obs
{col 5}{hline 41}
{col 5}Not matched{col 30}{res}               0
{txt}{col 5}Matched{col 30}{res}         459,784{txt}  (_merge==3)
{col 5}{hline 41}

{com}. drop _merge
{txt}
{com}. 
. * Save as a temporary file for later merge
. tempfile cdas
{txt}
{com}. save `cdas'
{txt}{p 0 4 2}
file {bf}
C:\Users\ayush\AppData\Local\Temp\ST_33c0_000003.tmp{rm}
saved
as .dta format
{p_end}

{com}. 
. *** ---------------------------------------------------------------------------
. *** Block 9: Household Consumption Expenditure
. *** ---------------------------------------------------------------------------
. 
. * Loading the dataset
. use ///
>         HHID /// Common variables 
>         Item_Group_Srl_No Value_of_Consumption_Last_30_Day /// Native variables
>         using "$nss_lab/raw/2009/extracted dta files/Block_9_Household consumer expenditure", clear
{txt}
{com}. 
. * Renaming variables
. rename *, lower
{res}{txt}
{com}. rename hhid                                                                     hh_key
{res}{txt}
{com}. 
. * Keeping only total expenditure 
. keep if inlist(item_group_srl_no, "40")
{txt}(2,580,291 observations deleted)

{com}. 
. * Renaming variable
. ren value_of_consumption_last_30_day exp_sum 
{res}{txt}
{com}. 
. drop item_group_srl_no
{txt}
{com}. 
. * Merge with previous blocks
. merge 1:m hh_key using `cdas'
{res}
{txt}{col 5}Result{col 33}Number of obs
{col 5}{hline 41}
{col 5}Not matched{col 30}{res}               0
{txt}{col 5}Matched{col 30}{res}         459,784{txt}  (_merge==3)
{col 5}{hline 41}

{com}. assert _merge != 1
{txt}
{com}. drop _merge
{txt}
{com}. 
. *** ---------------------------------------------------------------------------
. *** Merge with Household Characteristics
. *** ---------------------------------------------------------------------------
. 
. * Merge household-level characteristics to person-level dataset
. merge m:1 hh_key using "$nss_lab/intermediate/Blk_hc_2009"
{res}
{txt}{col 5}Result{col 33}Number of obs
{col 5}{hline 41}
{col 5}Not matched{col 30}{res}               0
{txt}{col 5}Matched{col 30}{res}         459,784{txt}  (_merge==3)
{col 5}{hline 41}

{com}. count if _merge !=3
  {res}0
{txt}
{com}. assert `r(N)' < 10 // to ensure quality of merge
{txt}
{com}. drop _merge 
{txt}
{com}. 
. * Generating monthly total per capita expenditure
. gen total_exp_hh = exp_sum/hh_size
{txt}
{com}. drop exp_sum
{txt}
{com}. 
. * Generating year and round variable
. gen year = 2009
{txt}
{com}. gen nss_round = 66
{txt}
{com}. 
. * Clean industry codes:
. *   - Remove spaces
. *   - Drop records with placeholder characters (X, Y)
. *   - Drop invalid or incomplete codes
. replace nic_2004_5d = subinstr(nic_2004_5d, " ", "",.)
{txt}(0 real changes made)

{com}. drop if regexm(nic_2004_5d, "x") | regexm(nic_2004_5d, "X") | regexm(nic_2004_5d, "Y") 
{txt}(0 observations deleted)

{com}. drop if strlen(nic_2004_5d)!= 5 & !mi(nic_2004_5d) 
{txt}(0 observations deleted)

{com}. replace nic_2004_5d = "" if nic_2004_5d == "00000" 
{txt}(297,863 real changes made)

{com}. 
. gisid person_key
{res}{txt}
{com}. 
. * Saving final dataset
. save "$nss_lab/intermediate/Blk_merged_2009.dta", replace
{txt}{p 0 4 2}
file {bf}
C:/Users/ayush/Dropbox/idl/nss/labour/intermediate/Blk_merged_2009.dta{rm}
saved
{p_end}

{com}. 
. 
. 
{txt}end of do-file

{com}. * Purpose: To clean data at the person-level EUS for year 2011-12 (July-June)                  
. 
. *** ---------------------------------------------------------------------------
. *** Block 5: Usual Activity Status
. *** ---------------------------------------------------------------------------
. 
. * Loading the dataset
. use ///
>         HHID Person_Serial_No /// Common variables
>         Age Usual_Principal_Activity_Status Usual_Principal_Activity_NIC2008 Usual_Principal_Activity_NCO2004 Whether_in_Subsidiary_Activity Location_of_Workspace Enterprise_Type Seeking_available_for_work Enterprise_uses_Electricity  /// Native variables
>         Multiplier_comb /// Multiplier- combined
>         using "$nss_lab/raw/2011/extracted dta files/Block_5_1_Usual principal activity particulars of household members", clear
{txt}
{com}. 
. * Generating person_key
. gegen person_key = concat(HHID Person_Serial_No)
{txt}concat() is not a gtools function and no by(); falling back on egen

{com}. 
. * Dropping irrelevant variables
. drop Person_Serial_No
{txt}
{com}. 
. * Renaming variables
. rename *, lower
{res}{txt}
{com}. rename hhid                                                                     hh_key
{res}{txt}
{com}. rename usual_principal_activity_status          act_code
{res}{txt}
{com}. rename usual_principal_activity_nic2008         nic_2008_5d
{res}{txt}
{com}. rename usual_principal_activity_nco2004         nco_2004_3d
{res}{txt}
{com}. rename whether_in_subsidiary_activity           act_code_sub 
{res}{txt}
{com}. rename location_of_workspace                            work_location
{res}{txt}
{com}. rename enterprise_type                                          ent_type
{res}{txt}
{com}. rename seeking_available_for_work                       seeking_work
{res}{txt}
{com}. rename enterprise_uses_electricity                      use_electricty
{res}{txt}
{com}. rename multiplier_comb                                          weight
{res}{txt}
{com}. 
. * Destringing variables
. destring age act_code act_code_sub ent_type work_location seeking_work, replace
{txt}age already numeric; no {res}replace
{txt}act_code: all characters numeric; {res}replaced {txt}as {res}byte
{txt}act_code_sub: all characters numeric; {res}replaced {txt}as {res}byte
{txt}ent_type: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(348495 missing values generated)
{res}{txt}work_location: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(349072 missing values generated)
{res}{txt}seeking_work: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(42755 missing values generated)
{res}{txt}
{com}. 
. * Generating NIC 2008 at the 3- and 4-digit level
. gen nic_2008_4d = substr(nic_2008_5d,1,4)
{txt}(298,664 missing values generated)

{com}. gen nic_2008_3d = substr(nic_2008_5d,1,3)
{txt}(298,664 missing values generated)

{com}. 
. * Save as a temporary file for later merge
. tempfile pc
{txt}
{com}. save `pc'
{txt}{p 0 4 2}
file {bf}
C:\Users\ayush\AppData\Local\Temp\ST_33c0_000001.tmp{rm}
saved
as .dta format
{p_end}

{com}. 
. *** ---------------------------------------------------------------------------
. *** Block 4: Demographics
. *** ---------------------------------------------------------------------------
. 
. * Loading the dataset
. use ///
>         HHID Person_Serial_No /// Common variables
>         Person_Serial_No Relation_to_Head Sex Marital_Status General_Education Technical_Education Registered_with_Emp_Exchange  /// Native variables
>         Multiplier_comb /// Multiplier- combined
>         using "$nss_lab/raw/2011/extracted dta files/Block_4_Demographic particulars of household members", clear
{txt}
{com}. 
. * Generating person_key
. gegen person_key = concat(HHID Person_Serial_No)
{txt}concat() is not a gtools function and no by(); falling back on egen

{com}. 
. * Renaming variables
. rename *, lower
{res}{txt}
{com}. rename hhid                                                                     hh_key
{res}{txt}
{com}. rename person_serial_no                                         person_srl_no
{res}{txt}
{com}. rename general_education                                        gen_edu_raw
{res}{txt}
{com}. rename technical_education                                      tech_edu_raw
{res}{txt}
{com}. rename registered_with_emp_exchange                     reg_emp_exch
{res}{txt}
{com}. rename multiplier_comb                                          weight
{res}{txt}
{com}. 
. * Merge demographics and usual activity dataset
. merge 1:1 person_key using `pc'
{res}
{txt}{col 5}Result{col 33}Number of obs
{col 5}{hline 41}
{col 5}Not matched{col 30}{res}               0
{txt}{col 5}Matched{col 30}{res}         456,999{txt}  (_merge==3)
{col 5}{hline 41}

{com}. drop _merge
{txt}
{com}. 
. * Save as a temporary file for later merge
. tempfile demographics
{txt}
{com}. save `demographics'
{txt}{p 0 4 2}
file {bf}
C:\Users\ayush\AppData\Local\Temp\ST_33c0_000002.tmp{rm}
saved
as .dta format
{p_end}

{com}. 
. *** ---------------------------------------------------------------------------
. *** Block 5: Person Daily and Weekly Activity Status
. *** ---------------------------------------------------------------------------
. 
. * Load Block 5 dataset (daily + current weekly activity)
. use ///
>         HHID Person_Serial_No /// Common variables
>         Srl_no_of_Activity Status Intensity_7th_Day Intensity_6th_Day Intensity_5th_Day Intensity_4th_Day Intensity_3rd_Day Intensity_2nd_Day Intensity_1st_Day Total_no_days_in_each_activity Wage_and_Salary_Earnings_Cash Wage_and_Salary_Earnings_Kind Wage_and_Salary_Earnings_Total Current_Weekly_Activity_Status Current_Weekly_Activity_NIC_2008 Current_Weekly_Activity_NCO_2004  /// Native variables
>         using "$nss_lab/raw/2011/extracted dta files/Block_5_3_Time disposition during the week ended on ...............dta", clear
{txt}
{com}. 
. * Generating person_key
. gegen person_key = concat(HHID Person_Serial_No)
{txt}concat() is not a gtools function and no by(); falling back on egen

{com}. drop HHID Person_Serial_No
{txt}
{com}. 
. rename *, lower
{res}{txt}
{com}. rename srl_no_of_activity                       cda_srl_no
{res}{txt}
{com}. rename status                                                   cdas
{res}{txt}
{com}. rename intensity_7th_day                                cda_seventh 
{res}{txt}
{com}. rename intensity_6th_day                                cda_sixth
{res}{txt}
{com}. rename intensity_5th_day                                cda_fifth
{res}{txt}
{com}. rename intensity_4th_day                                cda_fourth
{res}{txt}
{com}. rename intensity_3rd_day                                cda_third
{res}{txt}
{com}. rename intensity_2nd_day                                cda_second
{res}{txt}
{com}. rename intensity_1st_day                                cda_first
{res}{txt}
{com}. rename total_no_days_in_each_activity   cda_no_of_days
{res}{txt}
{com}. rename wage_and_salary_earnings_cash    wages_cash
{res}{txt}
{com}. rename wage_and_salary_earnings_kind    wages_kind                      
{res}{txt}
{com}. rename wage_and_salary_earnings_total   wages_total
{res}{txt}
{com}. rename current_weekly_activity_status   cwa_status
{res}{txt}
{com}. rename current_weekly_activity_nic_2008 cwa_nic_2008_5d
{res}{txt}
{com}. rename current_weekly_activity_nco_2004 cwa_nco_2004_3d
{res}{txt}
{com}. 
. * Assert to check that they are constant across person key
. foreach var of varlist cwa_status cwa_nic_2008_5d cwa_nco_2004_3d {c -(}
{txt}  2{com}.         bysort person_key: assert `var' == `var'[1] // assertion is false
{txt}  3{com}. {c )-}
{txt}
{com}. 
. * Reshape daily activity data from long to wide format
. reshape wide cdas cda_seventh cda_sixth cda_fifth cda_fourth cda_third cda_second cda_first cda_no_of_days wages_cash wages_kind wages_total, i(person_key cwa_status cwa_nic_2008_5d cwa_nco_2004_3d) j(cda_srl_no) string
{txt}(j = 1 2 3 4 5)

Data{col 36}Long{col 43}->{col 48}Wide
{hline 77}
Number of observations     {res}     495,016   {txt}->   {res}456,999     
{txt}Number of variables        {res}          17   {txt}->   {res}64          
{txt}j variable (5 values)        {res}cda_srl_no   {txt}->   (dropped)
xij variables:
                                   {res}cdas   {txt}->   {res}cdas1 cdas2 ... cdas5
                            cda_seventh   {txt}->   {res}cda_seventh1 cda_seventh2 ... cda_seventh5
                              cda_sixth   {txt}->   {res}cda_sixth1 cda_sixth2 ... cda_sixth5
                              cda_fifth   {txt}->   {res}cda_fifth1 cda_fifth2 ... cda_fifth5
                             cda_fourth   {txt}->   {res}cda_fourth1 cda_fourth2 ... cda_fourth5
                              cda_third   {txt}->   {res}cda_third1 cda_third2 ... cda_third5
                             cda_second   {txt}->   {res}cda_second1 cda_second2 ... cda_second5
                              cda_first   {txt}->   {res}cda_first1 cda_first2 ... cda_first5
                         cda_no_of_days   {txt}->   {res}cda_no_of_days1 cda_no_of_days2 ... cda_no_of_days5
                             wages_cash   {txt}->   {res}wages_cash1 wages_cash2 ... wages_cash5
                             wages_kind   {txt}->   {res}wages_kind1 wages_kind2 ... wages_kind5
                            wages_total   {txt}->   {res}wages_total1 wages_total2 ... wages_total5
{txt}{hline 77}

{com}. 
. * Merge with previous blocks
. merge 1:1 person_key using `demographics'
{res}
{txt}{col 5}Result{col 33}Number of obs
{col 5}{hline 41}
{col 5}Not matched{col 30}{res}               0
{txt}{col 5}Matched{col 30}{res}         456,999{txt}  (_merge==3)
{col 5}{hline 41}

{com}. drop _merge
{txt}
{com}. 
. * Save as a temporary file for later merge
. tempfile cdas
{txt}
{com}. save `cdas'
{txt}{p 0 4 2}
file {bf}
C:\Users\ayush\AppData\Local\Temp\ST_33c0_000003.tmp{rm}
saved
as .dta format
{p_end}

{com}. 
. *** ---------------------------------------------------------------------------
. *** Merge with Household Characteristics
. *** ---------------------------------------------------------------------------
. 
. * Merge household-level characteristics to person-level dataset
. use ///
>         HHID /// Common variables 
>         Item_Group_Srl_No Value_of_Consumption_Last_30_Day /// Native variables
>         using "$nss_lab/raw/2011/extracted dta files/Block_8_Household consumer expenditure", clear
{txt}
{com}. 
. * Renaming variables
. rename *, lower
{res}{txt}
{com}. rename hhid                                                                     hh_key
{res}{txt}
{com}. 
. * Keeping only total expenditure 
. keep if inlist(item_group_srl_no, "40")
{txt}(2,675,065 observations deleted)

{com}. 
. * Renaming variable
. ren value_of_consumption_last_30_day exp_sum 
{res}{txt}
{com}. 
. drop item_group_srl_no
{txt}
{com}. 
. * Merge with previous blocks
. merge 1:m hh_key using `cdas'
{res}
{txt}{col 5}Result{col 33}Number of obs
{col 5}{hline 41}
{col 5}Not matched{col 30}{res}              29
{txt}{col 9}from master{col 30}{res}               0{txt}  (_merge==1)
{col 9}from using{col 30}{res}              29{txt}  (_merge==2)

{col 5}Matched{col 30}{res}         456,970{txt}  (_merge==3)
{col 5}{hline 41}

{com}. assert _merge != 1
{txt}
{com}. drop _merge
{txt}
{com}. 
. *** ---------------------------------------------------------------------------
. *** Merge with Household Characteristics
. *** ---------------------------------------------------------------------------
. 
. * Merge household-level characteristics to person-level dataset
. merge m:1 hh_key using "$nss_lab/intermediate/Blk_hc_2011"
{res}
{txt}{col 5}Result{col 33}Number of obs
{col 5}{hline 41}
{col 5}Not matched{col 30}{res}               0
{txt}{col 5}Matched{col 30}{res}         456,999{txt}  (_merge==3)
{col 5}{hline 41}

{com}. count if _merge !=3
  {res}0
{txt}
{com}. assert `r(N)' < 10 // to ensure quality of merge
{txt}
{com}. drop _merge 
{txt}
{com}. 
. * Generating monthly total per capita expenditure
. gen total_exp_hh = exp_sum/hh_size
{txt}(29 missing values generated)

{com}. 
. * Generating year and round variable
. gen year = 2011
{txt}
{com}. gen nss_round = 68
{txt}
{com}. 
. * Clean industry codes:
. *   - Remove spaces
. *   - Drop records with placeholder characters (X, Y)
. *   - Drop invalid or incomplete codes
. replace nic_2008_5d = subinstr(nic_2008_5d, " ", "",.)
{txt}(0 real changes made)

{com}. drop if regexm(nic_2008_5d, "x") | regexm(nic_2008_5d, "X") | regexm(nic_2008_5d, "Y") 
{txt}(0 observations deleted)

{com}. drop if strlen(nic_2008_5d)!= 5 & !mi(nic_2008_5d) 
{txt}(0 observations deleted)

{com}. drop if nic_2008_5d == "00000" 
{txt}(0 observations deleted)

{com}. 
. gisid person_key
{res}{txt}
{com}. 
. * Saving final dataset
. save "$nss_lab/intermediate/Blk_merged_2011.dta", replace
{txt}{p 0 4 2}
file {bf}
C:/Users/ayush/Dropbox/idl/nss/labour/intermediate/Blk_merged_2011.dta{rm}
saved
{p_end}

{com}. 
{txt}end of do-file

{com}. * Append Hosuehold and Person characterstics merged datasets of NSS Labour Surveys 
. do "$code/nss/nss_lab/01_clean_append_lab.do"
{txt}
{com}. 
. /*
>         Purpose: APPEND  MERGED DATASET AND LABEL VARIABLES IN FINAL DATASET . 
>                          
>         SECTION 1.  APPEND INDIVIDUAL CHARACTERSTICS AND HOUSEHOLD CHARACTERSTICS MERGED DATASETS 
>         
>         SECTION 3.      HARMONISE THE VARIABLES
>                                 
>         SECTION 2.      LABEL VARIABLES IN FINAL OUTPUT FILE 
>         
> */
. 
. * 1. APPEND INDIVIDUAL CHARACTERSTICS AND HOUSEHOLD CHARACTERSTICS MERGED DATASETS 
.         
.         use "$nss_lab/intermediate/Blk_merged_1987.dta", clear
{txt}
{com}. 
.         foreach yr in "1983" "1993" "1999" "2004a" "2004b" "2005" "2007" "2009" "2011" {c -(}
{txt}  2{com}.                 
.                 append using "$nss_lab/intermediate/Blk_merged_`yr'.dta"
{txt}  3{com}.         {c )-}
{txt}{p 0 7 2}
(variable
{bf:hh_key} was {bf:str8}, now {bf:str11} to accommodate using data's values)
{p_end}
{p 0 7 2}
(variable
{bf:person_key} was {bf:str11}, now {bf:str13} to accommodate using data's values)
{p_end}
{p 0 7 2}
(variable
{bf:weight} was {bf:int}, now {bf:long} to accommodate using data's values)
{p_end}
{p 0 7 2}
(variable
{bf:weight} was {bf:long}, now {bf:double} to accommodate using data's values)
{p_end}
{p 0 7 2}
(variable
{bf:person_key} was {bf:str13}, now {bf:str14} to accommodate using data's values)
{p_end}
{p 0 7 2}
(variable
{bf:hh_size} was {bf:byte}, now {bf:float} to accommodate using data's values)
{p_end}
{p 0 7 2}
(variable
{bf:age} was {bf:byte}, now {bf:int} to accommodate using data's values)
{p_end}

{com}. 
. * 2. HARMONISE THE VARIABLES
. 
.         do "$code/nss/nss_lab/01_variable_clean.do"
{txt}
{com}. /* Purpose: To generate harmonized variables and add variable and value labels
> */
. 
. ********************************************************************************
. *** SECTOR *********************************************************************
. ********************************************************************************
. 
. *generating dummy variable for rural
. gen rural = .
{txt}(5,225,776 missing values generated)

{com}. replace rural = 1 if sector == 1
{txt}(3,316,296 real changes made)

{com}. replace rural = 0 if sector == 2
{txt}(1,909,089 real changes made)

{com}. drop sector 
{txt}
{com}. 
. ********************************************************************************
. *** HOUSEHOLD EMPLOYMENT TYPE **************************************************
. ********************************************************************************
. 
. *generating a categorical variable for emp_type_hh
. gen emp_type_hh = .
{txt}(5,225,776 missing values generated)

{com}. 
. replace emp_type_hh = 1 if inlist(emp_type_hh1, 12, 14) & year != 2011 // agricultural labourers
{txt}(1,899,436 real changes made)

{com}. replace emp_type_hh = 2 if inlist(emp_type_hh1, 11, 21) & year != 2011 // self employed in non agriculture
{txt}(1,223,179 real changes made)

{com}. replace emp_type_hh = 3 if inlist(emp_type_hh1, 22) & year != 2011 // regular wage/ salary earning
{txt}(593,775 real changes made)

{com}. replace emp_type_hh = 4 if inlist(emp_type_hh1, 13, 23) & year != 2011 // casual labour
{txt}(457,751 real changes made)

{com}. replace emp_type_hh = 5 if inlist(emp_type_hh1, 19, 29) & year != 2011 // other labour
{txt}(591,372 real changes made)

{com}. 
. replace emp_type_hh = 1 if inlist(emp_type_hh1, 11, 14) & year == 2011 // agricultural labourers
{txt}(107,547 real changes made)

{com}. replace emp_type_hh = 2 if inlist(emp_type_hh1, 12, 21) & year == 2011 // self employed in non agriculture
{txt}(148,179 real changes made)

{com}. replace emp_type_hh = 3 if inlist(emp_type_hh1, 22) & year == 2011 // regular wage/ salary earning
{txt}(67,134 real changes made)

{com}. replace emp_type_hh = 4 if inlist(emp_type_hh1, 13, 23) & year == 2011 // casual labour in non agriculture
{txt}(73,077 real changes made)

{com}. replace emp_type_hh = 5 if inlist(emp_type_hh1, 15, 19, 29) & year == 2011 // other labour
{txt}(60,986 real changes made)

{com}. 
. *Creating dummy variables for each emp type
. gen hh_employed_agri = emp_type_hh == 1
{txt}
{com}. gen hh_self_employed_non_agri = emp_type_hh == 2
{txt}
{com}. gen hh_regular_wage = emp_type_hh == 3
{txt}
{com}. gen hh_casual_labor = emp_type_hh == 4
{txt}
{com}. gen hh_other_labor = emp_type_hh == 5
{txt}
{com}. 
. ********************************************************************************
. *** CASTE **********************************************************************
. ********************************************************************************
. 
. *generating dummy variable for sc
. gen sc = .
{txt}(5,225,776 missing values generated)

{com}. replace sc = 1 if caste == 2
{txt}(804,116 real changes made)

{com}. replace sc = 0 if inlist(caste, 1, 3, 9)
{txt}(4,419,924 real changes made)

{com}. 
. *generating dummy variable for st
. gen st = .
{txt}(5,225,776 missing values generated)

{com}. replace st = 1 if caste == 1
{txt}(620,244 real changes made)

{com}. replace st = 0 if inlist(caste, 2, 3, 9)
{txt}(4,603,796 real changes made)

{com}. 
. *generating dummy variable for others
. gen caste_others = .
{txt}(5,225,776 missing values generated)

{com}. replace caste_others = 1 if caste == 3 | caste == 9
{txt}(3,799,680 real changes made)

{com}. replace caste_others = 0 if caste == 1 | caste == 2
{txt}(1,424,360 real changes made)

{com}. drop caste
{txt}
{com}. 
. ********************************************************************************
. *** RELIGION *******************************************************************
. ********************************************************************************
. 
. *generating dummy variable for muslim
. gen muslim = .
{txt}(5,225,776 missing values generated)

{com}. replace muslim = 1 if religion == 2
{txt}(714,250 real changes made)

{com}. replace muslim = 0 if religion != 2 
{txt}(4,511,526 real changes made)

{com}. replace muslim = . if religion == 0
{txt}(163 real changes made, 163 to missing)

{com}. 
. *generating dummy variable for hindu
. gen hindu = .
{txt}(5,225,776 missing values generated)

{com}. replace hindu = 1 if religion == 1
{txt}(3,971,232 real changes made)

{com}. replace hindu = 0 if religion != 1 
{txt}(1,254,544 real changes made)

{com}. replace hindu = . if religion == 0
{txt}(163 real changes made, 163 to missing)

{com}. 
. *generating dummy variable for other religions
. gen religion_others = 0
{txt}
{com}. replace religion_others = 1 if religion != 1 | religion != 2 
{txt}(5,225,776 real changes made)

{com}. replace religion_others = . if religion == 0
{txt}(163 real changes made, 163 to missing)

{com}. 
. 
. ********************************************************************************
. *** PERSON EMPLOYMENT TYPE *****************************************************
. ********************************************************************************
. 
. *generating categorical variables for emp_type at the person level
. gen emp_type = .
{txt}(5,225,776 missing values generated)

{com}. replace emp_type = 1 if inlist(act_code, 11, 21) & ( year == 1983 | year == 1987) // self-employed
{txt}(105,582 real changes made)

{com}. replace emp_type = 2 if inlist(act_code, 31) & ( year == 1983 | year == 1987) // salaried/ wage
{txt}(80,717 real changes made)

{com}. replace emp_type = 3 if inlist(act_code, 41, 51) & ( year == 1983 | year == 1987) // casual/ daily wage labour
{txt}(40,479 real changes made)

{com}. 
. replace emp_type = 1 if inlist(act_code, 11, 12, 21) & year != 1987 // self-employed
{txt}(771,820 real changes made)

{com}. replace emp_type = 2 if inlist(act_code, 31) & year != 1987 // salaried/ wage
{txt}(298,945 real changes made)

{com}. replace emp_type = 3 if inlist(act_code, 41, 51) & year != 1987  // casual/ daily wage labour
{txt}(335,523 real changes made)

{com}. 
. *Creating dummy variables for each emp type person
. gen p_self_employed = emp_type == 1
{txt}
{com}. gen p_salary_earning = emp_type == 2
{txt}
{com}. gen p_casual_wage = emp_type == 3
{txt}
{com}. 
. ********************************************************************************
. *** PERSON EMPLOYMENT STATUS ***************************************************
. ********************************************************************************
. 
. *generating a dummy variable for people out of the labour force
. gen olf = .
{txt}(5,225,776 missing values generated)

{com}. replace olf = 1 if inlist(act_code, 91, 92, 93, 94, 95, 96, 97, 98, 99)
{txt}(3,281,212 real changes made)

{com}. replace olf = 0 if inlist(act_code, 11, 21, 31, 41, 51, 61, 62, 71, 72, 81)
{txt}(1,685,873 real changes made)

{com}. replace olf = . if act_code == 99 & year == 1993
{txt}(0 real changes made)

{com}. 
. *generating a dummy variable for employed 
. gen employed = .
{txt}(5,225,776 missing values generated)

{com}. replace employed = 1 if inlist(act_code, 11, 21, 31, 41, 51)
{txt}(1,610,628 real changes made)

{com}. replace employed = 0 if inlist(act_code, 61, 62, 71, 72, 81, 91, 92, 93, 94, 95, 96, 97, 98, 99)
{txt}(3,356,457 real changes made)

{com}. replace employed = . if act_code == 99 & year == 1993
{txt}(0 real changes made)

{com}. 
. *generating a dummy variable for unemployed
. gen unemployed = .
{txt}(5,225,776 missing values generated)

{com}. replace unemployed = 1 if inlist(act_code, 61, 62, 71, 72, 81)
{txt}(75,245 real changes made)

{com}. replace unemployed = 0 if inlist(act_code, 11, 21, 31, 41, 51, 91, 92, 93, 94, 95, 96, 97, 98, 99)
{txt}(4,891,840 real changes made)

{com}. replace unemployed = . if act_code == 99 & year == 1993
{txt}(0 real changes made)

{com}. 
. 
. ********************************************************************************
. *** SEX  ***********************************************************************
. ********************************************************************************
. 
. *generating dummy variable for male
. gen male = .
{txt}(5,225,776 missing values generated)

{com}. replace male = 1 if sex == "1"
{txt}(2,690,459 real changes made)

{com}. replace male = 0 if sex == "2"
{txt}(2,534,818 real changes made)

{com}. drop sex
{txt}
{com}. 
. *generating dummy variable for female
. gen female = male == 0
{txt}
{com}. 
. 
. ********************************************************************************
. *** GENERAL EDUCATION LEVEL ****************************************************
. ********************************************************************************
. 
. gen gen_edu = .
{txt}(5,225,776 missing values generated)

{com}. replace gen_edu = 1 if gen_edu_raw == "00" & (year == 1983 | year == 1987) // not literate
{txt}(673,952 real changes made)

{com}. replace gen_edu = 2 if inlist(gen_edu_raw, "01", "02") & (year == 1983 | year == 1987) // literate but below primary
{txt}(213,142 real changes made)

{com}. replace gen_edu = 3 if gen_edu_raw == "03" & (year == 1983 | year == 1987) // primary 
{txt}(165,899 real changes made)

{com}. replace gen_edu = 4 if gen_edu_raw == "04" & (year == 1983 | year == 1987) // middle
{txt}(115,195 real changes made)

{com}. replace gen_edu = 5 if gen_edu_raw == "05" & (year == 1983 | year == 1987) // secondary
{txt}(89,903 real changes made)

{com}. replace gen_edu = 6 if inlist(gen_edu_raw, "06", "07", "08", "09") & (year == 1983 | year == 1987) // graduate and above
{txt}(30,967 real changes made)

{com}. 
. replace gen_edu = 1 if gen_edu_raw == "01" & (year == 1993 | year == 1999) // not literate
{txt}(228,741 real changes made)

{com}. replace gen_edu = 2 if inlist(gen_edu_raw, "02", "03", "04", "05") & (year == 1993 | year == 1999) // literate but below primary
{txt}(109,628 real changes made)

{com}. replace gen_edu = 3 if gen_edu_raw == "06" & (year == 1993 | year == 1999) // primary 
{txt}(75,816 real changes made)

{com}. replace gen_edu = 4 if gen_edu_raw == "07" & (year == 1993 | year == 1999) // middle
{txt}(75,591 real changes made)

{com}. replace gen_edu = 5 if inlist(gen_edu_raw, "08", "09") & (year == 1993 | year == 1999) // secondary
{txt}(77,857 real changes made)

{com}. replace gen_edu = 6 if inlist(gen_edu_raw, "10", "11", "12", "13") & (year == 1993 | year == 1999) // graduate and above
{txt}(28,325 real changes made)

{com}. 
. replace gen_edu = 1 if gen_edu_raw == "01" & year == 2004 & nss_round == 60 // not literate
{txt}(102,415 real changes made)

{com}. replace gen_edu = 2 if inlist(gen_edu_raw, "02", "03") & year == 2004 & nss_round == 60 // literate but below primary
{txt}(45,218 real changes made)

{com}. replace gen_edu = 3 if gen_edu_raw == "04" & year == 2004 & nss_round == 60 // primary 
{txt}(42,584 real changes made)

{com}. replace gen_edu = 4 if gen_edu_raw == "05" & year == 2004 & nss_round == 60 // middle
{txt}(45,647 real changes made)

{com}. replace gen_edu = 5 if inlist(gen_edu_raw, "06", "07", "08") & year == 2004 & nss_round == 60 // secondary, higher secondary, diploma/certificate
{txt}(52,129 real changes made)

{com}. replace gen_edu = 6 if inlist(gen_edu_raw, "10", "11") & year == 2004 & nss_round == 60 // graduate and above
{txt}(15,678 real changes made)

{com}. 
. replace gen_edu = 1 if gen_edu_raw == "01" & year == 2004 & nss_round == 61 // not literate
{txt}(205,313 real changes made)

{com}. replace gen_edu = 2 if inlist(gen_edu_raw, "02", "03", "04", "05") & year == 2004 & nss_round == 61 // literate but below primary
{txt}(111,073 real changes made)

{com}. replace gen_edu = 3 if gen_edu_raw == "06" & year == 2004 & nss_round == 61 // primary 
{txt}(88,352 real changes made)

{com}. replace gen_edu = 4 if gen_edu_raw == "07" & year == 2004 & nss_round == 61 // middle
{txt}(84,369 real changes made)

{com}. replace gen_edu = 5 if inlist(gen_edu_raw, "08", "10", "11") & year == 2004 & nss_round == 61 // secondary, higher secondary, diploma/certificate
{txt}(85,023 real changes made)

{com}. replace gen_edu = 6 if inlist(gen_edu_raw, "12", "13") & year == 2004 & nss_round == 61 // graduate and above
{txt}(28,290 real changes made)

{com}. 
. replace gen_edu = 1 if gen_edu_raw == "01" & inlist(year, 2005, 2009, 2011) // not literate
{txt}(352,475 real changes made)

{com}. replace gen_edu = 2 if inlist(gen_edu_raw, "02", "03", "04", "05") & inlist(year, 2005, 2009, 2011) // literate but below primary
{txt}(214,717 real changes made)

{com}. replace gen_edu = 3 if gen_edu_raw == "06" & inlist(year, 2005, 2009, 2011) // primary 
{txt}(181,627 real changes made)

{com}. replace gen_edu = 4 if gen_edu_raw == "07" & inlist(year, 2005, 2009, 2011) // middle
{txt}(197,151 real changes made)

{com}. replace gen_edu = 5 if inlist(gen_edu_raw, "08", "10", "11") & inlist(year, 2005, 2009, 2011) // secondary, higher secondary, diploma/certificate
{txt}(254,569 real changes made)

{com}. replace gen_edu = 6 if inlist(gen_edu_raw, "12", "13") & inlist(year, 2005, 2009, 2011) // graduate and above
{txt}(92,044 real changes made)

{com}. 
. replace gen_edu = 1 if gen_edu_raw == "01" & inlist(year, 2007) // not literate
{txt}(190,109 real changes made)

{com}. replace gen_edu = 2 if inlist(gen_edu_raw, "02", "03", "04", "05", "06") & inlist(year, 2007) // literate but below primary
{txt}(97,358 real changes made)

{com}. replace gen_edu = 3 if gen_edu_raw == "07" & inlist(year, 2007) // primary 
{txt}(83,274 real changes made)

{com}. replace gen_edu = 4 if gen_edu_raw == "08" & inlist(year, 2007) // middle
{txt}(82,565 real changes made)

{com}. replace gen_edu = 5 if inlist(gen_edu_raw, "10", "11", "12") & inlist(year, 2007) // secondary, higher secondary, diploma/certificate
{txt}(86,642 real changes made)

{com}. replace gen_edu = 6 if inlist(gen_edu_raw, "13", "14") & inlist(year, 2007) // graduate and above
{txt}(31,462 real changes made)

{com}. 
. *Generating dummy variables for gen edu 
. gen not_literate = gen_edu == 1
{txt}
{com}. gen below_primary = gen_edu == 2
{txt}
{com}. gen primary = gen_edu == 3
{txt}
{com}. gen middle = gen_edu == 4
{txt}
{com}. gen secondary = gen_edu == 5
{txt}
{com}. gen graduate_and_above = gen_edu == 6
{txt}
{com}. 
. 
. ********************************************************************************
. *** TECHNICAL EDUCATION LEVEL **************************************************
. ********************************************************************************
. 
. gen tech_edu = .
{txt}(5,225,776 missing values generated)

{com}. replace tech_edu = 1 if tech_edu_raw == "15" & year == 1983 // no technical education 
{txt}(616,823 real changes made)

{com}. replace tech_edu = 3 if inlist(tech_edu_raw, "10", "11", "12", "13", "14") & year == 1983 // diploma/certificate
{txt}(6,623 real changes made)

{com}. 
. replace tech_edu = 1 if tech_edu_raw == "00" & year == 1987 // no technical education 
{txt}(659,279 real changes made)

{com}. replace tech_edu = 3 if inlist(tech_edu_raw, "01", "02", "03", "04", "05") & year == 1987 // diploma/certificate
{txt}(8,525 real changes made)

{com}. 
. replace tech_edu = 1 if tech_edu_raw == "1" & inlist(year, 1993) // no technical education 
{txt}(0 real changes made)

{com}. replace tech_edu = 3 if inlist(tech_edu_raw, "2", "3", "4", "5", "9") & inlist(year, 1993) // diploma/certificate
{txt}(0 real changes made)

{com}. 
. replace tech_edu = 1 if tech_edu_raw == "1" & inlist(year, 1999) // no technical education 
{txt}(585,799 real changes made)

{com}. replace tech_edu = 2 if tech_edu_raw == "2" & inlist(year, 1999) // technical degree
{txt}(2,051 real changes made)

{com}. replace tech_edu = 3 if inlist(tech_edu_raw, "3", "4", "5", "6", "9") & inlist(year, 1999) // diploma/certificate
{txt}(8,836 real changes made)

{com}. 
. replace tech_edu = 1 if tech_edu_raw == "1" & nss_round == 60 // no technical education 
{txt}(292,733 real changes made)

{com}. replace tech_edu = 2 if tech_edu_raw == "2" & nss_round == 60  // technical degree
{txt}(1,113 real changes made)

{com}. replace tech_edu = 3 if inlist(tech_edu_raw, "3", "4", "5", "6", "9") & nss_round == 60  // diploma/certificate
{txt}(5,401 real changes made)

{com}. 
. replace tech_edu = 1 if tech_edu_raw == "01" & inlist(year, 2005, 2009, 2011) | nss_round == 61  // no technical education 
{txt}(1,871,145 real changes made)

{com}. replace tech_edu = 2 if tech_edu_raw == "02"  & inlist(year, 2005, 2009, 2011) | nss_round == 61  // technical degree
{txt}(606,110 real changes made)

{com}. replace tech_edu = 3 if (inlist(tech_edu_raw, "03", "04", "05", "06", "07", "08") | inlist(tech_edu_raw, "09", "10", "11", "12"))  & (inlist(year, 2005, 2009, 2011) | nss_round == 61)  // diploma/certificate
{txt}(30,666 real changes made)

{com}. 
. replace tech_edu = 1 if tech_edu_raw == "1" & inlist(year, 2007) // no technical education 
{txt}(563,898 real changes made)

{com}. replace tech_edu = 2 if (tech_edu_raw == "2" | tech_edu_raw == "3") & inlist(year, 2007) // technical degree
{txt}(2,277 real changes made)

{com}. replace tech_edu = 3 if inlist(tech_edu_raw, "4", "5", "6") & inlist(year, 2007) // diploma/certificate
{txt}(5,251 real changes made)

{com}. //drop tech_edu_raw
. 
. *Generating dummy variables for tech edu 
. gen no_tech_edu = tech_edu == 1
{txt}
{com}. gen tech_degree = tech_edu == 2
{txt}
{com}. gen tech_diploma = tech_edu == 3
{txt}
{com}. 
. /* Note: in 1987 we don't have any category for technical degree
> 
>            00   No Technical Education  
>            01   Additional Diploma/Certificate in Agriculture
>            02   Additional Diploma/Certificate in Engineering/Technology
>            03   Additional Diploma/Certificate in Medicine
>            04   Additional Diploma/Certificate in Crafts
>            05   Additional Diploma/Certificate in Other Subjects
> 
> */
. ********************************************************************************
. *** MARITAL STATUS *************************************************************
. ********************************************************************************
. 
. destring marital_status, replace
{txt}marital_status: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(565610 missing values generated)
{res}{txt}
{com}. replace marital_status = . if marital_status == 0
{txt}(467 real changes made, 467 to missing)

{com}. gen never_married = marital_status == 1
{txt}
{com}. gen currently_married = marital_status == 2
{txt}
{com}. gen widowed = marital_status == 3
{txt}
{com}. gen divorced_separated = marital_status == 4
{txt}
{com}. 
. ********************************************************************************
. *** WAGES **********************************************************************
. ********************************************************************************
. 
. /* For each current daily activity performed by a person we have wages reported:
>            1. in kind
>            2. in cash
> 
>            and the total of the above. 
> 
>            I generate a variable for the total wages earned by a person by summing those above.
> */
. 
. gegen wages_cash = rowtotal(wages_cash*)
{txt}rowtotal() is not a gtools function and no by(); falling back on egen

{com}. gegen wages_kind = rowtotal(wages_kind*)
{txt}rowtotal() is not a gtools function and no by(); falling back on egen

{com}. gegen wages_total = rowtotal(wages_total*)
{txt}rowtotal() is not a gtools function and no by(); falling back on egen

{com}. 
. 
. 
{txt}end of do-file

{com}. 
. * 3. ADD VALUE LABELS
. 
.         label define emp_type_p 1 "Self-employed worker" 2 "Regular salaried/ wage employee" 3 "Casual wage workers"
{txt}
{com}.         label value emp_type emp_type_p
{txt}
{com}. 
.         label define emp_type_hh 1 "Employed in agriculture" 2 "Self-employed in non-agriculture" 3 "Regular wage/ salary earning" 4 "Casual labour" 5 "Other households"
{txt}
{com}.         label value emp_type_hh emp_type_hh
{txt}
{com}.         
.         label define gen_edu 1 "Not literate" 2 "Literate but below primary" 3 "Primary" 4 "Middle" 5 "Secondary" 6 "Graduate and above"
{txt}
{com}.         label value gen_edu gen_edu
{txt}
{com}.         
.         label define tech_edu 1 "No technical education" 2 "Technical degree" 3 "Technical diploma/certificate"
{txt}
{com}.         label value tech_edu tech_edu
{txt}
{com}. 
. * 3. LABEL VARIABLES
. 
.         la var person_key                       "Person ID"
{txt}
{com}.         la var year                             "Survey Year"
{txt}
{com}.         la var nss_round                        "NSS Round"
{txt}
{com}.         la var rural                            "Sector: rural = 1; urban = 0"
{txt}
{com}.         la var nss_region                       "NSS Region"
{txt}
{com}.         la var dist_code                        "District"
{txt}
{com}.         la var emp_type_hh                      "Household Employment Type"
{txt}
{com}.         la var male                             "Sex: male = 1; female = 0"
{txt}
{com}.         la var act_code                         "Usual Principal Activity Status"
{txt}
{com}.         la var nic_2004_5d                      "Usual principal activity- NIC- 2004"
{txt}
{com}.         la var nco_2004_3d                      "Usual principal activity- NCO- 2004"
{txt}
{com}.         la var act_code_sub             "Engaged in subsidiary activity: Yes = 1; No = 0"
{txt}
{com}.         la var emp_type                         "Person Employment Type"
{txt}
{com}.         la var olf                                      "Person Employment Type: Out of labour force = 1, Others = 0"
{txt}
{com}.         la var employed                         "Person Employment Type: Employed = 1, Others = 0"
{txt}
{com}.         la var unemployed                       "Person Employment Type: Unemployed = 1, Others = 0"
{txt}
{com}.         la var p_self_employed          "Self-Employed"
{txt}
{com}.         la var p_salary_earning         "Regular Wage/ Salary Earning"
{txt}
{com}.         la var p_casual_wage            "Casual/ Daily Wage Laborer"
{txt}
{com}.         la var hh_self_employed_non_agri "Self-Employed in Non-Agriculture"
{txt}
{com}.         la var hh_employed_agri         "Employed in Agriculture"
{txt}
{com}.         la var hh_regular_wage          "Regular Wage Household"
{txt}
{com}.         la var hh_casual_labor          "Casual Labour Household"
{txt}
{com}.         la var muslim                           "Religion: Muslim = 1, Other = 0"
{txt}
{com}.         la var hindu                            "Religion: Hindu = 1, Other = 0"
{txt}
{com}.         la var religion_others          "Religion: Others = 1, Hindu & Muslim = 0"
{txt}
{com}.         la var sc                                       "Caste: Scheduled Caste = 1, Others = 0"
{txt}
{com}.         la var st                                       "Caste: Scheduled Tribe = 1, Others = 0"
{txt}
{com}.         la var caste_others             "Caste: Others = 1, SC & ST = 0"
{txt}
{com}.         la var weight                           "Multiplier Combined"
{txt}
{com}.         la var marital_status           "Marital Status"
{txt}
{com}.         la var total_exp_hh             "Total Monthly Per Capita Expenditure (INR)"
{txt}
{com}.         la var not_literate             "Not literate"
{txt}
{com}.         la var below_primary            "Below primary"
{txt}
{com}.         la var primary                          "Primary"
{txt}
{com}.         la var middle                           "Middle"
{txt}
{com}.         la var secondary                        "Secondary"
{txt}
{com}.         la var graduate_and_above       "Graduate and above"
{txt}
{com}.         la var no_tech_edu                      "No technical education"
{txt}
{com}.         la var tech_degree                      "Technical degree"
{txt}
{com}.         la var tech_diploma             "Technical diploma/ certificate"
{txt}
{com}.         la var nic_1987_2d                      "Employed principal activity: NIC 1987 2 digit"
{txt}
{com}.         la var nic_1987_3d                      "Employed principal activity: NIC 1987 3 digit"
{txt}
{com}.         la var nic_1998_3d                      "Employed principal activity: NIC 1998 3 digit"
{txt}
{com}.         la var nic_1998_4d                      "Employed principal activity: NIC 1998 4 digit"
{txt}
{com}.         la var nic_1998_5d                      "Employed principal activity: NIC 1998 5 digit"
{txt}
{com}.         la var nic_2004_3d                      "Employed principal activity: NIC 2004 3 digit"
{txt}
{com}.         la var nic_2004_4d                      "Employed principal activity: NIC 2004 4 digit"
{txt}
{com}.         la var nic_2004_5d                      "Employed principal activity: NIC 2004 5 digit"
{txt}
{com}.         la var nic_2008_3d                      "Employed principal activity: NIC 2008 3 digit"
{txt}
{com}.         la var nic_2008_4d                      "Employed principal activity: NIC 2008 4 digit"
{txt}
{com}.         la var nic_2008_5d                      "Employed principal activity: NIC 2008 5 digit"
{txt}
{com}.         
.         compress
  {txt}variable {bf}{res}hh_size{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}year{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}int{sf}
  {txt}variable {bf}{res}nss_round{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}rural{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}emp_type_hh{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}hh_employed_agri{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}hh_self_employed_non_agri{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}hh_regular_wage{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}hh_casual_labor{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}hh_other_labor{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}sc{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}st{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}caste_others{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}muslim{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}hindu{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}religion_others{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}emp_type{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}p_self_employed{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}p_salary_earning{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}p_casual_wage{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}olf{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}employed{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}unemployed{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}male{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}female{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}gen_edu{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}not_literate{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}below_primary{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}primary{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}middle{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}secondary{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}graduate_and_above{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}tech_edu{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}no_tech_edu{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}tech_degree{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}tech_diploma{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}never_married{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}currently_married{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}widowed{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}divorced_separated{sf}{txt} was {bf}{res}float{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}wages_kind0{sf}{txt} was {bf}{res}double{sf}{txt} now {bf}{res}int{sf}
  {txt}variable {bf}{res}wages_cash5{sf}{txt} was {bf}{res}double{sf}{txt} now {bf}{res}int{sf}
  {txt}variable {bf}{res}wages_kind5{sf}{txt} was {bf}{res}double{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}wages_total5{sf}{txt} was {bf}{res}double{sf}{txt} now {bf}{res}int{sf}
  {txt}variable {bf}{res}wages_kind6{sf}{txt} was {bf}{res}double{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}wages_kind7{sf}{txt} was {bf}{res}double{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}wages_cash8{sf}{txt} was {bf}{res}double{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}wages_kind8{sf}{txt} was {bf}{res}double{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}wages_total8{sf}{txt} was {bf}{res}double{sf}{txt} now {bf}{res}byte{sf}
  {txt}variable {bf}{res}wages_kind9{sf}{txt} was {bf}{res}double{sf}{txt} now {bf}{res}byte{sf}
{txt}  (971,994,336 bytes saved)

{com}.         lab data "NSS Labour: 1987- 2011"
{txt}
{com}.         
.         #d ;
{txt}delimiter now ;
{com}.         keep 
>         hh_key
>         person_key person_srl_no
>         year
>         nss_round
>         block 
>         rural
>         st_code nss_region dist_code relation_to_head
>         nic_1970_3d nic_1987_2d nic_1987_3d
>         nic_1998_3d nic_1998_4d nic_1998_5d
>         nic_2004_3d nic_2004_4d nic_2004_5d
>         nic_2008_3d nic_2008_4d nic_2008_5d
>         hh_size religion emp_type_hh hh_casual_labor hh_employed_agri hh_regular_wage hh_self_employed_non_agri hh_other_labor
>         emp_type_hh1 total_exp_hh never_married currently_married widowed divorced_separated
>         sc st caste_others muslim hindu religion_others p_self_employed p_salary_earning p_casual_wage
>         age male female emp_type olf employed unemployed gen_edu tech_edu
>         not_literate below_primary primary middle secondary graduate_and_above no_tech_edu tech_degree tech_diploma
>         act_code act_code_sub sub_act_no
>         nco_1968_3d nco_2004_3d marital_status
>         wages_cash wages_kind wages_total
>         //skill work_location ent_type seeking_work 
>         weight;
{txt}
{com}.                 #d cr 
{txt}delimiter now cr
{com}.         
.         #d ;
{txt}delimiter now ;
{com}.         order 
>         hh_key
>         person_key person_srl_no
>         year
>         nss_round
>         block 
>         rural
>         st_code nss_region dist_code relation_to_head
>         nic_1970_3d nic_1987_2d nic_1987_3d
>         nic_1998_3d nic_1998_4d nic_1998_5d
>         nic_2004_3d nic_2004_4d nic_2004_5d
>         nic_2008_3d nic_2008_4d nic_2008_5d
>         hh_size religion never_married currently_married widowed divorced_separated
>         emp_type_hh hh_casual_labor hh_employed_agri hh_regular_wage hh_self_employed_non_agri hh_other_labor emp_type_hh1 
>         total_exp_hh 
>         gen_edu tech_edu 
>         not_literate below_primary primary middle secondary graduate_and_above no_tech_edu tech_degree tech_diploma marital_status
>         sc st caste_others muslim hindu religion_others
>         age male female emp_type p_*
>         olf employed unemployed p_self_employed p_salary_earning p_casual_wage
>         act_code act_code_sub sub_act_no
>         nco_1968_3d nco_2004_3d
>         wages_cash wages_kind wages_total
>         //skill work_location ent_type seeking_work 
>         weight;
{txt}
{com}.                 #d cr 
{txt}delimiter now cr
{com}. 
.         destring st_code dist_code, replace
{txt}st_code: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(391 missing values generated)
{res}{txt}dist_code: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(1188583 missing values generated)
{res}{txt}
{com}. 
. * 4. SAVE THE FINAL DATASET
.         save "$nss_lab/intermediate/nss_lab_clean.dta", replace
{txt}{p 0 4 2}
file {bf}
C:/Users/ayush/Dropbox/idl/nss/labour/intermediate/nss_lab_clean.dta{rm}
saved
{p_end}

{com}. 
. 
{txt}end of do-file

{com}. 
. * Making District names consistent on 1991 district names. 
. do "$code/nss/nss_lab/02_nss_consistent_districts.do"
{txt}
{com}. /*
> 
>         Purpose: Merges NSS labour data with district_names data to bring district_names_91. Merges
>                           has been done on round x st_code x dist_code x nss_region variables.
> 
> */
. 
. * 1. CLEANING THE DISTRICT NAMES FILE
.         
.         * Loading and Cleaning Districts file 
.         import excel using "$idl_git/documentation/district_concordance/nss_lab_district.xlsx", firstrow clear 
{res}{text}(11 vars, 5,086 obs)

{com}.         
.         drop if nss_round == 38 // dropping years 1983 which doesn't have district codes
{txt}(32 observations deleted)

{com}. 
.         * Converting to lower case
.         foreach var in st_name_1991 dist_name_1991 {c -(}
{txt}  2{com}.             replace `var' = lower(`var')
{txt}  3{com}.                 replace `var' = ustrtrim(`var')
{txt}  4{com}.         {c )-}
{txt}(5,054 real changes made)
(0 real changes made)
(5,054 real changes made)
(2 real changes made)

{com}. 
.         /* To make consistent district names, district names for all the years have been been given respective 1991 district name. 
>         In the above excel file, given that data is unique at nss_round X st_code X dist_code X nss_region, we are adding nss_round in the excel
>         to merge the datasets. */
.         
.         **Round 60 and 61 have similar NSS regions, so we fill the nss regions for round 61 similar to the ones in round 60
.         preserve
{txt}
{com}.         
.         keep if nss_round == 60 | nss_round == 61
{txt}(3,861 observations deleted)

{com}.         
.         *For each district we take the non missing value of nss_region code and name
.         bysort st_code dist_code (nss_region): egen nss_region_60 = max(nss_region) 
{txt}
{com}.         bysort st_code dist_code (nss_region): gen nss_region_name_60 = nss_region_name[1]
{txt}
{com}. 
.         *Replace missing values
.         replace nss_region = nss_region_60 if missing(nss_region)       
{txt}(463 real changes made)

{com}.         replace nss_region_name = nss_region_name_60 if nss_region_name == ""
{txt}(461 real changes made)

{com}.         
.         drop nss_region_60 nss_region_name_60
{txt}
{com}.         
.         keep if nss_round == 61
{txt}(600 observations deleted)

{com}.         
.         tempfile nss_region_60
{txt}
{com}.         save `nss_region_60'
{txt}{p 0 4 2}
file {bf}
C:\Users\ayush\AppData\Local\Temp\ST_33c0_000002.tmp{rm}
saved
as .dta format
{p_end}

{com}.         
.         restore
{txt}
{com}.         
.         *Merging the data with nss regiosn for round 61 with the original dataset we have for districts 
.         merge m:1 nss_round st_code dist_code using `nss_region_60', update
{res}
{txt}{col 5}Result{col 33}Number of obs
{col 5}{hline 41}
{col 5}Not matched{col 30}{res}           4,461
{txt}{col 9}from master{col 30}{res}           4,461{txt}  (_merge==1)
{col 9}from using{col 30}{res}               0{txt}  (_merge==2)

{col 5}Matched{col 30}{res}             593
{txt}{col 9}not updated{col 30}{res}             130{txt}  (_merge==3)
{col 9}missing updated{col 30}{res}             463{txt}  (_merge==4)
{col 9}nonmissing conflict{col 30}{res}               0{txt}  (_merge==5)
{col 5}{hline 41}

{com}.         drop _merge
{txt}
{com}. 
.         duplicates drop // 2 obs dropped

{p 0 4}{txt}Duplicates in terms of {txt} all variables{p_end}

(2 observations deleted)

{com}.         bys nss_round nss_region st_code dist_code: gen dup = cond(_N==1,0,_n)
{txt}
{com}.         drop if dup >1 // 1 obs deleted 
{txt}(357 observations deleted)

{com}.         drop dup
{txt}
{com}. 
.         tempfile districts_91
{txt}
{com}.         save `districts_91'
{txt}{p 0 4 2}
file {bf}
C:\Users\ayush\AppData\Local\Temp\ST_33c0_000003.tmp{rm}
saved
as .dta format
{p_end}

{com}. 
. * 2. LOADING THE NSS LABOUR DATASET 
.         use "$nss_lab/intermediate/nss_lab_clean.dta", clear
{txt}(NSS Labour: 1987- 2011)

{com}. 
.         *Dropping round 38 and 50 which don't have district codes
.         drop if nss_round == 38 | nss_round == 50
{txt}(1,187,837 observations deleted)

{com}.                 
.         destring nss_region st_code dist_code, replace
{txt}nss_region: all characters numeric; {res}replaced {txt}as {res}int
{txt}(391 missing values generated)
{res}{txt}st_code already numeric; no {res}replace
{txt}dist_code already numeric; no {res}replace
{txt}
{com}.                         
.         *Merging the district harmonization with the nss labour dataset 
.         merge m:1 nss_round nss_region st_code dist_code using `districts_91', gen(dist_merge)
{res}
{txt}{col 5}Result{col 33}Number of obs
{col 5}{hline 41}
{col 5}Not matched{col 30}{res}         101,238
{txt}{col 9}from master{col 30}{res}         101,003{txt}  (dist_merge==1)
{col 9}from using{col 30}{res}             235{txt}  (dist_merge==2)

{col 5}Matched{col 30}{res}       3,936,936{txt}  (dist_merge==3)
{col 5}{hline 41}

{com}.         
.         keep if dist_merge == 3 // There are some unmerged from the nss labour dataset as the data for nss regions differs from what is tehre in the documentation
{txt}(101,238 observations deleted)

{com}.         drop dist_merge 
{txt}
{com}.                         
.         *Saving the final dataset 
.         save "$nss_lab/intermediate/nss_lab_dist_merge.dta", replace
{txt}{p 0 4 2}
file {bf}
C:/Users/ayush/Dropbox/idl/nss/labour/intermediate/nss_lab_dist_merge.dta{rm}
saved
{p_end}

{com}. 
. 
{txt}end of do-file

{com}. 
. * CONSISTENT INDUSTRY CODES 
. do "$code/nss/nss_lab/03_consistent_industry_codes.do"
{txt}
{com}. 
. /*
> PURPOSE: CREATE A VARIABLE WITH CONSISTENT NIC INDUSTRY CODES FOR ALL YEARS
> */
. 
.         use "$nss_lab/intermediate/nss_lab_dist_merge.dta", clear
{txt}(NSS Labour: 1987- 2011)

{com}. 
.         
. * MERGE 2004 NIC CODES FOR YEARS >= 2008
.         preserve
{txt}
{com}.         
.         *loading the code file for nic 2008 and 2004- 5 digit codes
.         use "${c -(}nic_concordances{c )-}/concordance_nic_2008_5d_2004_5d.dta", clear
{txt}
{com}.         
.         *trimming the codes
.         foreach var in nic_2008_5d nic_2004_5d {c -(}
{txt}  2{com}.             replace `var' = ustrtrim(`var' )
{txt}  3{com}.         {c )-}
{txt}(101 real changes made)
(0 real changes made)

{com}.         
.         bys nic_2008_5d: gen dup = cond(_N==1,0,_n)
{txt}
{com}.         drop if dup > 1 // 160 obs deleted
{txt}(244 observations deleted)

{com}.         gisid nic_2008_5d
{res}{txt}
{com}.         
.         tempfile nic04_08
{txt}
{com}.         save `nic04_08'
{txt}{p 0 4 2}
file {bf}
C:\Users\ayush\AppData\Local\Temp\ST_33c0_000002.tmp{rm}
saved
as .dta format
{p_end}

{com}.         restore
{txt}
{com}.         
.         *merging the nic concordance for nic 2008 to nic 2004 
.         merge m:1 nic_2008_5d using "`nic04_08'", gen(nic_merge) keepusing(nic_2004_5d) update
{res}{txt}{p 0 7 2}
(variable
{bf:nic_2008_5d} was {bf:str5}, now {bf:str6} to accommodate using data's values)
{p_end}

{col 5}Result{col 33}Number of obs
{col 5}{hline 41}
{col 5}Not matched{col 30}{res}       3,906,823
{txt}{col 9}from master{col 30}{res}       3,906,751{txt}  (nic_merge==1)
{col 9}from using{col 30}{res}              72{txt}  (nic_merge==2)

{col 5}Matched{col 30}{res}          30,185
{txt}{col 9}not updated{col 30}{res}               0{txt}  (nic_merge==3)
{col 9}missing updated{col 30}{res}          30,185{txt}  (nic_merge==4)
{col 9}nonmissing conflict{col 30}{res}               0{txt}  (nic_merge==5)
{col 5}{hline 41}

{com}.         
.         gen nic_2008_2d = substr(nic_2008_5d, 1, 2)
{txt}(3,781,190 missing values generated)

{com}.         
.         //assert nic_2004_5d != "" if nic_2008_5d != ""  &  (nic_2008_2d < 32 & nic_2008_2d > 10) // assertion is false, we are only checking for manufacturing section
.         
.         gdistinct nic_2008_5d // 1211 obs
{res}{txt}{res}
{txt}{col 14}{c |}        Observations
{col 14}{c |}      total   distinct
{hline 13}{c +}{hline 22}
{res} {txt}nic_2008_5d {c |}  {res}   155818       1210
{txt}
{com}.         
.         gdistinct nic_2004_5d if nic_2008_5d != "" // 439 obs
{res}{txt}{res}
{txt}{col 14}{c |}        Observations
{col 14}{c |}      total   distinct
{hline 13}{c +}{hline 22}
{res} {txt}nic_2004_5d {c |}  {res}    30257        530
{txt}
{com}. 
.         * whenever nic 2008 code is avail, now nic 2004 code is also avail
.         *assert nic_2004_5d != "" if nic_2008_5d != ""  
.         
.         drop if nic_merge == 2
{txt}(72 observations deleted)

{com}.         drop nic_merge 
{txt}
{com}. 
. * MERGE 1998 NIC CODES FOR YEARS >= 2004
.         preserve
{txt}
{com}.         use "${c -(}nic_concordances{c )-}/concordance_nic_2004_5d_1998_5d.dta", clear
{txt}
{com}.         bys nic_2004_5d: gen dup = cond(_N==1,0,_n)
{txt}
{com}.         drop if dup > 1
{txt}(3 observations deleted)

{com}.         gisid nic_2004_5d
{res}{txt}
{com}.         tempfile nic04_5d
{txt}
{com}.         save `nic04_5d'
{txt}{p 0 4 2}
file {bf}
C:\Users\ayush\AppData\Local\Temp\ST_33c0_000004.tmp{rm}
saved
as .dta format
{p_end}

{com}.         restore
{txt}
{com}. 
.         merge m:1 nic_2004_5d using "`nic04_5d'", gen(nic_merge) keepusing(nic_1998_5d) update
{res}{txt}{p 0 7 2}
(variable
{bf:nic_2004_5d} was {bf:str5}, now {bf:str11} to accommodate using data's values)
{p_end}
{p 0 7 2}
(variable
{bf:nic_1998_5d} was {bf:str5}, now {bf:str8} to accommodate using data's values)
{p_end}

{col 5}Result{col 33}Number of obs
{col 5}{hline 41}
{col 5}Not matched{col 30}{res}       3,791,950
{txt}{col 9}from master{col 30}{res}       3,791,917{txt}  (nic_merge==1)
{col 9}from using{col 30}{res}              33{txt}  (nic_merge==2)

{col 5}Matched{col 30}{res}         145,019
{txt}{col 9}not updated{col 30}{res}               0{txt}  (nic_merge==3)
{col 9}missing updated{col 30}{res}         145,019{txt}  (nic_merge==4)
{col 9}nonmissing conflict{col 30}{res}               0{txt}  (nic_merge==5)
{col 5}{hline 41}

{com}.         
.         gen nic_2004_2d = substr(nic_2004_5d, 1, 2)
{txt}(3,409,878 missing values generated)

{com}.         destring nic_2004_2d, replace
{txt}nic_2004_2d: all characters numeric; {res}replaced {txt}as {res}byte
{txt}(3409878 missing values generated)
{res}{txt}
{com}.         //assert nic_1998_5d != "" if nic_2004_5d != ""  &  (nic_2004_2d < 32 & nic_2004_2d > 10) // 18019 is wrong nic it shoudl be 18109
.         
.         drop if nic_merge == 2
{txt}(33 observations deleted)

{com}.         drop nic_merge
{txt}
{com}.         
. 
. * MERGE 1987 NIC CODES FOR YEARS 1998
.         preserve
{txt}
{com}.         use "${c -(}nic_concordances{c )-}/concordnace_1987_3d_1998_4d.dta", clear
{txt}
{com}.         bys nic_1987_3d: gen dup = cond(_N==1,0,_n)
{txt}
{com}.         drop if dup > 1
{txt}(141 observations deleted)

{com}.         gisid nic_1987_3d
{res}{txt}
{com}.         tempfile nic98_87
{txt}
{com}.         save `nic98_87'
{txt}{p 0 4 2}
file {bf}
C:\Users\ayush\AppData\Local\Temp\ST_33c0_000006.tmp{rm}
saved
as .dta format
{p_end}

{com}.         restore
{txt}
{com}. 
.         /*
>         merge m:1 nic_1987_3d using "`nic98_87'", gen(nic_merge) keepusing(nic_1998_4d) update
> 
>         drop if nic_merge == 2
>         drop nic_merge
>         */
. * MERGE 1998 5D NIC CODES WITH 1987 4D NIC CODES 
. 
.         replace nic_1998_4d = substr(nic_1998_5d, 1, 4)
{txt}(145,019 real changes made)

{com}. 
.         tab year if nic_1998_4d == "" 

{txt}Survey Year {c |}      Freq.     Percent        Cum.
{hline 12}{c +}{hline 35}
       1987 {c |}{res}    620,432       19.03       19.03
{txt}       1999 {c |}{res}    382,830       11.74       30.78
{txt}       2004 {c |}{res}    563,407       17.28       48.06
{txt}       2005 {c |}{res}    332,579       10.20       58.26
{txt}       2007 {c |}{res}    523,354       16.06       74.32
{txt}       2009 {c |}{res}    417,102       12.80       87.12
{txt}       2011 {c |}{res}    419,976       12.88      100.00
{txt}{hline 12}{c +}{hline 35}
      Total {c |}{res}  3,259,680      100.00
{txt}
{com}.         
.         replace nic_1998_3d = substr(nic_1998_4d, 1, 3)
{txt}(145,019 real changes made)

{com}.         replace nic_1998_4d = substr(nic_1998_5d, 1, 4)
{txt}(0 real changes made)

{com}.         
.         gen nic_1998_2d = substr(nic_1998_4d, 1, 2)
{txt}(3,259,680 missing values generated)

{com}. 
.         
.         save "$nss_lab/intermediate/nss_consistent_ind.dta", replace
{txt}{p 0 4 2}
file {bf}
C:/Users/ayush/Dropbox/idl/nss/labour/intermediate/nss_consistent_ind.dta{rm}
saved
{p_end}

{com}. 
. 
. /*
> 
> Steps involved:
>         1. Loading the nss labour dataset
>         2. Merging the 2004 NIC codes with the corresponding 2008 NIC codes. This is done for the codes from the manufacturing sector. 
>            This is from NIC 2d > 9 and NIC 2d < 34.
>         3. Then we merge the NIC Concordance (2004 to 1998) with labour dataset. This gives the the corresponding 1998 NIC codes for the NIC 2004 codes.
>            We try to ensure that all codes from the manufacturing code are merged. For some of the codes which were new in 2004 for instance the nec codes,
>            in the concordance we fill the 4 digit code against them. For eg. if a code 12519 is a new nic 2004 code, we enter 12510 as its 1998 nic counterpart. 
>         4. For the year 1987, we have 1987 codes at the 3 digit level. So, we Merge NIC Concordance (1987 to 1998) with labour dataset giving us the the 
>            corresponding 1998 NIC codes at the 4 digit level. 
> 
>         # This ensures that we have 
>                 - nic 1998 4 digit codes against nic 1987 codes at the 3 digit level
>                 - nic 1998 5 digit codes against nic 2004 and nic 2008 at the 5 digit level
>                 - nic 1998 at the 5 digit level for all nic 1987, 2004 and 2008 for the manufacturing sector #
>         
> 
> 

{txt}end of do-file

{com}. 
. * CONSISTENT OCCUPATION CODES 
. do "$code/nss/nss_lab/04_consistent_occupation_codes.do"
{txt}
{com}. *Purpose: Have consitent occupation codes 
. 
. use "$nss_lab/intermediate/nss_consistent_ind.dta", clear
{txt}(NSS Labour: 1987- 2011)

{com}. 
. preserve
{txt}
{com}. 
. *loading the nco concordance 
. import excel "${c -(}nic_concordances{c )-}/concordance_2004_3d_1968_3d_nco.xlsx", sheet("Sheet1") firstrow allstring clear
{res}{text}(3 vars, 116 obs)

{com}. drop C // this is the description
{txt}
{com}. 
. *split in various variables as for each nco 2004 there are many nco 1968
. split nco_1968_3d , parse(,)
{res}variables created as string: 
{txt}{col 1}nco_1968_3d1{col 15}nco_1968_3d7{col 29}nco_1968_~13{col 43}nco_1968_~19{col 57}nco_1968_~25
{col 1}nco_1968_3d2{col 15}nco_1968_3d8{col 29}nco_1968_~14{col 43}nco_1968_~20{col 57}nco_1968_~26
{col 1}nco_1968_3d3{col 15}nco_1968_3d9{col 29}nco_1968_~15{col 43}nco_1968_~21{col 57}nco_1968_~27
{col 1}nco_1968_3d4{col 15}nco_1968_~10{col 29}nco_1968_~16{col 43}nco_1968_~22{col 57}nco_1968_~28
{col 1}nco_1968_3d5{col 15}nco_1968_~11{col 29}nco_1968_~17{col 43}nco_1968_~23{col 57}nco_1968_~29
{col 1}nco_1968_3d6{col 15}nco_1968_~12{col 29}nco_1968_~18{col 43}nco_1968_~24{col 57}nco_1968_~30

{com}. drop nco_1968_3d
{txt}
{com}. 
. *reshaping the data
. reshape long nco_1968_3d, i( nco_2004_3d ) j(nco_1968)
{txt}(j = 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30)

Data{col 36}Wide{col 43}->{col 48}Long
{hline 77}
Number of observations     {res}         116   {txt}->   {res}3,480       
{txt}Number of variables        {res}          31   {txt}->   {res}3           
{txt}j variable (30 values)                    ->   {res}nco_1968
{txt}xij variables:
{res}nco_1968_3d1 nco_1968_3d2 ... nco_1968_3d30{txt}->  {res}nco_1968_3d
{txt}{hline 77}

{com}. drop if nco_1968_3d == ""
{txt}(2,826 observations deleted)

{com}. 
. *for now we only keep one nco 1968 against each nco 2004 so we can have a m:1 merge with nss
. bys nco_2004_3d: gen dup = cond(_N==1,0,_n)
{txt}
{com}. drop if dup > 1 
{txt}(545 observations deleted)

{com}. gisid nco_2004_3d
{res}{txt}
{com}. 
. keep nco_1968_3d nco_2004_3d
{txt}
{com}. 
. tempfile nco04_68
{txt}
{com}. save `nco04_68'
{txt}{p 0 4 2}
file {bf}
C:\Users\ayush\AppData\Local\Temp\ST_33c0_000002.tmp{rm}
saved
as .dta format
{p_end}

{com}. 
. restore
{txt}
{com}. 
. *merging with nss dataset 
. merge m:1 nco_2004_3d using "`nco04_68'", gen(nco_merge) update
{res}{txt}{p 0 7 2}
(variable
{bf:nco_1968_3d} was {bf:str3}, now {bf:str4} to accommodate using data's values)
{p_end}

{col 5}Result{col 33}Number of obs
{col 5}{hline 41}
{col 5}Not matched{col 30}{res}       3,431,086
{txt}{col 9}from master{col 30}{res}       3,431,086{txt}  (nco_merge==1)
{col 9}from using{col 30}{res}               0{txt}  (nco_merge==2)

{col 5}Matched{col 30}{res}         505,850
{txt}{col 9}not updated{col 30}{res}               0{txt}  (nco_merge==3)
{col 9}missing updated{col 30}{res}         505,850{txt}  (nco_merge==4)
{col 9}nonmissing conflict{col 30}{res}               0{txt}  (nco_merge==5)
{col 5}{hline 41}

{com}. 
. drop nco_merge
{txt}
{com}. 
. save "$nss_lab/clean/nss_lab_final.dta", replace
{txt}{p 0 4 2}
file {bf}
C:/Users/ayush/Dropbox/idl/nss/labour/clean/nss_lab_final.dta{rm}
saved
{p_end}

{com}. 
. // save "$ida/data/nss/nss_lab_final.dta", replace
. 
{txt}end of do-file

{com}. 
. * Check for missing obs and create graphs for key variables to check for consistency. 
. // do "$code/nss/nss_lab/05a_nss_lab_graphs.do"
. 
. // do "$code/nss/nss_lab/05b_nss_lab_graphs.do"
. . log close
      {txt}name:  {res}<unnamed>
       {txt}log:  {res}C:\Users\ayush\OneDrive\Documents\idli_ext\code\nss\testlog2.smcl
  {txt}log type:  {res}smcl
 {txt}closed on:  {res}19 Nov 2025, 16:10:12
{txt}{.-}
{smcl}
{txt}{sf}{ul off}